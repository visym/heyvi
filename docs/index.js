URLS=[
"heyvi/index.html",
"heyvi/detection.html",
"heyvi/label.html",
"heyvi/model/index.html",
"heyvi/model/ResNets_3D_PyTorch/index.html",
"heyvi/model/ResNets_3D_PyTorch/resnet.html",
"heyvi/model/face/index.html",
"heyvi/model/face/detection.html",
"heyvi/model/face/faster_rcnn.html",
"heyvi/model/face/recognition.html",
"heyvi/model/yolov3/index.html",
"heyvi/model/yolov3/network.html",
"heyvi/model/yolov3/utils/index.html",
"heyvi/model/yolov3/utils/parse_config.html",
"heyvi/model/yolov3/utils/utils.html",
"heyvi/model/yolov5/index.html",
"heyvi/model/yolov5/models/index.html",
"heyvi/model/yolov5/models/common.html",
"heyvi/model/yolov5/models/experimental.html",
"heyvi/model/yolov5/models/export.html",
"heyvi/model/yolov5/models/yolo.html",
"heyvi/model/yolov5/utils/index.html",
"heyvi/model/yolov5/utils/activations.html",
"heyvi/model/yolov5/utils/autoanchor.html",
"heyvi/model/yolov5/utils/general.html",
"heyvi/model/yolov5/utils/google_utils.html",
"heyvi/model/yolov5/utils/loss.html",
"heyvi/model/yolov5/utils/metrics.html",
"heyvi/model/yolov5/utils/torch_utils.html",
"heyvi/recognition.html",
"heyvi/sensor.html",
"heyvi/system.html",
"heyvi/util.html",
"heyvi/version.html"
];
INDEX=[
{
"ref":"heyvi",
"url":0,
"doc":" \"Hey Vi!\" HEYVI is a python package for visual AI that provides systems and trained models for activity detection and object tracking in videos. HEYVI provides:  Real time activity detection of the [MEVA activity classes](https: mevadata.org)  Real time visual object tracking in long duration videos  Live streaming of annotated videos to youtube live  Visual AI from RTSP cameras  Getting Started Create a video from a file and track the objects, then create an annotation visualization of the tracked video output (vo)   v = vipy.video.Scene(filename='/path/to/video.mp4').framerate(5) T = heyvi.system.Tracker() vo = T(v).annotate('/path/to/annotation.mp4')   Create a default RTSP camera and stream the privacy preserving annotated video (e.g. pixelated bounding boxes with captions) to a YouTube live stream.   v = heyvi.sensor.rtsp().framerate(5) T = heyvi.system.Tracker() with heyvi.system.YoutubeLive(fps=5, encoder='480p') as s: T(v, frame_callback=lambda im: s(im.pixelize().annotate( )    Customization The following environment varibles may be set by the client to specify live camera streams VIPY_RTSP_URL='rtsp: user@password:127.0.0.1' VIPY_RTSP_URL_0='rtsp: user@password:127.0.0.1' VIPY_RTSP_URL_1='rtsp: user@password:127.0.0.2' VIPY_YOUTUBE_STREAMKEY='xxxx-xxxx-xxxx-xxxx-xxxx' VIPY_CACHE='/home/username/.vipy' Where the environment variables VIPY_RTSP_URL_N are the list of cameras that are returned in  heyvi.sensor.cameralist , and VIPY_RTSP_URL refers to the default RTSP camera in  heyvi.sensor.rtsp . Please refer to the [vipy](https: visym.github.io/vipy) documentation for additional environment variables.  Versioning To determine what heyvi version you are running you can use: >>> heyvi.__version__ >>> heyvi.version.is_at_least('0.0.6')  Contact Visym Labs  "
},
{
"ref":"heyvi.detection",
"url":1,
"doc":""
},
{
"ref":"heyvi.detection.TorchNet",
"url":1,
"doc":""
},
{
"ref":"heyvi.detection.TorchNet.gpu",
"url":1,
"doc":"",
"func":1
},
{
"ref":"heyvi.detection.TorchNet.cpu",
"url":1,
"doc":"",
"func":1
},
{
"ref":"heyvi.detection.TorchNet.iscpu",
"url":1,
"doc":"",
"func":1
},
{
"ref":"heyvi.detection.TorchNet.isgpu",
"url":1,
"doc":"",
"func":1
},
{
"ref":"heyvi.detection.TorchNet.batchsize",
"url":1,
"doc":"",
"func":1
},
{
"ref":"heyvi.detection.FaceDetector",
"url":1,
"doc":"Faster R-CNN based face detector"
},
{
"ref":"heyvi.detection.FaceDetector.batchsize",
"url":1,
"doc":"",
"func":1
},
{
"ref":"heyvi.detection.Yolov5",
"url":1,
"doc":"Yolov5 based object detector >>> d = heyvi.detection.Detector() >>> d(vipy.image.vehicles( .show()"
},
{
"ref":"heyvi.detection.Yolov5.classlist",
"url":1,
"doc":"",
"func":1
},
{
"ref":"heyvi.detection.Yolov3",
"url":1,
"doc":"Yolov3 based object detector >>> d = heyvi.detection.Detector() >>> d(vipy.image.vehicles( .show()"
},
{
"ref":"heyvi.detection.Yolov3.classlist",
"url":1,
"doc":"",
"func":1
},
{
"ref":"heyvi.detection.ObjectDetector",
"url":1,
"doc":"Default object detector"
},
{
"ref":"heyvi.detection.MultiscaleObjectDetector",
"url":1,
"doc":"Given a list of images, break each one into a set of overlapping tiles, and ObjectDetector() on each, then recombining detections"
},
{
"ref":"heyvi.detection.VideoDetector",
"url":1,
"doc":"Iterate ObjectDetector() over each frame of video, yielding the detected frame"
},
{
"ref":"heyvi.detection.MultiscaleVideoDetector",
"url":1,
"doc":"Given a list of images, break each one into a set of overlapping tiles, and ObjectDetector() on each, then recombining detections"
},
{
"ref":"heyvi.detection.VideoTracker",
"url":1,
"doc":"Default object detector"
},
{
"ref":"heyvi.detection.VideoTracker.track",
"url":1,
"doc":"Batch tracking",
"func":1
},
{
"ref":"heyvi.detection.FaceTracker",
"url":1,
"doc":"Faster R-CNN based face detector"
},
{
"ref":"heyvi.detection.FaceTracker.track",
"url":1,
"doc":"Batch tracking",
"func":1
},
{
"ref":"heyvi.detection.MultiscaleVideoTracker",
"url":1,
"doc":"MultiscaleVideoTracker() class Args: minconf: [float]: The minimum confidence of an object detection to be considered for tracking miniou: [float]: The minimum IoU of an object detection with a track to be considered for assignment maxhistory: [int]: The maximum frame history lookback for assignment of a detection with a broken track smoothing: [str]: Unused objects: [list]: The list of allowable objects for tracking as supported by  heyvi.detection.MultiscaleObjectDetector.classlist . trackconf: [float]: The minimum confidence of an unassigned detection to spawn a new track verbose: [bool]: Logging verbosity gpu: [list]: List of GPU indexes to use batchsize: [int]: The GPU batchsize weightfile: [str]: The modelfile for the object detector overlapfrac: [int]: FIXME, this is a legacy parameter detbatchsize: [int]: The detection batchsize per image gate: [int]: The maximum distance in pixels around a detection to search for candidate tracks"
},
{
"ref":"heyvi.detection.MultiscaleVideoTracker.stream",
"url":1,
"doc":"",
"func":1
},
{
"ref":"heyvi.detection.MultiscaleVideoTracker.track",
"url":1,
"doc":"Batch tracking",
"func":1
},
{
"ref":"heyvi.detection.WeakAnnotationTracker",
"url":1,
"doc":"heyvi.detection.WeakAnnotationTracker() Given a weak annotation of an object bounding box from a human annotator, refine this weak annotation into a tight box using object detection proposals and tracking. Approach: - The input video should have weak tracks provided by live annotators with class names that intersect  heyvi.detection.MultiscaleVideoTracker.classlist . - Weak annotations are too loose, too tight, or poorly centered boxes provided by live annotators while recording. - This function runs a low confidence object detector and rescores object detection confidences based on overlap with the proposal. - Detections that maximally overlap the proposal with high detection confidence are proritized for tracking. - The tracker compbines these rescored detections as in the VideoTracker. - When done, each proposal is assigned to one track, and track IDs and activity IDs are mappped accordingly. - Activities that no longer overlap the actor track are removed - The tracker is run at a lower framerate (5Hz) then tracks are resampled to the input framerate Usage: Batch annotation tracker:   T = heyvi.detection.WeakAnnotationTracker() v = vipy.video.Scene( .)  contains weak annotations vt = T.track(v)  refined proposals vm = vt.combine(v.trackmap(lambda t: t.category('weak annotation' )   Streaming annotation tracker:   T = heyvi.detection.WeakAnnotationTracker() v = vipy.video.Scene( .)  contains weak annotations for vt in T(v): print(vt)    note - The video vt will be a clone of v such that each track in vt will be a refined track of a track in v. - All track and activities IDs are mapped appropriately from the input video. - The combined video vm has both the weak annotation and the refined tracks. - The tracker is run at a lower framerate (5Hz) then tracks are resampled to the input framerate. This is useful for linear track interpolation. - The tracker does not handle synonyms or capitalization differences like 'motorcycle' vs. 'Motorbike'. Be sure that the weak annotation input video overlaps with  heyvi.detection.MultiscaleVideoTracker.classlist "
},
{
"ref":"heyvi.detection.WeakAnnotationTracker.track",
"url":1,
"doc":"Batch tracking",
"func":1
},
{
"ref":"heyvi.detection.WeakAnnotationFaceTracker",
"url":1,
"doc":"heyvi.detection.WeakAnnotationFaceTracker() Given a weak annotation of an person, face or head bounding box from a human annotator, refine this weak annotation into a tight box around the face using object detection proposals and tracking. Approach: - The input video should have weak tracks provided by live annotators with class names that are in ['person', 'face', 'head'] - Weak annotations are too loose, too tight, or poorly centered boxes provided by live annotators while recording. - This function runs a low confidence face detector and rescores face detection confidences based on overlap with the proposal. - Detections that maximally overlap the proposal with high detection confidence are proritized for track assignment. - The tracker compbines these rescored detections as in the VideoTracker. - When done, each track is assigned to a proposal. - The tracker is run at a lower framerate (5Hz) then tracks are resampled to the input framerate See also:  heyvi.detection.WeakAnnotationTracker "
},
{
"ref":"heyvi.detection.WeakAnnotationFaceTracker.track",
"url":1,
"doc":"Batch tracking",
"func":1
},
{
"ref":"heyvi.detection.ActorAssociation",
"url":1,
"doc":"heyvi.detection.VideoAssociation() class Select the best object track of the target class associated with the primary actor class by gated spatial IOU and distance. Add the best object track to the scene and associate with all activities performed by the primary actor.  warning This is scheduled for deprecation, as the gating is unreliable. This should be replaced by the WeakAnnotationTracker for a target class."
},
{
"ref":"heyvi.detection.ActorAssociation.isallowable",
"url":1,
"doc":"",
"func":1
},
{
"ref":"heyvi.detection.ActorAssociation.track",
"url":1,
"doc":"Batch tracking",
"func":1
},
{
"ref":"heyvi.label",
"url":2,
"doc":""
},
{
"ref":"heyvi.label.Label",
"url":2,
"doc":""
},
{
"ref":"heyvi.label.piplabel_to_mevalabel",
"url":2,
"doc":"",
"func":1
},
{
"ref":"heyvi.label.mevalabel_to_index",
"url":2,
"doc":"",
"func":1
},
{
"ref":"heyvi.label.piplabel_to_index",
"url":2,
"doc":"",
"func":1
},
{
"ref":"heyvi.label.pip_250k_powerset",
"url":2,
"doc":"",
"func":1
},
{
"ref":"heyvi.model",
"url":3,
"doc":""
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch",
"url":4,
"doc":""
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet",
"url":5,
"doc":""
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.get_inplanes",
"url":5,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.conv3x3x3",
"url":5,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.conv1x1x1",
"url":5,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.BasicBlock",
"url":5,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.BasicBlock.dump_patches",
"url":5,
"doc":""
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.BasicBlock.training",
"url":5,
"doc":""
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.BasicBlock.expansion",
"url":5,
"doc":""
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.BasicBlock.forward",
"url":5,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.Bottleneck",
"url":5,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.Bottleneck.dump_patches",
"url":5,
"doc":""
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.Bottleneck.training",
"url":5,
"doc":""
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.Bottleneck.expansion",
"url":5,
"doc":""
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.Bottleneck.forward",
"url":5,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.ResNet",
"url":5,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.ResNet.dump_patches",
"url":5,
"doc":""
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.ResNet.training",
"url":5,
"doc":""
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.ResNet.forward",
"url":5,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.ResNets_3D_PyTorch.resnet.generate_model",
"url":5,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.face",
"url":6,
"doc":""
},
{
"ref":"heyvi.model.face.detection",
"url":7,
"doc":""
},
{
"ref":"heyvi.model.face.detection.log_info",
"url":7,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.face.detection.FaceRCNN",
"url":7,
"doc":"Wrapper for PyTorch RCNN detector"
},
{
"ref":"heyvi.model.face.detection.FaceRCNN.dets_to_scene",
"url":7,
"doc":"Convert detections returned from this object to a vipy.image.Scene object",
"func":1
},
{
"ref":"heyvi.model.face.detection.FaceRCNN.detect",
"url":7,
"doc":"Run detection on a numpy image, with specified padding and min size",
"func":1
},
{
"ref":"heyvi.model.face.detection.FaceRCNN.select_from_rotated",
"url":7,
"doc":"Given that we tried rotating the image, select the best rotation to use",
"func":1
},
{
"ref":"heyvi.model.face.detection.FaceRCNN.im_detect",
"url":7,
"doc":"Detect object classes in an image given object proposals. Arguments: net (pytorch): Fast R-CNN network to use im (ndarray): color image to test (in BGR order, as (H, W, C) boxes (ndarray): R x 4 array of object proposals or None (for RPN) Returns: scores (ndarray): R x K array of object class scores (K includes background as object category 0) boxes (ndarray): R x (4 K) array of predicted bounding boxes",
"func":1
},
{
"ref":"heyvi.model.face.detection.FaceRCNN.bbox_transform",
"url":7,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.face.detection.FaceRCNN.bbox_transform_inv",
"url":7,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.face.detection.FaceRCNN.clip_boxes",
"url":7,
"doc":"Clip boxes to image boundaries.",
"func":1
},
{
"ref":"heyvi.model.face.faster_rcnn",
"url":8,
"doc":""
},
{
"ref":"heyvi.model.face.faster_rcnn.RpnLayers",
"url":8,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.face.faster_rcnn.RpnLayers.dump_patches",
"url":8,
"doc":""
},
{
"ref":"heyvi.model.face.faster_rcnn.RpnLayers.training",
"url":8,
"doc":""
},
{
"ref":"heyvi.model.face.faster_rcnn.RpnLayers.forward",
"url":8,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.face.faster_rcnn.BottomLayers",
"url":8,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.face.faster_rcnn.BottomLayers.dump_patches",
"url":8,
"doc":""
},
{
"ref":"heyvi.model.face.faster_rcnn.BottomLayers.training",
"url":8,
"doc":""
},
{
"ref":"heyvi.model.face.faster_rcnn.BottomLayers.forward",
"url":8,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.face.faster_rcnn.TopLayers",
"url":8,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.face.faster_rcnn.TopLayers.dump_patches",
"url":8,
"doc":""
},
{
"ref":"heyvi.model.face.faster_rcnn.TopLayers.training",
"url":8,
"doc":""
},
{
"ref":"heyvi.model.face.faster_rcnn.TopLayers.forward",
"url":8,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.face.faster_rcnn.FasterRCNN",
"url":8,
"doc":"PyTorch-1.3 model conversion of ResNet-101_faster_rcnn_ohem_iter_20000.caffemodel, leveraging MMDNN conversion tools Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.face.faster_rcnn.FasterRCNN.dump_patches",
"url":8,
"doc":""
},
{
"ref":"heyvi.model.face.faster_rcnn.FasterRCNN.training",
"url":8,
"doc":""
},
{
"ref":"heyvi.model.face.faster_rcnn.FasterRCNN.forward",
"url":8,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.face.faster_rcnn.FasterRCNN_MMDNN",
"url":8,
"doc":"PyTorch-1.3 model conversion of ResNet-101_faster_rcnn_ohem_iter_20000.caffemodel, leveraging MMDNN conversion tools Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.face.faster_rcnn.FasterRCNN_MMDNN.dump_patches",
"url":8,
"doc":""
},
{
"ref":"heyvi.model.face.faster_rcnn.FasterRCNN_MMDNN.training",
"url":8,
"doc":""
},
{
"ref":"heyvi.model.face.faster_rcnn.FasterRCNN_MMDNN.forward",
"url":8,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.face.faster_rcnn.conversion",
"url":8,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.face.recognition",
"url":9,
"doc":""
},
{
"ref":"heyvi.model.face.recognition.convert_resnet101v4_image",
"url":9,
"doc":"Convert an RGB byte image to a FloatTensor suitable for processing with the network. This function assumes the image has already been resized, cropped, jittered, etc.",
"func":1
},
{
"ref":"heyvi.model.face.recognition.unconvert_resnet101v4_image",
"url":9,
"doc":"Convert a FloatTensor to an RGB byte Image",
"func":1
},
{
"ref":"heyvi.model.face.recognition.conv3x3",
"url":9,
"doc":"3x3 convolution with padding",
"func":1
},
{
"ref":"heyvi.model.face.recognition.BasicBlock",
"url":9,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.face.recognition.BasicBlock.dump_patches",
"url":9,
"doc":""
},
{
"ref":"heyvi.model.face.recognition.BasicBlock.training",
"url":9,
"doc":""
},
{
"ref":"heyvi.model.face.recognition.BasicBlock.expansion",
"url":9,
"doc":""
},
{
"ref":"heyvi.model.face.recognition.BasicBlock.forward",
"url":9,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.face.recognition.Bottleneck",
"url":9,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.face.recognition.Bottleneck.dump_patches",
"url":9,
"doc":""
},
{
"ref":"heyvi.model.face.recognition.Bottleneck.training",
"url":9,
"doc":""
},
{
"ref":"heyvi.model.face.recognition.Bottleneck.expansion",
"url":9,
"doc":""
},
{
"ref":"heyvi.model.face.recognition.Bottleneck.forward",
"url":9,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.face.recognition.ConcatChannels",
"url":9,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.face.recognition.ConcatChannels.dump_patches",
"url":9,
"doc":""
},
{
"ref":"heyvi.model.face.recognition.ConcatChannels.training",
"url":9,
"doc":""
},
{
"ref":"heyvi.model.face.recognition.ConcatChannels.forward",
"url":9,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.face.recognition.Multiply",
"url":9,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.face.recognition.Multiply.dump_patches",
"url":9,
"doc":""
},
{
"ref":"heyvi.model.face.recognition.Multiply.training",
"url":9,
"doc":""
},
{
"ref":"heyvi.model.face.recognition.Multiply.forward",
"url":9,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.face.recognition.ResNet",
"url":9,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.face.recognition.ResNet.dump_patches",
"url":9,
"doc":""
},
{
"ref":"heyvi.model.face.recognition.ResNet.training",
"url":9,
"doc":""
},
{
"ref":"heyvi.model.face.recognition.ResNet.forward",
"url":9,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.face.recognition.resnet101v6",
"url":9,
"doc":"Construct resnet-101v6 model",
"func":1
},
{
"ref":"heyvi.model.yolov3",
"url":10,
"doc":""
},
{
"ref":"heyvi.model.yolov3.network",
"url":11,
"doc":""
},
{
"ref":"heyvi.model.yolov3.network.create_modules",
"url":11,
"doc":"Constructs module list of layer blocks from module configuration in module_defs",
"func":1
},
{
"ref":"heyvi.model.yolov3.network.Upsample",
"url":11,
"doc":"nn.Upsample is deprecated Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov3.network.Upsample.dump_patches",
"url":11,
"doc":""
},
{
"ref":"heyvi.model.yolov3.network.Upsample.training",
"url":11,
"doc":""
},
{
"ref":"heyvi.model.yolov3.network.Upsample.forward",
"url":11,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov3.network.EmptyLayer",
"url":11,
"doc":"Placeholder for 'route' and 'shortcut' layers Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov3.network.EmptyLayer.dump_patches",
"url":11,
"doc":""
},
{
"ref":"heyvi.model.yolov3.network.EmptyLayer.training",
"url":11,
"doc":""
},
{
"ref":"heyvi.model.yolov3.network.EmptyLayer.forward",
"url":11,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov3.network.YOLOLayer",
"url":11,
"doc":"Detection layer Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov3.network.YOLOLayer.dump_patches",
"url":11,
"doc":""
},
{
"ref":"heyvi.model.yolov3.network.YOLOLayer.training",
"url":11,
"doc":""
},
{
"ref":"heyvi.model.yolov3.network.YOLOLayer.compute_grid_offsets",
"url":11,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov3.network.YOLOLayer.forward",
"url":11,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov3.network.Darknet",
"url":11,
"doc":"YOLOv3 object detection model Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov3.network.Darknet.dump_patches",
"url":11,
"doc":""
},
{
"ref":"heyvi.model.yolov3.network.Darknet.training",
"url":11,
"doc":""
},
{
"ref":"heyvi.model.yolov3.network.Darknet.forward",
"url":11,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov3.network.Darknet.load_darknet_weights",
"url":11,
"doc":"Parses and loads the weights stored in 'weights_path'",
"func":1
},
{
"ref":"heyvi.model.yolov3.network.Darknet.save_darknet_weights",
"url":11,
"doc":"@:param path - path of the new weights file @:param cutoff - save layers between 0 and cutoff (cutoff = -1 -> all are saved)",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils",
"url":12,
"doc":""
},
{
"ref":"heyvi.model.yolov3.utils.parse_config",
"url":13,
"doc":""
},
{
"ref":"heyvi.model.yolov3.utils.parse_config.parse_model_config",
"url":13,
"doc":"Parses the yolo-v3 layer configuration file and returns module definitions",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils.parse_config.parse_data_config",
"url":13,
"doc":"Parses the data configuration file",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils.utils",
"url":14,
"doc":""
},
{
"ref":"heyvi.model.yolov3.utils.utils.to_cpu",
"url":14,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils.utils.load_classes",
"url":14,
"doc":"Loads class labels at 'path'",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils.utils.weights_init_normal",
"url":14,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils.utils.rescale_boxes",
"url":14,
"doc":"Rescales bounding boxes to the original shape",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils.utils.xywh2xyxy",
"url":14,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils.utils.ap_per_class",
"url":14,
"doc":"Compute the average precision, given the recall and precision curves. Source: https: github.com/rafaelpadilla/Object-Detection-Metrics.  Arguments tp: True positives (list). conf: Objectness value from 0-1 (list). pred_cls: Predicted object classes (list). target_cls: True object classes (list).  Returns The average precision as computed in py-faster-rcnn.",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils.utils.compute_ap",
"url":14,
"doc":"Compute the average precision, given the recall and precision curves. Code originally from https: github.com/rbgirshick/py-faster-rcnn.  Arguments recall: The recall curve (list). precision: The precision curve (list).  Returns The average precision as computed in py-faster-rcnn.",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils.utils.get_batch_statistics",
"url":14,
"doc":"Compute true positives, predicted scores and predicted labels per sample",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils.utils.bbox_wh_iou",
"url":14,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils.utils.bbox_iou",
"url":14,
"doc":"Returns the IoU of two bounding boxes",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils.utils.non_max_suppression",
"url":14,
"doc":"Removes detections with lower object confidence score than 'conf_thres' and performs Non-Maximum Suppression to further filter detections. Returns detections with shape: (x1, y1, x2, y2, object_conf, class_score, class_pred)",
"func":1
},
{
"ref":"heyvi.model.yolov3.utils.utils.build_targets",
"url":14,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5",
"url":15,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models",
"url":16,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.autopad",
"url":17,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.DWConv",
"url":17,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.Conv",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.common.Conv.dump_patches",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.Conv.training",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.Conv.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.Conv.fuseforward",
"url":17,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.Bottleneck",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.common.Bottleneck.dump_patches",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.Bottleneck.training",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.Bottleneck.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.BottleneckCSP",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.common.BottleneckCSP.dump_patches",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.BottleneckCSP.training",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.BottleneckCSP.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.SPP",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.common.SPP.dump_patches",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.SPP.training",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.SPP.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.Focus",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.common.Focus.dump_patches",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.Focus.training",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.Focus.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.Concat",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.common.Concat.dump_patches",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.Concat.training",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.Concat.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.NMS",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.common.NMS.dump_patches",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.NMS.training",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.NMS.conf",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.NMS.iou",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.NMS.classes",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.NMS.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.Detections",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.Detections.display",
"url":17,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.Detections.print",
"url":17,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.Detections.show",
"url":17,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.Detections.save",
"url":17,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.Detections.tolist",
"url":17,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.Flatten",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.common.Flatten.dump_patches",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.Flatten.training",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.Flatten.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.common.Classify",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.common.Classify.dump_patches",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.Classify.training",
"url":17,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.common.Classify.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.experimental",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.CrossConv",
"url":18,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.experimental.CrossConv.dump_patches",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.CrossConv.training",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.CrossConv.forward",
"url":18,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.experimental.C3",
"url":18,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.experimental.C3.dump_patches",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.C3.training",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.C3.forward",
"url":18,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.experimental.Sum",
"url":18,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.experimental.Sum.dump_patches",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.Sum.training",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.Sum.forward",
"url":18,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.experimental.GhostConv",
"url":18,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.experimental.GhostConv.dump_patches",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.GhostConv.training",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.GhostConv.forward",
"url":18,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.experimental.GhostBottleneck",
"url":18,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.experimental.GhostBottleneck.dump_patches",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.GhostBottleneck.training",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.GhostBottleneck.forward",
"url":18,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.experimental.MixConv2d",
"url":18,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.experimental.MixConv2d.dump_patches",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.MixConv2d.training",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.MixConv2d.forward",
"url":18,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.experimental.Ensemble",
"url":18,
"doc":"Holds submodules in a list. :class: ~torch.nn.ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all :class: ~torch.nn.Module methods. Arguments: modules (iterable, optional): an iterable of modules to add Example class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)]) def forward(self, x):  ModuleList can act as an iterable, or be indexed using ints for i, l in enumerate(self.linears): x = self.linears[i  2](x) + l(x) return x Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.experimental.Ensemble.dump_patches",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.Ensemble.training",
"url":18,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.experimental.Ensemble.forward",
"url":18,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.experimental.attempt_load",
"url":18,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.export",
"url":19,
"doc":"Exports a YOLOv5  .pt model to ONNX and TorchScript formats Usage: $ export PYTHONPATH=\"$PWD\"  python models/export.py  weights ./weights/yolov5s.pt  img 640  batch 1"
},
{
"ref":"heyvi.model.yolov5.models.yolo",
"url":20,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.yolo.Detect",
"url":20,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.yolo.Detect.dump_patches",
"url":20,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.yolo.Detect.training",
"url":20,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.yolo.Detect.stride",
"url":20,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.yolo.Detect.export",
"url":20,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.yolo.Detect.forward",
"url":20,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.yolo.Model",
"url":20,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.models.yolo.Model.dump_patches",
"url":20,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.yolo.Model.training",
"url":20,
"doc":""
},
{
"ref":"heyvi.model.yolov5.models.yolo.Model.forward",
"url":20,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.yolo.Model.forward_once",
"url":20,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.yolo.Model.fuse",
"url":20,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.yolo.Model.nms",
"url":20,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.yolo.Model.info",
"url":20,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.models.yolo.parse_model",
"url":20,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils",
"url":21,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations",
"url":22,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations.Swish",
"url":22,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.utils.activations.Swish.dump_patches",
"url":22,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations.Swish.training",
"url":22,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations.Swish.forward",
"url":22,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.activations.Hardswish",
"url":22,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.utils.activations.Hardswish.dump_patches",
"url":22,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations.Hardswish.training",
"url":22,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations.Hardswish.forward",
"url":22,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.activations.MemoryEfficientSwish",
"url":22,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.utils.activations.MemoryEfficientSwish.dump_patches",
"url":22,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations.MemoryEfficientSwish.training",
"url":22,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations.MemoryEfficientSwish.F",
"url":22,
"doc":"Records operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use this class: https: pytorch.org/docs/stable/notes/extending.html extending-torch-autograd Every operation performed on :class: Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies ( input  >> class Exp(Function): >>> >>> @staticmethod >>> def forward(ctx, i): >>> result = i.exp() >>> ctx.save_for_backward(result) >>> return result >>> >>> @staticmethod >>> def backward(ctx, grad_output): >>> result, = ctx.saved_tensors >>> return grad_output  result >>> >>>  Use it by calling the apply method: >>> output = Exp.apply(input)"
},
{
"ref":"heyvi.model.yolov5.utils.activations.MemoryEfficientSwish.forward",
"url":22,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.activations.Mish",
"url":22,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.utils.activations.Mish.dump_patches",
"url":22,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations.Mish.training",
"url":22,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations.Mish.forward",
"url":22,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.activations.MemoryEfficientMish",
"url":22,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.utils.activations.MemoryEfficientMish.dump_patches",
"url":22,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations.MemoryEfficientMish.training",
"url":22,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations.MemoryEfficientMish.F",
"url":22,
"doc":"Records operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use this class: https: pytorch.org/docs/stable/notes/extending.html extending-torch-autograd Every operation performed on :class: Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies ( input  >> class Exp(Function): >>> >>> @staticmethod >>> def forward(ctx, i): >>> result = i.exp() >>> ctx.save_for_backward(result) >>> return result >>> >>> @staticmethod >>> def backward(ctx, grad_output): >>> result, = ctx.saved_tensors >>> return grad_output  result >>> >>>  Use it by calling the apply method: >>> output = Exp.apply(input)"
},
{
"ref":"heyvi.model.yolov5.utils.activations.MemoryEfficientMish.forward",
"url":22,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.activations.FReLU",
"url":22,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.utils.activations.FReLU.dump_patches",
"url":22,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations.FReLU.training",
"url":22,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.activations.FReLU.forward",
"url":22,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.autoanchor",
"url":23,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.autoanchor.check_anchor_order",
"url":23,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.autoanchor.check_anchors",
"url":23,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.autoanchor.kmean_anchors",
"url":23,
"doc":"Creates kmeans-evolved anchors from training dataset Arguments: path: path to dataset  .yaml, or a loaded dataset n: number of anchors img_size: image size used for training thr: anchor-label wh ratio threshold hyperparameter hyp['anchor_t'] used for training, default=4.0 gen: generations to evolve anchors using genetic algorithm verbose: print all results Return: k: kmeans evolved anchors Usage: from utils.autoanchor import  ; _ = kmean_anchors()",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general",
"url":24,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.general.set_logging",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.init_seeds",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.get_latest_run",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.check_git_status",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.check_img_size",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.check_file",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.check_dataset",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.make_divisible",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.labels_to_class_weights",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.labels_to_image_weights",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.coco80_to_coco91_class",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.xyxy2xywh",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.xywh2xyxy",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.scale_coords",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.clip_coords",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.bbox_iou",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.box_iou",
"url":24,
"doc":"Return intersection-over-union (Jaccard index) of boxes. Both sets of boxes are expected to be in (x1, y1, x2, y2) format. Arguments: box1 (Tensor[N, 4]) box2 (Tensor[M, 4]) Returns: iou (Tensor[N, M]): the NxM matrix containing the pairwise IoU values for every element in boxes1 and boxes2",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.wh_iou",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.non_max_suppression",
"url":24,
"doc":"Performs Non-Maximum Suppression (NMS) on inference results Returns: detections with shape: nx6 (x1, y1, x2, y2, conf, cls)",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.strip_optimizer",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.print_mutation",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.apply_classifier",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.general.increment_path",
"url":24,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.google_utils",
"url":25,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.google_utils.gsutil_getsize",
"url":25,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.google_utils.attempt_download",
"url":25,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.google_utils.gdrive_download",
"url":25,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.google_utils.get_token",
"url":25,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.loss",
"url":26,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.loss.smooth_BCE",
"url":26,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.loss.BCEBlurWithLogitsLoss",
"url":26,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.utils.loss.BCEBlurWithLogitsLoss.dump_patches",
"url":26,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.loss.BCEBlurWithLogitsLoss.training",
"url":26,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.loss.BCEBlurWithLogitsLoss.forward",
"url":26,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.loss.FocalLoss",
"url":26,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.utils.loss.FocalLoss.dump_patches",
"url":26,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.loss.FocalLoss.training",
"url":26,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.loss.FocalLoss.forward",
"url":26,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.loss.QFocalLoss",
"url":26,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"heyvi.model.yolov5.utils.loss.QFocalLoss.dump_patches",
"url":26,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.loss.QFocalLoss.training",
"url":26,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.loss.QFocalLoss.forward",
"url":26,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.loss.compute_loss",
"url":26,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.loss.build_targets",
"url":26,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.metrics",
"url":27,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.metrics.fitness",
"url":27,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.metrics.ap_per_class",
"url":27,
"doc":"Compute the average precision, given the recall and precision curves. Source: https: github.com/rafaelpadilla/Object-Detection-Metrics.  Arguments tp: True positives (nparray, nx1 or nx10). conf: Objectness value from 0-1 (nparray). pred_cls: Predicted object classes (nparray). target_cls: True object classes (nparray). plot: Plot precision-recall curve at mAP@0.5 save_dir: Plot save directory  Returns The average precision as computed in py-faster-rcnn.",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.metrics.compute_ap",
"url":27,
"doc":"Compute the average precision, given the recall and precision curves. Source: https: github.com/rbgirshick/py-faster-rcnn.  Arguments recall: The recall curve (list). precision: The precision curve (list).  Returns The average precision as computed in py-faster-rcnn.",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.metrics.ConfusionMatrix",
"url":27,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.metrics.ConfusionMatrix.process_batch",
"url":27,
"doc":"Return intersection-over-union (Jaccard index) of boxes. Both sets of boxes are expected to be in (x1, y1, x2, y2) format. Arguments: detections (Array[N, 6]), x1, y1, x2, y2, conf, class labels (Array[M, 5]), class, x1, y1, x2, y2 Returns: None, updates confusion matrix accordingly",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.metrics.ConfusionMatrix.matrix",
"url":27,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.metrics.ConfusionMatrix.plot",
"url":27,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.metrics.ConfusionMatrix.print",
"url":27,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.metrics.plot_pr_curve",
"url":27,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils",
"url":28,
"doc":""
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.torch_distributed_zero_first",
"url":28,
"doc":"Decorator to make all processes in distributed training wait for each local_master to do something.",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.init_torch_seeds",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.select_device",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.time_synchronized",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.is_parallel",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.intersect_dicts",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.initialize_weights",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.find_modules",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.sparsity",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.prune",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.fuse_conv_and_bn",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.model_info",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.load_classifier",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.scale_img",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.copy_attr",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.ModelEMA",
"url":28,
"doc":"Model Exponential Moving Average from https: github.com/rwightman/pytorch-image-models Keep a moving average of everything in the model state_dict (parameters and buffers). This is intended to allow functionality like https: www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage A smoothed version of the weights is necessary for some training schemes to perform well. This class is sensitive where it is initialized in the sequence of model init, GPU assignment and distributed training wrappers."
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.ModelEMA.update",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.model.yolov5.utils.torch_utils.ModelEMA.update_attr",
"url":28,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition",
"url":29,
"doc":""
},
{
"ref":"heyvi.recognition.ActivityRecognition",
"url":29,
"doc":""
},
{
"ref":"heyvi.recognition.ActivityRecognition.class_to_index",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.ActivityRecognition.index_to_class",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.ActivityRecognition.classlist",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.ActivityRecognition.num_classes",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.ActivityRecognition.fromindex",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.ActivityRecognition.label_confidence",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.ActivityRecognition.activity",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.ActivityRecognition.top1",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.ActivityRecognition.topk",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.ActivityRecognition.temporal_support",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.ActivityRecognition.totensor",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.ActivityRecognition.binary_vector",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.PIP_250k",
"url":29,
"doc":"Activity recognition using people in public - 250k stabilized"
},
{
"ref":"heyvi.recognition.PIP_250k.dump_patches",
"url":29,
"doc":""
},
{
"ref":"heyvi.recognition.PIP_250k.training",
"url":29,
"doc":""
},
{
"ref":"heyvi.recognition.PIP_250k.category",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.PIP_250k.category_confidence",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.PIP_250k.topk",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.PIP_250k.topk_probability",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.PIP_250k.forward",
"url":29,
"doc":"Same as :meth: torch.nn.Module.forward() , however in Lightning you want this to define the operations you want to use for prediction (i.e.: on a server or as a feature extractor). Normally you'd call  self() from your :meth: training_step method. This makes it easy to write a complex system for training with the outputs you'd want in a prediction setting. You may also find the :func: ~pytorch_lightning.core.decorators.auto_move_data decorator useful when using the module outside Lightning in a production setting. Args:  args: Whatever you decide to pass into the forward method.  kwargs: Keyword arguments are also possible. Return: Predicted output Examples:  code-block python  example if we were using this model as a feature extractor def forward(self, x): feature_maps = self.convnet(x) return feature_maps def training_step(self, batch, batch_idx): x, y = batch feature_maps = self(x) logits = self.classifier(feature_maps)   . return loss  splitting it this way allows model to be used a feature extractor model = MyModelAbove() inputs = server.get_request() results = model(inputs) server.write_results(results)        -  This is in stark contrast to torch.nn.Module where normally you would have this: def forward(self, batch): x, y = batch feature_maps = self.convnet(x) logits = self.classifier(feature_maps) return logits",
"func":1
},
{
"ref":"heyvi.recognition.PIP_250k.configure_optimizers",
"url":29,
"doc":"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. - Single optimizer. - List or Tuple - List of optimizers. - Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict). - Dictionary, with an 'optimizer' key, and (optionally) a 'lr_scheduler' key whose value is a single LR scheduler or lr_dict. - Tuple of dictionaries as described, with an optional 'frequency' key. - None - Fit will run without any optimizer. Note: The 'frequency' value is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: In the former case, all optimizers will operate on the given batch in each optimization step. In the latter, only one optimizer will operate on the given batch at every step. The lr_dict is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below.  code-block python { 'scheduler': lr_scheduler,  The LR scheduler instance (required) 'interval': 'epoch',  The unit of the scheduler's step size 'frequency': 1,  The frequency of the scheduler 'reduce_on_plateau': False,  For ReduceLROnPlateau scheduler 'monitor': 'val_loss',  Metric for ReduceLROnPlateau to monitor 'strict': True,  Whether to crash the training if  monitor is not found 'name': None,  Custom name for LearningRateMonitor to use } Only the  scheduler key is required, the rest will be set to the defaults above. Examples:  code-block python  most cases def configure_optimizers(self): opt = Adam(self.parameters(), lr=1e-3) return opt  multiple optimizer case (e.g.: GAN) def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) return generator_opt, disriminator_opt  example with learning rate schedulers def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) discriminator_sched = CosineAnnealing(discriminator_opt, T_max=10) return [generator_opt, disriminator_opt], [discriminator_sched]  example with step-based learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) gen_sched = {'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step'}  called after each training step dis_sched = CosineAnnealing(discriminator_opt, T_max=10)  called every epoch return [gen_opt, dis_opt], [gen_sched, dis_sched]  example with optimizer frequencies  see training procedure in  Improved Training of Wasserstein GANs , Algorithm 1  https: arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) Note: Some things to know: - Lightning calls  .backward() and  .step() on each optimizer and learning rate scheduler as needed. - If you use 16-bit precision ( precision=16 ), Lightning will automatically handle the optimizers for you. - If you use multiple optimizers, :meth: training_step will have an additional  optimizer_idx parameter. - If you use LBFGS Lightning handles the closure function automatically for you. - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. - If you need to control how often those optimizers step or override the default  .step() schedule, override the :meth: optimizer_step hook. - If you only want to call a learning rate scheduler every  x step or epoch, or want to monitor a custom metric, you can specify these in a lr_dict:  code-block python { 'scheduler': lr_scheduler, 'interval': 'step',  or 'epoch' 'monitor': 'val_f1', 'frequency': x, }",
"func":1
},
{
"ref":"heyvi.recognition.PIP_250k.training_step",
"url":29,
"doc":"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): Integer displaying index of this batch optimizer_idx (int): When using multiple optimizers, this argument will also be present. hiddens(:class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps > 0. Return: Any of. - :class: ~torch.Tensor - The loss tensor -  dict - A dictionary. Can include any keys, but must include the key 'loss' -  None - Training will skip to the next batch In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional  optimizer_idx parameter.  code-block python  Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx  0:  do training_step with encoder if optimizer_idx  1:  do training_step with decoder If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step.  code-block python  Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens):  hiddens are the hidden states from the previous truncated backprop step  . out, hiddens = self.lstm(data, hiddens)  . return {'loss': loss, 'hiddens': hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step.",
"func":1
},
{
"ref":"heyvi.recognition.PIP_250k.validation_step",
"url":29,
"doc":"Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val datasets used) Return: Any of. - Any object or value -  None - Validation will skip to the next batch  code-block python  pseudocode of order out = validation_step() if defined('validation_step_end'): out = validation_step_end(out) out = validation_epoch_end(out)  code-block python  if you have one val dataloader: def validation_step(self, batch, batch_idx)  if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx) Examples:  code-block python  CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch  implement your own out = self(x) loss = self.loss(out, y)  log 6 example images  or generated text . or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0)  calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y  labels_hat).item() / (len(y)  1.0)  log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val datasets, validation_step will have an additional argument.  code-block python  CASE 2: multiple validation datasets def validation_step(self, batch, batch_idx, dataloader_idx):  dataloader_idx tells you which dataset this is. Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled.",
"func":1
},
{
"ref":"heyvi.recognition.PIP_250k.validation_epoch_end",
"url":29,
"doc":"Called at the end of the validation epoch with the outputs of all validation steps.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: outputs: List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note: If you didn't define a :meth: validation_step , this won't be called. Examples: With a single dataloader:  code-block python def validation_epoch_end(self, val_step_outputs): for out in val_step_outputs:  do something With multiple dataloaders,  outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each validation step for that dataloader.  code-block python def validation_epoch_end(self, outputs): for dataloader_output_result in outputs: dataloader_outs = dataloader_output_result.dataloader_i_outputs self.log('final_metric', final_value)",
"func":1
},
{
"ref":"heyvi.recognition.PIP_250k.from_checkpoint",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.PIP_250k.totensor",
"url":29,
"doc":"Return captured lambda function if v=None, else return tensor",
"func":1
},
{
"ref":"heyvi.recognition.PIP_370k",
"url":29,
"doc":"Activity recognition using people in public - 250k stabilized"
},
{
"ref":"heyvi.recognition.PIP_370k.dump_patches",
"url":29,
"doc":""
},
{
"ref":"heyvi.recognition.PIP_370k.training",
"url":29,
"doc":""
},
{
"ref":"heyvi.recognition.PIP_370k.forward",
"url":29,
"doc":"Same as :meth: torch.nn.Module.forward() , however in Lightning you want this to define the operations you want to use for prediction (i.e.: on a server or as a feature extractor). Normally you'd call  self() from your :meth: training_step method. This makes it easy to write a complex system for training with the outputs you'd want in a prediction setting. You may also find the :func: ~pytorch_lightning.core.decorators.auto_move_data decorator useful when using the module outside Lightning in a production setting. Args:  args: Whatever you decide to pass into the forward method.  kwargs: Keyword arguments are also possible. Return: Predicted output Examples:  code-block python  example if we were using this model as a feature extractor def forward(self, x): feature_maps = self.convnet(x) return feature_maps def training_step(self, batch, batch_idx): x, y = batch feature_maps = self(x) logits = self.classifier(feature_maps)   . return loss  splitting it this way allows model to be used a feature extractor model = MyModelAbove() inputs = server.get_request() results = model(inputs) server.write_results(results)        -  This is in stark contrast to torch.nn.Module where normally you would have this: def forward(self, batch): x, y = batch feature_maps = self.convnet(x) logits = self.classifier(feature_maps) return logits",
"func":1
},
{
"ref":"heyvi.recognition.PIP_370k.topk",
"url":29,
"doc":"Return the top-k classes for a 3 second activity proposal along with framewise ground truth",
"func":1
},
{
"ref":"heyvi.recognition.PIP_370k.totensor",
"url":29,
"doc":"Return captured lambda function if v=None, else return tensor",
"func":1
},
{
"ref":"heyvi.recognition.PIP_370k.configure_optimizers",
"url":29,
"doc":"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. - Single optimizer. - List or Tuple - List of optimizers. - Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict). - Dictionary, with an 'optimizer' key, and (optionally) a 'lr_scheduler' key whose value is a single LR scheduler or lr_dict. - Tuple of dictionaries as described, with an optional 'frequency' key. - None - Fit will run without any optimizer. Note: The 'frequency' value is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: In the former case, all optimizers will operate on the given batch in each optimization step. In the latter, only one optimizer will operate on the given batch at every step. The lr_dict is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below.  code-block python { 'scheduler': lr_scheduler,  The LR scheduler instance (required) 'interval': 'epoch',  The unit of the scheduler's step size 'frequency': 1,  The frequency of the scheduler 'reduce_on_plateau': False,  For ReduceLROnPlateau scheduler 'monitor': 'val_loss',  Metric for ReduceLROnPlateau to monitor 'strict': True,  Whether to crash the training if  monitor is not found 'name': None,  Custom name for LearningRateMonitor to use } Only the  scheduler key is required, the rest will be set to the defaults above. Examples:  code-block python  most cases def configure_optimizers(self): opt = Adam(self.parameters(), lr=1e-3) return opt  multiple optimizer case (e.g.: GAN) def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) return generator_opt, disriminator_opt  example with learning rate schedulers def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) discriminator_sched = CosineAnnealing(discriminator_opt, T_max=10) return [generator_opt, disriminator_opt], [discriminator_sched]  example with step-based learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) gen_sched = {'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step'}  called after each training step dis_sched = CosineAnnealing(discriminator_opt, T_max=10)  called every epoch return [gen_opt, dis_opt], [gen_sched, dis_sched]  example with optimizer frequencies  see training procedure in  Improved Training of Wasserstein GANs , Algorithm 1  https: arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) Note: Some things to know: - Lightning calls  .backward() and  .step() on each optimizer and learning rate scheduler as needed. - If you use 16-bit precision ( precision=16 ), Lightning will automatically handle the optimizers for you. - If you use multiple optimizers, :meth: training_step will have an additional  optimizer_idx parameter. - If you use LBFGS Lightning handles the closure function automatically for you. - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. - If you need to control how often those optimizers step or override the default  .step() schedule, override the :meth: optimizer_step hook. - If you only want to call a learning rate scheduler every  x step or epoch, or want to monitor a custom metric, you can specify these in a lr_dict:  code-block python { 'scheduler': lr_scheduler, 'interval': 'step',  or 'epoch' 'monitor': 'val_f1', 'frequency': x, }",
"func":1
},
{
"ref":"heyvi.recognition.PIP_370k.training_step",
"url":29,
"doc":"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): Integer displaying index of this batch optimizer_idx (int): When using multiple optimizers, this argument will also be present. hiddens(:class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps > 0. Return: Any of. - :class: ~torch.Tensor - The loss tensor -  dict - A dictionary. Can include any keys, but must include the key 'loss' -  None - Training will skip to the next batch In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional  optimizer_idx parameter.  code-block python  Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx  0:  do training_step with encoder if optimizer_idx  1:  do training_step with decoder If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step.  code-block python  Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens):  hiddens are the hidden states from the previous truncated backprop step  . out, hiddens = self.lstm(data, hiddens)  . return {'loss': loss, 'hiddens': hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step.",
"func":1
},
{
"ref":"heyvi.recognition.PIP_370k.validation_step",
"url":29,
"doc":"Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val datasets used) Return: Any of. - Any object or value -  None - Validation will skip to the next batch  code-block python  pseudocode of order out = validation_step() if defined('validation_step_end'): out = validation_step_end(out) out = validation_epoch_end(out)  code-block python  if you have one val dataloader: def validation_step(self, batch, batch_idx)  if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx) Examples:  code-block python  CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch  implement your own out = self(x) loss = self.loss(out, y)  log 6 example images  or generated text . or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0)  calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y  labels_hat).item() / (len(y)  1.0)  log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val datasets, validation_step will have an additional argument.  code-block python  CASE 2: multiple validation datasets def validation_step(self, batch, batch_idx, dataloader_idx):  dataloader_idx tells you which dataset this is. Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled.",
"func":1
},
{
"ref":"heyvi.recognition.PIP_370k.validation_epoch_end",
"url":29,
"doc":"Called at the end of the validation epoch with the outputs of all validation steps.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: outputs: List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note: If you didn't define a :meth: validation_step , this won't be called. Examples: With a single dataloader:  code-block python def validation_epoch_end(self, val_step_outputs): for out in val_step_outputs:  do something With multiple dataloaders,  outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each validation step for that dataloader.  code-block python def validation_epoch_end(self, outputs): for dataloader_output_result in outputs: dataloader_outs = dataloader_output_result.dataloader_i_outputs self.log('final_metric', final_value)",
"func":1
},
{
"ref":"heyvi.recognition.CAP",
"url":29,
"doc":"Activity recognition using people in public - 250k stabilized"
},
{
"ref":"heyvi.recognition.CAP.dump_patches",
"url":29,
"doc":""
},
{
"ref":"heyvi.recognition.CAP.training",
"url":29,
"doc":""
},
{
"ref":"heyvi.recognition.CAP.forward",
"url":29,
"doc":"Same as :meth: torch.nn.Module.forward() , however in Lightning you want this to define the operations you want to use for prediction (i.e.: on a server or as a feature extractor). Normally you'd call  self() from your :meth: training_step method. This makes it easy to write a complex system for training with the outputs you'd want in a prediction setting. You may also find the :func: ~pytorch_lightning.core.decorators.auto_move_data decorator useful when using the module outside Lightning in a production setting. Args:  args: Whatever you decide to pass into the forward method.  kwargs: Keyword arguments are also possible. Return: Predicted output Examples:  code-block python  example if we were using this model as a feature extractor def forward(self, x): feature_maps = self.convnet(x) return feature_maps def training_step(self, batch, batch_idx): x, y = batch feature_maps = self(x) logits = self.classifier(feature_maps)   . return loss  splitting it this way allows model to be used a feature extractor model = MyModelAbove() inputs = server.get_request() results = model(inputs) server.write_results(results)        -  This is in stark contrast to torch.nn.Module where normally you would have this: def forward(self, batch): x, y = batch feature_maps = self.convnet(x) logits = self.classifier(feature_maps) return logits",
"func":1
},
{
"ref":"heyvi.recognition.CAP.validation_step",
"url":29,
"doc":"Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val datasets used) Return: Any of. - Any object or value -  None - Validation will skip to the next batch  code-block python  pseudocode of order out = validation_step() if defined('validation_step_end'): out = validation_step_end(out) out = validation_epoch_end(out)  code-block python  if you have one val dataloader: def validation_step(self, batch, batch_idx)  if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx) Examples:  code-block python  CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch  implement your own out = self(x) loss = self.loss(out, y)  log 6 example images  or generated text . or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0)  calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y  labels_hat).item() / (len(y)  1.0)  log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val datasets, validation_step will have an additional argument.  code-block python  CASE 2: multiple validation datasets def validation_step(self, batch, batch_idx, dataloader_idx):  dataloader_idx tells you which dataset this is. Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled.",
"func":1
},
{
"ref":"heyvi.recognition.CAP.validation_step_end",
"url":29,
"doc":"Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code.  code-block python  pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: None or anything  code-block python  WITHOUT validation_step_end  if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx):  batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log('val_loss', loss)          with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx):  batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs:  do something with these See Also: See the :ref: multi_gpu guide for more details.",
"func":1
},
{
"ref":"heyvi.recognition.CAP.validation_epoch_end",
"url":29,
"doc":"Called at the end of the validation epoch with the outputs of all validation steps.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: outputs: List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note: If you didn't define a :meth: validation_step , this won't be called. Examples: With a single dataloader:  code-block python def validation_epoch_end(self, val_step_outputs): for out in val_step_outputs:  do something With multiple dataloaders,  outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each validation step for that dataloader.  code-block python def validation_epoch_end(self, outputs): for dataloader_output_result in outputs: dataloader_outs = dataloader_output_result.dataloader_i_outputs self.log('final_metric', final_value)",
"func":1
},
{
"ref":"heyvi.recognition.CAP.totensor",
"url":29,
"doc":"Return captured lambda function if v=None, else return tensor",
"func":1
},
{
"ref":"heyvi.recognition.CAP.calibration",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.CAP.topk",
"url":29,
"doc":"Return the top-k classes for a 3 second activity proposal along with framewise ground truth",
"func":1
},
{
"ref":"heyvi.recognition.CAP.configure_optimizers",
"url":29,
"doc":"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. - Single optimizer. - List or Tuple - List of optimizers. - Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict). - Dictionary, with an 'optimizer' key, and (optionally) a 'lr_scheduler' key whose value is a single LR scheduler or lr_dict. - Tuple of dictionaries as described, with an optional 'frequency' key. - None - Fit will run without any optimizer. Note: The 'frequency' value is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: In the former case, all optimizers will operate on the given batch in each optimization step. In the latter, only one optimizer will operate on the given batch at every step. The lr_dict is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below.  code-block python { 'scheduler': lr_scheduler,  The LR scheduler instance (required) 'interval': 'epoch',  The unit of the scheduler's step size 'frequency': 1,  The frequency of the scheduler 'reduce_on_plateau': False,  For ReduceLROnPlateau scheduler 'monitor': 'val_loss',  Metric for ReduceLROnPlateau to monitor 'strict': True,  Whether to crash the training if  monitor is not found 'name': None,  Custom name for LearningRateMonitor to use } Only the  scheduler key is required, the rest will be set to the defaults above. Examples:  code-block python  most cases def configure_optimizers(self): opt = Adam(self.parameters(), lr=1e-3) return opt  multiple optimizer case (e.g.: GAN) def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) return generator_opt, disriminator_opt  example with learning rate schedulers def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) discriminator_sched = CosineAnnealing(discriminator_opt, T_max=10) return [generator_opt, disriminator_opt], [discriminator_sched]  example with step-based learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) gen_sched = {'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step'}  called after each training step dis_sched = CosineAnnealing(discriminator_opt, T_max=10)  called every epoch return [gen_opt, dis_opt], [gen_sched, dis_sched]  example with optimizer frequencies  see training procedure in  Improved Training of Wasserstein GANs , Algorithm 1  https: arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) Note: Some things to know: - Lightning calls  .backward() and  .step() on each optimizer and learning rate scheduler as needed. - If you use 16-bit precision ( precision=16 ), Lightning will automatically handle the optimizers for you. - If you use multiple optimizers, :meth: training_step will have an additional  optimizer_idx parameter. - If you use LBFGS Lightning handles the closure function automatically for you. - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. - If you need to control how often those optimizers step or override the default  .step() schedule, override the :meth: optimizer_step hook. - If you only want to call a learning rate scheduler every  x step or epoch, or want to monitor a custom metric, you can specify these in a lr_dict:  code-block python { 'scheduler': lr_scheduler, 'interval': 'step',  or 'epoch' 'monitor': 'val_f1', 'frequency': x, }",
"func":1
},
{
"ref":"heyvi.recognition.CAP.training_step",
"url":29,
"doc":"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): Integer displaying index of this batch optimizer_idx (int): When using multiple optimizers, this argument will also be present. hiddens(:class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps > 0. Return: Any of. - :class: ~torch.Tensor - The loss tensor -  dict - A dictionary. Can include any keys, but must include the key 'loss' -  None - Training will skip to the next batch In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional  optimizer_idx parameter.  code-block python  Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx  0:  do training_step with encoder if optimizer_idx  1:  do training_step with decoder If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step.  code-block python  Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens):  hiddens are the hidden states from the previous truncated backprop step  . out, hiddens = self.lstm(data, hiddens)  . return {'loss': loss, 'hiddens': hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step.",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTracker",
"url":29,
"doc":"Video Activity detection. Args (__call__): vi [generator of  vipy.video.Scene ]: The input video to be updated in place with detections. This is a generator which is output from heyvi.detection.MultiscaleVideoTracker.__call__ activityiou [float]: The minimum temporal iou for activity assignment mirror [bool]: If true, encode using the mean of a video encoding and the mirrored video encoding. This is slower as it requires 2x GPU forward passes minprob [float]: The minimum probability for new activity detection trackconf [float]: The minimum object detection confidence for new tracks maxdets [int]: The maximum number of allowable detections per frame. If there are more detections per frame tha maxdets, sort them by confidence and use only the top maxdets best avgdets [int]: The number of allowable detections per frame if throttled buffered [bool]: If true, then buffer streams. This is useful for activity detection on live streams. finalized [bool, int]: If False then do not finalize(), If True finalize() only at the end, If int, then finalize every int frames. This is useful for streaming activity detection on unbounded inputs. Returns: The input video is updated in place."
},
{
"ref":"heyvi.recognition.ActivityTracker.dump_patches",
"url":29,
"doc":""
},
{
"ref":"heyvi.recognition.ActivityTracker.training",
"url":29,
"doc":""
},
{
"ref":"heyvi.recognition.ActivityTracker.temporal_stride",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTracker.forward",
"url":29,
"doc":"Overload forward for multi-gpu batch. Don't use torch DataParallel!",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTracker.lrt",
"url":29,
"doc":"top-k with likelihood ratio test with background null hypothesis",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTracker.logit_pooling",
"url":29,
"doc":"",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTracker.softmax",
"url":29,
"doc":"Return a list of lists [(class_label, float(softmax), float(logit)  . ] for all classes and batches",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTracker.finalize",
"url":29,
"doc":"In place filtering of video to finalize",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTracker.topk",
"url":29,
"doc":"Return the top-k classes for a 3 second activity proposal along with framewise ground truth",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTracker.totensor",
"url":29,
"doc":"Return captured lambda function if v=None, else return tensor",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTracker.configure_optimizers",
"url":29,
"doc":"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. - Single optimizer. - List or Tuple - List of optimizers. - Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict). - Dictionary, with an 'optimizer' key, and (optionally) a 'lr_scheduler' key whose value is a single LR scheduler or lr_dict. - Tuple of dictionaries as described, with an optional 'frequency' key. - None - Fit will run without any optimizer. Note: The 'frequency' value is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: In the former case, all optimizers will operate on the given batch in each optimization step. In the latter, only one optimizer will operate on the given batch at every step. The lr_dict is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below.  code-block python { 'scheduler': lr_scheduler,  The LR scheduler instance (required) 'interval': 'epoch',  The unit of the scheduler's step size 'frequency': 1,  The frequency of the scheduler 'reduce_on_plateau': False,  For ReduceLROnPlateau scheduler 'monitor': 'val_loss',  Metric for ReduceLROnPlateau to monitor 'strict': True,  Whether to crash the training if  monitor is not found 'name': None,  Custom name for LearningRateMonitor to use } Only the  scheduler key is required, the rest will be set to the defaults above. Examples:  code-block python  most cases def configure_optimizers(self): opt = Adam(self.parameters(), lr=1e-3) return opt  multiple optimizer case (e.g.: GAN) def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) return generator_opt, disriminator_opt  example with learning rate schedulers def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) discriminator_sched = CosineAnnealing(discriminator_opt, T_max=10) return [generator_opt, disriminator_opt], [discriminator_sched]  example with step-based learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) gen_sched = {'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step'}  called after each training step dis_sched = CosineAnnealing(discriminator_opt, T_max=10)  called every epoch return [gen_opt, dis_opt], [gen_sched, dis_sched]  example with optimizer frequencies  see training procedure in  Improved Training of Wasserstein GANs , Algorithm 1  https: arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) Note: Some things to know: - Lightning calls  .backward() and  .step() on each optimizer and learning rate scheduler as needed. - If you use 16-bit precision ( precision=16 ), Lightning will automatically handle the optimizers for you. - If you use multiple optimizers, :meth: training_step will have an additional  optimizer_idx parameter. - If you use LBFGS Lightning handles the closure function automatically for you. - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. - If you need to control how often those optimizers step or override the default  .step() schedule, override the :meth: optimizer_step hook. - If you only want to call a learning rate scheduler every  x step or epoch, or want to monitor a custom metric, you can specify these in a lr_dict:  code-block python { 'scheduler': lr_scheduler, 'interval': 'step',  or 'epoch' 'monitor': 'val_f1', 'frequency': x, }",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTracker.training_step",
"url":29,
"doc":"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): Integer displaying index of this batch optimizer_idx (int): When using multiple optimizers, this argument will also be present. hiddens(:class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps > 0. Return: Any of. - :class: ~torch.Tensor - The loss tensor -  dict - A dictionary. Can include any keys, but must include the key 'loss' -  None - Training will skip to the next batch In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional  optimizer_idx parameter.  code-block python  Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx  0:  do training_step with encoder if optimizer_idx  1:  do training_step with decoder If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step.  code-block python  Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens):  hiddens are the hidden states from the previous truncated backprop step  . out, hiddens = self.lstm(data, hiddens)  . return {'loss': loss, 'hiddens': hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step.",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTracker.validation_step",
"url":29,
"doc":"Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val datasets used) Return: Any of. - Any object or value -  None - Validation will skip to the next batch  code-block python  pseudocode of order out = validation_step() if defined('validation_step_end'): out = validation_step_end(out) out = validation_epoch_end(out)  code-block python  if you have one val dataloader: def validation_step(self, batch, batch_idx)  if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx) Examples:  code-block python  CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch  implement your own out = self(x) loss = self.loss(out, y)  log 6 example images  or generated text . or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0)  calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y  labels_hat).item() / (len(y)  1.0)  log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val datasets, validation_step will have an additional argument.  code-block python  CASE 2: multiple validation datasets def validation_step(self, batch, batch_idx, dataloader_idx):  dataloader_idx tells you which dataset this is. Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled.",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTracker.validation_epoch_end",
"url":29,
"doc":"Called at the end of the validation epoch with the outputs of all validation steps.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: outputs: List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note: If you didn't define a :meth: validation_step , this won't be called. Examples: With a single dataloader:  code-block python def validation_epoch_end(self, val_step_outputs): for out in val_step_outputs:  do something With multiple dataloaders,  outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each validation step for that dataloader.  code-block python def validation_epoch_end(self, outputs): for dataloader_output_result in outputs: dataloader_outs = dataloader_output_result.dataloader_i_outputs self.log('final_metric', final_value)",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTrackerCap",
"url":29,
"doc":"Video Activity detection. Args (__call__): vi [generator of  vipy.video.Scene ]: The input video to be updated in place with detections. This is a generator which is output from heyvi.detection.MultiscaleVideoTracker.__call__ activityiou [float]: The minimum temporal iou for activity assignment mirror [bool]: If true, encode using the mean of a video encoding and the mirrored video encoding. This is slower as it requires 2x GPU forward passes minprob [float]: The minimum probability for new activity detection trackconf [float]: The minimum object detection confidence for new tracks maxdets [int]: The maximum number of allowable detections per frame. If there are more detections per frame tha maxdets, sort them by confidence and use only the top maxdets best avgdets [int]: The number of allowable detections per frame if throttled buffered [bool]: If true, then buffer streams. This is useful for activity detection on live streams. finalized [bool, int]: If False then do not finalize(), If True finalize() only at the end, If int, then finalize every int frames. This is useful for streaming activity detection on unbounded inputs. Returns: The input video is updated in place."
},
{
"ref":"heyvi.recognition.ActivityTrackerCap.dump_patches",
"url":29,
"doc":""
},
{
"ref":"heyvi.recognition.ActivityTrackerCap.training",
"url":29,
"doc":""
},
{
"ref":"heyvi.recognition.ActivityTrackerCap.forward",
"url":29,
"doc":"Overload forward for multi-gpu batch. Don't use torch DataParallel!",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTrackerCap.lrt",
"url":29,
"doc":"top-k with likelihood ratio test with background null hypothesis",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTrackerCap.softmax",
"url":29,
"doc":"Return a list of lists [(class_label, float(softmax), float(logit)  . ] for all classes and batches",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTrackerCap.finalize",
"url":29,
"doc":"In place filtering of video to finalize",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTrackerCap.topk",
"url":29,
"doc":"Return the top-k classes for a 3 second activity proposal along with framewise ground truth",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTrackerCap.totensor",
"url":29,
"doc":"Return captured lambda function if v=None, else return tensor",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTrackerCap.configure_optimizers",
"url":29,
"doc":"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. - Single optimizer. - List or Tuple - List of optimizers. - Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict). - Dictionary, with an 'optimizer' key, and (optionally) a 'lr_scheduler' key whose value is a single LR scheduler or lr_dict. - Tuple of dictionaries as described, with an optional 'frequency' key. - None - Fit will run without any optimizer. Note: The 'frequency' value is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: In the former case, all optimizers will operate on the given batch in each optimization step. In the latter, only one optimizer will operate on the given batch at every step. The lr_dict is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below.  code-block python { 'scheduler': lr_scheduler,  The LR scheduler instance (required) 'interval': 'epoch',  The unit of the scheduler's step size 'frequency': 1,  The frequency of the scheduler 'reduce_on_plateau': False,  For ReduceLROnPlateau scheduler 'monitor': 'val_loss',  Metric for ReduceLROnPlateau to monitor 'strict': True,  Whether to crash the training if  monitor is not found 'name': None,  Custom name for LearningRateMonitor to use } Only the  scheduler key is required, the rest will be set to the defaults above. Examples:  code-block python  most cases def configure_optimizers(self): opt = Adam(self.parameters(), lr=1e-3) return opt  multiple optimizer case (e.g.: GAN) def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) return generator_opt, disriminator_opt  example with learning rate schedulers def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) discriminator_sched = CosineAnnealing(discriminator_opt, T_max=10) return [generator_opt, disriminator_opt], [discriminator_sched]  example with step-based learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) gen_sched = {'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step'}  called after each training step dis_sched = CosineAnnealing(discriminator_opt, T_max=10)  called every epoch return [gen_opt, dis_opt], [gen_sched, dis_sched]  example with optimizer frequencies  see training procedure in  Improved Training of Wasserstein GANs , Algorithm 1  https: arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) Note: Some things to know: - Lightning calls  .backward() and  .step() on each optimizer and learning rate scheduler as needed. - If you use 16-bit precision ( precision=16 ), Lightning will automatically handle the optimizers for you. - If you use multiple optimizers, :meth: training_step will have an additional  optimizer_idx parameter. - If you use LBFGS Lightning handles the closure function automatically for you. - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. - If you need to control how often those optimizers step or override the default  .step() schedule, override the :meth: optimizer_step hook. - If you only want to call a learning rate scheduler every  x step or epoch, or want to monitor a custom metric, you can specify these in a lr_dict:  code-block python { 'scheduler': lr_scheduler, 'interval': 'step',  or 'epoch' 'monitor': 'val_f1', 'frequency': x, }",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTrackerCap.training_step",
"url":29,
"doc":"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): Integer displaying index of this batch optimizer_idx (int): When using multiple optimizers, this argument will also be present. hiddens(:class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps > 0. Return: Any of. - :class: ~torch.Tensor - The loss tensor -  dict - A dictionary. Can include any keys, but must include the key 'loss' -  None - Training will skip to the next batch In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional  optimizer_idx parameter.  code-block python  Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx  0:  do training_step with encoder if optimizer_idx  1:  do training_step with decoder If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step.  code-block python  Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens):  hiddens are the hidden states from the previous truncated backprop step  . out, hiddens = self.lstm(data, hiddens)  . return {'loss': loss, 'hiddens': hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step.",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTrackerCap.validation_step",
"url":29,
"doc":"Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val datasets used) Return: Any of. - Any object or value -  None - Validation will skip to the next batch  code-block python  pseudocode of order out = validation_step() if defined('validation_step_end'): out = validation_step_end(out) out = validation_epoch_end(out)  code-block python  if you have one val dataloader: def validation_step(self, batch, batch_idx)  if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx) Examples:  code-block python  CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch  implement your own out = self(x) loss = self.loss(out, y)  log 6 example images  or generated text . or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0)  calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y  labels_hat).item() / (len(y)  1.0)  log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val datasets, validation_step will have an additional argument.  code-block python  CASE 2: multiple validation datasets def validation_step(self, batch, batch_idx, dataloader_idx):  dataloader_idx tells you which dataset this is. Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled.",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTrackerCap.validation_epoch_end",
"url":29,
"doc":"Called at the end of the validation epoch with the outputs of all validation steps.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: outputs: List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note: If you didn't define a :meth: validation_step , this won't be called. Examples: With a single dataloader:  code-block python def validation_epoch_end(self, val_step_outputs): for out in val_step_outputs:  do something With multiple dataloaders,  outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each validation step for that dataloader.  code-block python def validation_epoch_end(self, outputs): for dataloader_output_result in outputs: dataloader_outs = dataloader_output_result.dataloader_i_outputs self.log('final_metric', final_value)",
"func":1
},
{
"ref":"heyvi.recognition.ActivityTrackerCap.validation_step_end",
"url":29,
"doc":"Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code.  code-block python  pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: None or anything  code-block python  WITHOUT validation_step_end  if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx):  batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log('val_loss', loss)          with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx):  batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs:  do something with these See Also: See the :ref: multi_gpu guide for more details.",
"func":1
},
{
"ref":"heyvi.sensor",
"url":30,
"doc":""
},
{
"ref":"heyvi.sensor.rtsp",
"url":30,
"doc":"Return an RTSP camera. >>> v = heyvi.sensor.rtsp() >>> im = v.preview().show().saveas('out.jpg') >>> for im in v: >>> print(im)  live stream >>> print(im.numpy(  of numpy frames Args: url: [str] The URL for the rtsp camera, must start with 'rtsp: ' fps: [float] The framerate of the returned camera, can also be set after Env: VIPY_RTSP_URL: If this environment variable is set, use this as the URL that contains integrated credentials",
"func":1
},
{
"ref":"heyvi.sensor.camera",
"url":30,
"doc":"Return RSTP camera with index n in cameralist, or the default RTSP camera if None",
"func":1
},
{
"ref":"heyvi.sensor.cameralist",
"url":30,
"doc":"Return all online RTSP cameras set up on the current network. This requires setting environment variables: VIPY_RTSP_URL_0='rtsp: user:passwd@ip.addr.0' VIPY_RTSP_URL_1='rtsp: user:passwd@ip.addr.1' VIPY_RTSP_URL_2='rtsp: user:passwd@ip.addr.2' Args: online: [bool]: If True, return only those cameras that are online. If a camera is offline return None in that camera index. If false, return all cameras specified by the environment variables.",
"func":1
},
{
"ref":"heyvi.system",
"url":31,
"doc":""
},
{
"ref":"heyvi.system.YoutubeLive",
"url":31,
"doc":"Youtube Live stream. >>> Y = heyvi.system.YoutubeLive(encoder='480p') >>> v = heyvi.sensor.rtsp() >>> Y(v) Args: encoder [str]['480p, '720p', '360p']: The encoder settings for the youtube live stream fps [float]: The framerate in frames per second of the output stream. streamkey [str]: The youtube live key (https: support.google.com/youtube/answer/9854503?hl=en), or set as envronment variable VIPY_YOUTUBE_STREAMKEY"
},
{
"ref":"heyvi.system.Recorder",
"url":31,
"doc":"Record a livestream to an output video file This will record an out streaming to the provided outfile >>> v = vipy.video.Scene(url='rtsp:  .', framerate=30) >>> R = Recorder('/tmp/out.mp4', framerate=5) >>> R(v, seconds=60 60) To buffer to memory, you do not need this recorder, use (for small durations): >>> v = v.duration(seconds=3).load().saveas('/tmp/out.mp4') This will record three seconds from the provided RTSP stream and save in the usual way to the output file To record frame by frame: >>> v = vipy.video.RandomScene() >>> with Recorder('out.mp4') as r: >>> for im in v: >>> r(im.annotate().rgb(  write individual frames from video v"
},
{
"ref":"heyvi.system.Tracker",
"url":31,
"doc":"heyvi.system.Tracker class To run on a livestream:   v = heyvi.sensor.rtsp() T = heyvi.system.Tracker() with heyvi.system.YoutubeLive(fps=5, encoder='480p') as s: T(v, frame_callback=lambda im: s(im.pixelize().annotate(fontsize=15, timestamp=heyvi.util.timestamp(), timestampoffset=(6,10 ), minconf=0.5)   To run on an input file as a batch:   v = vipy.video.Scene(filename=/path/to/infile.mp4', framerate=5) T = heyvi.system.Tracker() v_tracked = T(v) v_tracked.annotate('annotation.mp4')   To stream tracks computed per frame   vi = vipy.video.Scene(filename=/path/to/infile.mp4', framerate=5) for (f,vo) in enumerate(T.stream(vi : print(vo)  tracking result at frame f   To stream tracks computed per frame, along with pixels for current frame   vi = vipy.video.Scene(filename=/path/to/infile.mp4', framerate=5) for (f,(im,vo in enumerate(zip(vi, T.stream(vi ) print(vo)  tracking result at frame f print(im)   vipy.image.Image with pixels available as im.numpy()   To stream tracks computed per frame, along with the most recent video clip of length 16:   vi = vipy.video.Scene(filename=/path/to/infile.mp4', framerate=5) for (f,(vc,vo in enumerate(zip(vi.stream().clip(16), T.stream(vi ) print(vo)  tracking result at frame f print(vc)   vipy.video.Scene with pixels for clips of length 16   For additional use cases for streaming batches, clips, frames, delays see the [vipy documentation](https: visym.github.io/vipy) Returns:  vipy.video.Scene objects with tracks corresponding to objects in  heyvi.detection.MultiscaleVideoTracker.classlist . Object tracks are \"person\", \"vehicle\", \"bicycle\"."
},
{
"ref":"heyvi.system.Tracker.stream",
"url":31,
"doc":"Tracking iterator of input video",
"func":1
},
{
"ref":"heyvi.system.Actev21",
"url":31,
"doc":"heyvi.system.Actev21 class Real time activity detection for the 37 MEVA (https: mevadata.org) activity classes >>> v = heyvi.sensor.rtsp().framerate(5) >>> S = heyvi.system.Actev21() >>> with heyvi.system.YoutubeLive(fps=5, encoder='480p') as s: >>> S(v, frame_callback=lambda im, imraw, v: s(im), minconf=0.2)"
},
{
"ref":"heyvi.system.Actev21.annotate",
"url":31,
"doc":"",
"func":1
},
{
"ref":"heyvi.util",
"url":32,
"doc":""
},
{
"ref":"heyvi.util.timestamp",
"url":32,
"doc":"Datetime stamp in eastern timezone with microsecond resolution",
"func":1
},
{
"ref":"heyvi.version",
"url":33,
"doc":""
},
{
"ref":"heyvi.version.num",
"url":33,
"doc":"Convert the version string of the form 'X.Y.Z' to an integer 100000 X + 100 Y + Z for version comparison",
"func":1
},
{
"ref":"heyvi.version.split",
"url":33,
"doc":"Split the version string 'X.Y.Z' and return tuple (int(X), int(Y), int(Z ",
"func":1
},
{
"ref":"heyvi.version.major",
"url":33,
"doc":"Return the major version number int(X) for versionstring 'X.Y.Z'",
"func":1
},
{
"ref":"heyvi.version.minor",
"url":33,
"doc":"Return the minor version number int(Y) for versionstring 'X.Y.Z'",
"func":1
},
{
"ref":"heyvi.version.release",
"url":33,
"doc":"Return the release version number int(Z) for versionstring 'X.Y.Z'",
"func":1
},
{
"ref":"heyvi.version.at_least_version",
"url":33,
"doc":"Is versionstring='X.Y.Z' at least the current version?",
"func":1
},
{
"ref":"heyvi.version.is_at_least",
"url":33,
"doc":"Synonym for at_least_version",
"func":1
},
{
"ref":"heyvi.version.is_exactly",
"url":33,
"doc":"Is the versionstring = 'X,Y.Z' exactly equal to heyvi.__version__",
"func":1
},
{
"ref":"heyvi.version.at_least_major_version",
"url":33,
"doc":"is the major version (e.g. X, for version X.Y.Z) greater than or equal to the major version integer supplied?",
"func":1
}
]