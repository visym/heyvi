<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>heyvi.recognition API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>heyvi.recognition</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L0-L704" class="git-link">Browse git</a>
</summary>
<pre><code class="python">import os
import sys
import random
import torch
import vipy
import vipy.data.meva
import shutil
import numpy as np
from vipy.util import remkdir, filetail, readlist, tolist, filepath
from datetime import datetime
from heyvi.model.yolov3.network import Darknet
import vipy.activity
import itertools
import copy
import os
import torch
from torch import nn
import torch.nn.functional as F
from torchvision.datasets import MNIST
import torch.utils.data
from torch.utils.data import DataLoader, random_split, Dataset
from torchvision import transforms
import pytorch_lightning as pl
import json
import math
import heyvi.label
import heyvi.model.ResNets_3D_PyTorch.resnet


class ActivityRecognition(object):
    def __init__(self, pretrained=True):
        self.net =  None
        self._class_to_index = {}
        self._num_frames = 0

    def class_to_index(self, c=None):
        return self._class_to_index if c is None else self._class_to_index[c]
    
    def index_to_class(self, index=None):
        d = {v:k for (k,v) in self.class_to_index().items()}
        return d if index is None else d[index]
    
    def classlist(self):
        return [k for (k,v) in sorted(list(self.class_to_index().items()), key=lambda x: x[0])]  # sorted in index order

    def num_classes(self):
        return len(self.classlist())

    def fromindex(self, k):
        index_to_class = {v:k for (k,v) in self.class_to_index().items()}
        assert k in index_to_class, &#34;Invalid class index &#39;%s&#39;&#34; % (str(k))
        return index_to_class[k]

    def label_confidence(self, video=None, tensor=None, threshold=None):
        raise
        logits = self.__call__(video, tensor)
        conf = [[(self.index_to_class(j), s[j]) for j in i[::-1] if threshold is None or s[j]&gt;threshold] for (s,i) in zip(logits, np.argsort(logits, axis=1))]
        return conf if len(logits) &gt; 1 else conf[0]

    def activity(self, video, threshold=None):
        (c,s) = zip(*self.label_confidence(video=video, threshold=None))
        return vipy.activity.Activity(startframe=0, endframe=self._num_frames, category=c[0], actorid=video.actorid(), confidence=s[0]) if (threshold is None or s[0]&gt;threshold) else None
            
    def top1(self, video=None, tensor=None, threshold=None):
        raise
        return self.topk(k=1, video=video, tensor=tensor, threshold=threshold)

    def topk(self, k, video=None, tensor=None, threshold=None):
        raise
        logits = self.__call__(video, tensor)
        topk = [[self.index_to_class(j) for j in i[-k:][::-1] if threshold is None or s[j] &gt;= threshold] for (s,i) in zip(logits, np.argsort(logits, axis=1))]
        return topk if len(topk) &gt; 1 else topk[0]

    def temporal_support(self):
        return self._num_frames

    def totensor(self, training=False):
        raise

    def binary_vector(self, categories):
        y = np.zeros(len(self.classlist())).astype(np.float32)
        for c in tolist(categories):
            y[self.class_to_index(c)] = 1
        return torch.from_numpy(y).type(torch.FloatTensor)
        
    
    
class PIP_250k(pl.LightningModule, ActivityRecognition):
    &#34;&#34;&#34;Activity recognition using people in public - 250k stabilized&#34;&#34;&#34;
    
    def __init__(self, pretrained=True, deterministic=False, modelfile=None, mlbl=False, mlfl=False):

        # FIXME: remove dependencies here
        from heyvi.model.pyvideoresearch.bases.resnet50_3d import ResNet503D, ResNet3D, Bottleneck3D
        import heyvi.model.ResNets_3D_PyTorch.resnet

        
        super().__init__()
        self._input_size = 112
        self._num_frames = 16        
        self._mean = [0.485, 0.456, 0.406]
        self._std = [0.229, 0.224, 0.225]
        self._mlfl = mlfl
        self._mlbl = mlbl

        if deterministic:
            np.random.seed(42)

        self._class_to_weight = {&#39;car_drops_off_person&#39;: 1.4162811344926518, &#39;car_picks_up_person&#39;: 1.4103618337303332, &#39;car_reverses&#39;: 1.0847976470131024, &#39;car_starts&#39;: 1.0145749063037774, &#39;car_stops&#39;: 0.6659236295324015, &#39;car_turns_left&#39;: 2.942269221156227, &#39;car_turns_right&#39;: 1.1077783089040996, &#39;hand_interacts_with_person_highfive&#39;: 2.793646013249904, &#39;person&#39;: 0.4492053391155403, &#39;person_abandons_object&#39;: 1.0944029463871692, &#39;person_carries_heavy_object&#39;: 0.5848339202761978, &#39;person_closes_car_door&#39;: 0.8616907697519004, &#39;person_closes_car_trunk&#39;: 1.468393359799126, &#39;person_closes_facility_door&#39;: 0.8927495923340439, &#39;person_embraces_person&#39;: 0.6072654081071569, &#39;person_enters_car&#39;: 1.3259274145537951, &#39;person_enters_scene_through_structure&#39;: 0.6928103470838287, &#39;person_exits_car&#39;: 1.6366577285051707, &#39;person_exits_scene_through_structure&#39;: 0.8368692178634396, &#39;person_holds_hand&#39;: 1.2378881634203558, &#39;person_interacts_with_laptop&#39;: 1.6276031281396193, &#39;person_loads_car&#39;: 2.170167410167583, &#39;person_opens_car_door&#39;: 0.7601817241565009, &#39;person_opens_car_trunk&#39;: 1.7255285914206204, &#39;person_opens_facility_door&#39;: 0.9167411017455822, &#39;person_picks_up_object_from_floor&#39;: 1.123251610875369, &#39;person_picks_up_object_from_table&#39;: 3.5979689180114205, &#39;person_purchases_from_cashier&#39;: 7.144918373837205, &#39;person_purchases_from_machine&#39;: 5.920886403645001, &#39;person_puts_down_object_on_floor&#39;: 0.7295795950752353, &#39;person_puts_down_object_on_shelf&#39;: 9.247614426653692, &#39;person_puts_down_object_on_table&#39;: 1.9884672074906158, &#39;person_reads_document&#39;: 0.7940480628992879, &#39;person_rides_bicycle&#39;: 2.662661823600623, &#39;person_shakes_hand&#39;: 0.7819547332927879, &#39;person_sits_down&#39;: 0.8375202893491961, &#39;person_stands_up&#39;: 1.0285510019795079, &#39;person_steals_object_from_person&#39;: 1.0673909796893626, &#39;person_talks_on_phone&#39;: 0.3031855242664589, &#39;person_talks_to_person&#39;: 0.334895684562076, &#39;person_texts_on_phone&#39;: 0.713951043919232, &#39;person_transfers_object_to_car&#39;: 3.2832615561297605, &#39;person_transfers_object_to_person&#39;: 0.9633429807282274, &#39;person_unloads_car&#39;: 1.1051597100801462, &#39;vehicle&#39;: 1.1953172363332243}
        self._class_to_weight[&#39;person_puts_down_object_on_shelf&#39;] = 1.0   # run 5

        self._class_to_index = {&#39;car_drops_off_person&#39;: 0, &#39;car_picks_up_person&#39;: 1, &#39;car_reverses&#39;: 2, &#39;car_starts&#39;: 3, &#39;car_stops&#39;: 4, &#39;car_turns_left&#39;: 5, &#39;car_turns_right&#39;: 6, &#39;hand_interacts_with_person_highfive&#39;: 7, &#39;person&#39;: 8, &#39;person_abandons_object&#39;: 9, &#39;person_carries_heavy_object&#39;: 10, &#39;person_closes_car_door&#39;: 11, &#39;person_closes_car_trunk&#39;: 12, &#39;person_closes_facility_door&#39;: 13, &#39;person_embraces_person&#39;: 14, &#39;person_enters_car&#39;: 15, &#39;person_enters_scene_through_structure&#39;: 16, &#39;person_exits_car&#39;: 17, &#39;person_exits_scene_through_structure&#39;: 18, &#39;person_holds_hand&#39;: 19, &#39;person_interacts_with_laptop&#39;: 20, &#39;person_loads_car&#39;: 21, &#39;person_opens_car_door&#39;: 22, &#39;person_opens_car_trunk&#39;: 23, &#39;person_opens_facility_door&#39;: 24, &#39;person_picks_up_object_from_floor&#39;: 25, &#39;person_picks_up_object_from_table&#39;: 26, &#39;person_purchases_from_cashier&#39;: 27, &#39;person_purchases_from_machine&#39;: 28, &#39;person_puts_down_object_on_floor&#39;: 29, &#39;person_puts_down_object_on_shelf&#39;: 30, &#39;person_puts_down_object_on_table&#39;: 31, &#39;person_reads_document&#39;: 32, &#39;person_rides_bicycle&#39;: 33, &#39;person_shakes_hand&#39;: 34, &#39;person_sits_down&#39;: 35, &#39;person_stands_up&#39;: 36, &#39;person_steals_object_from_person&#39;: 37, &#39;person_talks_on_phone&#39;: 38, &#39;person_talks_to_person&#39;: 39, &#39;person_texts_on_phone&#39;: 40, &#39;person_transfers_object_to_car&#39;: 41, &#39;person_transfers_object_to_person&#39;: 42, &#39;person_unloads_car&#39;: 43, &#39;vehicle&#39;: 44}

        self._verb_to_noun = {k:set([&#39;car&#39;,&#39;vehicle&#39;,&#39;motorcycle&#39;,&#39;bus&#39;,&#39;truck&#39;]) if (k.startswith(&#39;car&#39;) or k.startswith(&#39;motorcycle&#39;) or k.startswith(&#39;vehicle&#39;)) else set([&#39;person&#39;]) for k in self.classlist()}        
        self._class_to_shortlabel = pycollector.label.pip_to_shortlabel  # FIXME: remove dependency here

        if pretrained:
            self._load_pretrained()
            self.net.fc = nn.Linear(self.net.fc.in_features, self.num_classes())
        elif modelfile is not None:
            self._load_trained(modelfile)
        
    def category(self, x):
        yh = self.forward(x if x.ndim == 5 else torch.unsqueeze(x, 0))
        return [self.index_to_class(int(k)) for (c,k) in zip(*torch.max(yh, dim=1))]

    def category_confidence(self, x):
        yh = self.forward(x if x.ndim == 5 else torch.unsqueeze(x, 0))
        return [(self.index_to_class(int(k)), float(c)) for (c,k) in zip(*torch.max(yh, dim=1))]

    def topk(self, x_logits, k):
        yh = x_logits.detach().cpu().numpy()
        topk = [[(self.index_to_class(j), s[j]) for j in i[-k:][::-1]] for (s,i) in zip(yh, np.argsort(yh, axis=1))]
        return topk

    def topk_probability(self, x_logits, k):
        yh = x_logits.detach().cpu().numpy()
        yh_prob = F.softmax(x_logits, dim=1).detach().cpu().numpy()
        topk = [[(self.index_to_class(j), c[j], p[j]) for j in i[-k:][::-1]] for (c,p,i) in zip(yh, yh_prob, np.argsort(yh, axis=1))]
        return topk
        
    # ---- &lt;LIGHTNING&gt;
    def forward(self, x):
        return self.net(x)  # lighting handles device

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)
        return optimizer

    def training_step(self, batch, batch_nb, logging=True, valstep=False):
        (x,Y) = batch  
        y_hat = self.forward(x)
        y_hat_softmax = F.softmax(y_hat, dim=1)

        (loss, n_valid) = (0, 0)
        C = torch.tensor([self._class_to_weight[k] for (k,v) in sorted(self._class_to_index.items(), key=lambda x: x[1])], device=y_hat.device)  # inverse class frequency        
        for (yh, yhs, labelstr) in zip(y_hat, y_hat_softmax, Y):
            labels = json.loads(labelstr)
            if labels is None:
                continue  # skip me
            lbllist = [l for lbl in labels for l in lbl]  # list of multi-labels within clip (unpack from JSON to use default collate_fn)
            lbl_frequency = vipy.util.countby(lbllist, lambda x: x)  # frequency within clip
            lbl_weight = {k:v/float(len(lbllist)) for (k,v) in lbl_frequency.items()}  # multi-label likelihood within clip, sums to one
            for (y,w) in lbl_weight.items():
                if valstep:
                    # Pick all labels normalized (https://papers.nips.cc/paper/2019/file/da647c549dde572c2c5edc4f5bef039c-Paper.pdf
                    loss += float(w)*F.cross_entropy(torch.unsqueeze(yh, dim=0), torch.tensor([self._class_to_index[y]], device=y_hat.device), weight=C)
                elif self._mlfl:
                    # Pick all labels normalized, with multi-label focal loss
                    loss += torch.min(torch.tensor(1.0, device=y_hat.device), ((w-yhs[self._class_to_index[y]])/w)**2)*float(w)*F.cross_entropy(torch.unsqueeze(yh, dim=0), torch.tensor([self._class_to_index[y]], device=y_hat.device), weight=C)  
                elif self._mlbl:
                    # Pick all labels normalized with multi-label background loss
                    j_bg_person = self._class_to_index[&#39;person&#39;] if &#39;person&#39; in self._class_to_index else self._class_to_index[&#39;person_walks&#39;]  # FIXME: does not generalize
                    j_bg_vehicle = self._class_to_index[&#39;vehicle&#39;] if &#39;vehicle&#39; in self._class_to_index else self._class_to_index[&#39;car_moves&#39;]  # FIXME: does not generalize
                    j = j_bg_person if (y.startswith(&#39;person&#39;) or y.startswith(&#39;hand&#39;)) else j_bg_vehicle
                    loss += ((1-torch.sqrt(yhs[j]*yhs[self._class_to_index[y]]))**2)*float(w)*F.cross_entropy(torch.unsqueeze(yh, dim=0), torch.tensor([self._class_to_index[y]], device=y_hat.device), weight=C)  
                else:
                    # Pick all labels normalized (https://papers.nips.cc/paper/2019/file/da647c549dde572c2c5edc4f5bef039c-Paper.pdf
                    loss += float(w)*F.cross_entropy(torch.unsqueeze(yh, dim=0), torch.tensor([self._class_to_index[y]], device=y_hat.device), weight=C)

            n_valid += 1
        loss = loss / float(max(1, n_valid))  # batch reduction: mean

        if logging:
            self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return {&#39;loss&#39;: loss}

    def validation_step(self, batch, batch_nb):
        loss = self.training_step(batch, batch_nb, logging=False, valstep=True)[&#39;loss&#39;]
        self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=False, logger=True)
        return {&#39;val_loss&#39;: loss}

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x[&#39;val_loss&#39;] for x in outputs]).mean()
        self.log(&#39;val_loss&#39;, avg_loss, on_epoch=True, prog_bar=False, logger=True)
        self.log(&#39;avg_val_loss&#39;, avg_loss, on_epoch=True, prog_bar=True, logger=True)                
        return {&#39;val_loss&#39;: avg_loss, &#39;avg_val_loss&#39;: avg_loss}                         
    #---- &lt;/LIGHTNING&gt;
    
    @classmethod
    def from_checkpoint(cls, checkpointpath):
        return cls().load_from_checkpoint(checkpointpath)  # lightning
            
    def _load_trained(self, ckptfile):
        self.net = heyvi.model.ResNets_3D_PyTorch.resnet.generate_model(50, n_classes=self.num_classes())
        t = torch.split(self.net.conv1.weight.data, dim=1, split_size_or_sections=1)
        self.net.conv1.weight.data = torch.cat( (*t, t[-1]), dim=1).contiguous()
        self.net.conv1.in_channels = 4  # inflate RGB -&gt; RGBA
        self.load_state_dict(torch.load(ckptfile)[&#39;state_dict&#39;])  # FIXME
        self.eval()
        return self
        
    def _load_pretrained(self):

        pthfile = vipy.downloader.downloadif(&#39;https://dl.dropboxusercontent.com/s/t3xge6lrfqpklr0/r3d50_kms_200ep.pth&#39;,
                                                vipy.util.tocache(&#39;r3d50_KMS_200ep.pth&#39;),  # set VIPY_CACHE env 
                                                sha1=&#39;39ea626355308d8f75307cab047a8d75862c3261&#39;)
        
        net = heyvi.model.ResNets_3D_PyTorch.resnet.generate_model(50, n_classes=1139)
        pretrain = torch.load(pthfile, map_location=&#39;cpu&#39;)
        net.load_state_dict(pretrain[&#39;state_dict&#39;])

        # Inflate RGB -&gt; RGBA         
        t = torch.split(net.conv1.weight.data, dim=1, split_size_or_sections=1)
        net.conv1.weight.data = torch.cat( (*t, t[-1]), dim=1).contiguous()
        net.conv1.in_channels = 4

        self.net = net

        return self

    @staticmethod
    def _totensor(v, training, validation, input_size, num_frames, mean, std, noflip=None, show=False, doflip=False):
        assert isinstance(v, vipy.video.Scene), &#34;Invalid input&#34;
        
        try:
            v = v.download() if (not v.hasfilename() and v.hasurl()) else v  # fetch it if necessary, but do not do this during training!        
            if training or validation:
                random.seed()  # force randomness after fork() 
                (ai,aj) = (v.primary_activity().startframe(), v.primary_activity().endframe())  # activity (start,end)
                (ti,tj) = (v.actor().startframe(), v.actor().endframe())  # track (start,end) 
                startframe = random.randint(max(0, ti-(num_frames//2)), max(1, tj-(num_frames//2)))  # random startframe that contains track
                endframe = min((startframe+num_frames), aj)  # endframe truncated to be end of activity
                (startframe, endframe) = (startframe, endframe) if (startframe &lt; endframe) else (max(0, aj-num_frames), aj)  # fallback
                assert endframe - startframe &lt;= num_frames
                vc = v.clone().clip(startframe, endframe)    # may fail for some short clips
                vc = vc.trackcrop(dilate=1.2, maxsquare=True)  # may be None if clip contains no track
                vc = vc.resize(input_size, input_size)   
                vc = vc.fliplr() if (doflip or (random.random() &gt; 0.5)) and (noflip is None or vc.category() not in noflip) else vc
            else:
                vc = v.trackcrop(dilate=1.2, maxsquare=True)  # may be None if clip contains no track
                vc = vc.resize(input_size, input_size)  # TESTING: this may introduce a preview()
                vc = vc.fliplr() if doflip and (noflip is None or vc.category() not in noflip) else vc
                
            if show:
                vc.clone().resize(512,512).show(timestamp=True)
                vc.clone().binarymask().frame(0).rgb().show(figure=&#39;binary mask: frame 0&#39;)
                
            vc = vc.load(shape=(input_size, input_size, 3)).normalize(mean=mean, std=std, scale=1.0/255.0)  # [0,255] -&gt; [0,1], triggers load() with known shape
            (t,lbl) = vc.torch(startframe=0, length=num_frames, boundary=&#39;cyclic&#39;, order=&#39;cdhw&#39;, withlabel=training or validation, nonelabel=True)  # (c=3)x(d=num_frames)x(H=input_size)x(W=input_size), reuses vc._array
            t = torch.cat((t, vc.asfloatmask(fg=0.5, bg=-0.5).torch(startframe=0, length=num_frames, boundary=&#39;cyclic&#39;, order=&#39;cdhw&#39;)), dim=0)  # (c=4) x (d=num_frames) x (H=input_size) x (W=input_size), copy
            
        except Exception as e:
            if training or validation:
                #print(&#39;ERROR: %s&#39; % (str(v)))
                t = torch.zeros(4, num_frames, input_size, input_size)  # skip me
                lbl = None
            else:
                print(&#39;WARNING: discarding tensor for video &#34;%s&#34; with exception &#34;%s&#34;&#39; % (str(v), str(e)))
                t = torch.zeros(4, num_frames, input_size, input_size)  # skip me (should never get here)
            
        if training or validation:
            return (t, json.dumps(lbl))  # json to use default collate_fn
        else:
            return t

    def totensor(self, v=None, training=False, validation=False, show=False, doflip=False):
        &#34;&#34;&#34;Return captured lambda function if v=None, else return tensor&#34;&#34;&#34;    
        assert v is None or isinstance(v, vipy.video.Scene), &#34;Invalid input&#34;
        f = (lambda v, num_frames=self._num_frames, input_size=self._input_size, mean=self._mean, std=self._std, training=training, validation=validation, show=show:
             PIP_250k._totensor(v, training, validation, input_size, num_frames, mean, std, noflip=[&#39;car_turns_left&#39;, &#39;car_turns_right&#39;], show=show, doflip=doflip))
        return f(v) if v is not None else f
    


class PIP_370k(PIP_250k, pl.LightningModule, ActivityRecognition):

    def __init__(self, pretrained=True, deterministic=False, modelfile=None, mlbl=False, mlfl=False):
        pl.LightningModule.__init__(self)
        ActivityRecognition.__init__(self)  

        self._input_size = 112
        self._num_frames = 16        
        self._mean = [0.485, 0.456, 0.406]
        self._std = [0.229, 0.224, 0.225]
        self._mlfl = mlfl
        self._mlbl = mlbl
        if deterministic:
            np.random.seed(42)
        
        # Generated using vipy.dataset.Dataset(...).multilabel_inverse_frequency_weight()
        self._class_to_weight = {&#39;car_drops_off_person&#39;: 0.7858124882763793, &#39;car_moves&#39;: 0.18439798528529147, &#39;car_picks_up_person&#39;: 0.7380666753394193, &#39;car_reverses&#39;: 0.5753369570213479, &#39;car_starts&#39;: 0.47486292483745757, &#39;car_stops&#39;: 0.44244800737774037, &#39;car_turns_left&#39;: 0.7697107319736983, &#39;car_turns_right&#39;: 0.5412936796835607, &#39;hand_interacts_with_person&#39;: 0.2794031245117859, &#39;person_abandons_package&#39;: 1.0789960714517162, &#39;person_carries_heavy_object&#39;: 0.5032333530901552, &#39;person_closes_car_door&#39;: 0.46460114438995603, &#39;person_closes_car_trunk&#39;: 0.6824201392305784, &#39;person_closes_facility_door&#39;: 0.38990434394080076, &#39;person_embraces_person&#39;: 0.6457437695527715, &#39;person_enters_car&#39;: 0.6934926810021877, &#39;person_enters_scene_through_structure&#39;: 0.2586965095740063, &#39;person_exits_car&#39;: 0.6766386632434479, &#39;person_exits_scene_through_structure&#39;: 0.33054895987676847, &#39;person_interacts_with_laptop&#39;: 0.6720176496986436, &#39;person_loads_car&#39;: 0.6880555743488312, &#39;person_opens_car_door&#39;: 0.4069868136393968, &#39;person_opens_car_trunk&#39;: 0.6911966903970317, &#39;person_opens_facility_door&#39;: 0.3018924474724252, &#39;person_picks_up_object&#39;: 0.4298381074082487, &#39;person_purchases_from_cashier&#39;: 5.479834409621331, &#39;person_purchases_from_machine&#39;: 5.31528236654537, &#39;person_puts_down_object&#39;: 0.2804690906037155, &#39;person_reads_document&#39;: 0.5476186269530937, &#39;person_rides_bicycle&#39;: 1.6090962879286763, &#39;person_sits_down&#39;: 0.4750148103149501, &#39;person_stands_up&#39;: 0.5022364750834624, &#39;person_steals_object&#39;: 0.910991409921711, &#39;person_talks_on_phone&#39;: 0.15771902851484076, &#39;person_talks_to_person&#39;: 0.21362675034201736, &#39;person_texts_on_phone&#39;: 0.3328378404741194, &#39;person_transfers_object_to_car&#39;: 2.964890512157848, &#39;person_transfers_object_to_person&#39;: 0.6481292773603928, &#39;person_unloads_car&#39;: 0.515379337544623, &#39;person_walks&#39;: 6.341278284010202}
        
        # Generated using vipy.dataset.Dataset(...).class_to_index()
        self._class_to_index = {&#39;car_drops_off_person&#39;: 0, &#39;car_moves&#39;: 1, &#39;car_picks_up_person&#39;: 2, &#39;car_reverses&#39;: 3, &#39;car_starts&#39;: 4, &#39;car_stops&#39;: 5, &#39;car_turns_left&#39;: 6, &#39;car_turns_right&#39;: 7, &#39;hand_interacts_with_person&#39;: 8, &#39;person_abandons_package&#39;: 9, &#39;person_carries_heavy_object&#39;: 10, &#39;person_closes_car_door&#39;: 11, &#39;person_closes_car_trunk&#39;: 12, &#39;person_closes_facility_door&#39;: 13, &#39;person_embraces_person&#39;: 14, &#39;person_enters_car&#39;: 15, &#39;person_enters_scene_through_structure&#39;: 16, &#39;person_exits_car&#39;: 17, &#39;person_exits_scene_through_structure&#39;: 18, &#39;person_interacts_with_laptop&#39;: 19, &#39;person_loads_car&#39;: 20, &#39;person_opens_car_door&#39;: 21, &#39;person_opens_car_trunk&#39;: 22, &#39;person_opens_facility_door&#39;: 23, &#39;person_picks_up_object&#39;: 24, &#39;person_purchases_from_cashier&#39;: 25, &#39;person_purchases_from_machine&#39;: 26, &#39;person_puts_down_object&#39;: 27, &#39;person_reads_document&#39;: 28, &#39;person_rides_bicycle&#39;: 29, &#39;person_sits_down&#39;: 30, &#39;person_stands_up&#39;: 31, &#39;person_steals_object&#39;: 32, &#39;person_talks_on_phone&#39;: 33, &#39;person_talks_to_person&#39;: 34, &#39;person_texts_on_phone&#39;: 35, &#39;person_transfers_object_to_car&#39;: 36, &#39;person_transfers_object_to_person&#39;: 37, &#39;person_unloads_car&#39;: 38, &#39;person_walks&#39;: 39}
        
        self._verb_to_noun = {k:set([&#39;car&#39;,&#39;vehicle&#39;,&#39;motorcycle&#39;,&#39;bus&#39;,&#39;truck&#39;]) if (k.startswith(&#39;car&#39;) or k.startswith(&#39;motorcycle&#39;) or k.startswith(&#39;vehicle&#39;)) else set([&#39;person&#39;]) for k in self.classlist()}        
        self._class_to_shortlabel = heyvi.label.pip_to_shortlabel
        self._class_to_shortlabel.update( vipy.data.meva.d_category_to_shortlabel )

        if pretrained:
            self._load_pretrained()
            self.net.fc = nn.Linear(self.net.fc.in_features, self.num_classes())
        elif modelfile is not None:
            self._load_trained(modelfile)

    def topk(self, x, k=None):
        &#34;&#34;&#34;Return the top-k classes for a 3 second activity proposal along with framewise ground truth&#34;&#34;&#34;        
        yh = self.forward(x if x.ndim == 5 else x.unsqueeze(0)).detach().cpu().numpy()
        k = k if k is not None else self.num_classes()
        return [ [self.index_to_class(int(j)) for j in i[-k:][::-1]] for (s,i) in zip(yh, np.argsort(yh, axis=1))]
            
    @staticmethod
    def _totensor(v, training, validation, input_size, num_frames, mean, std, noflip=None, show=False, doflip=False, stride_jitter=3, asjson=False, classname=&#39;heyvi.recognition.PIP_370k&#39;):
        assert isinstance(v, vipy.video.Scene), &#34;Invalid input&#34;
        
        try:
            v = v.download() if (v.hasurl() and not v.hasfilename()) else v  # fetch it if necessary, but do not do this during training!        
            vc = v.clone()
            if training or validation:
                random.seed()  # force randomness after fork() 
                (clipstart, clipend) = vc.cliprange()  # clip (start, end) relative to video 
                (clipstart, clipend) = (clipstart if clipstart is not None else 0,   
                                        clipend if clipend is not None else int(np.floor(v.duration_in_frames_of_videofile() * (vc.framerate() / v.framerate_of_videofile()))))  # (yuck)
                # WARNINGS: 
                # - There exist videos with tracks outside the image rectangle due to the padding in stabilization.  
                # - There exist MEVA videos that have no tracks at the beginning and end of the padded clip since the annotations only exist for the activity
                # - There exist MEVA videos with activities that are longer than the tracks, if so, keep the interval of the activity that contains the track
                # - There exist MEVA videos with multiple objects, need to include only primary actor
                
                # - turning activities may be outside the frame (filter these)
                # - turning activities may turn into the stabilized black area.  Is this avoidaable?
                # - all of the training activities should be centered on the activity.  See if not.
                # - 
                
                if (clipend - clipstart) &gt; (num_frames + stride_jitter):
                    a = vc.primary_activity().clone().padto(num_frames/float(vc.framerate()))  # for context only, may be past end of clip now!
                    (ai, aj) = (a.startframe(), a.endframe())  # activity (start,end) relative to (clipstart, clipend)
                    (ai, aj) = (max(ai, vc.actor().startframe()), min(aj, vc.actor().endframe()))  # clip activity to when actor is present
                    startframe = random.randint(ai, aj-num_frames-1) if aj-num_frames-1 &gt; ai else ai
                    startframe = max(0, startframe + random.randint(-stride_jitter, stride_jitter))   # +/- 3 frames jitter for activity stride
                    endframe = min(clipend-clipstart-1, startframe + num_frames)  # new end cannot be past duration of clip
                    if (endframe &gt; startframe) and ((endframe - startframe) &lt; (clipend - clipstart)):
                        vc = vc.clip(startframe, endframe)
                    else: 
                        raise ValueError(&#39;invalid clip for &#34;%s&#34;&#39; % str(v))
                vc = vc.trackcrop(dilate=1.2, maxsquare=True)  # may be None if clip contains no track
                vc = vc.resize(input_size, input_size)   
                vc = vc.fliplr() if (doflip or (random.random() &gt; 0.5)) and (noflip is None or vc.category() not in noflip) else vc
            else:
                vc = vc.trackcrop(dilate=1.2, maxsquare=True)  # may be None if clip contains no track
                vc = vc.resize(input_size, input_size)  # This may introduce a preview()
                vc = vc.fliplr() if doflip and (noflip is None or vc.category() not in noflip) else vc
                
            if show:
                vc.clone().resize(512,512).show(timestamp=True)
                vc.clone().binarymask().frame(0).gain(255).rgb().show(figure=&#39;binary mask: frame 0&#39;)
                
            vc = vc.load(shape=(input_size, input_size, 3)).normalize(mean=mean, std=std, scale=1.0/255.0)  # [0,255] -&gt; [0,1], triggers load() with known shape
            (t,lbl) = vc.torch(startframe=0, length=num_frames, boundary=&#39;repeat&#39;, order=&#39;cdhw&#39;, withlabel=training or validation, nonelabel=True)  # (c=3)x(d=num_frames)x(H=input_size)x(W=input_size), reuses vc._array
            t = torch.cat((t, vc.asfloatmask(fg=0.5, bg=-0.5).torch(startframe=0, length=num_frames, boundary=&#39;repeat&#39;, order=&#39;cdhw&#39;)), dim=0)  # (c=4) x (d=num_frames) x (H=input_size) x (W=input_size), copy

        except Exception as e:
            if training or validation:
                print(&#39;[heyvi.recognition.%s._totensor][SKIPPING]: video=&#34;%s&#34;, exception=&#34;%s&#34;&#39; % (classname, str(vc), str(e)))
                (t, lbl) = (torch.zeros(4, num_frames, input_size, input_size), None)  # must always return conformal tensor (label=None means it will be ignored)
            else:
                print(&#39;[heyvi.recognition.%s._totensor][ERROR]: discarding tensor for video &#34;%s&#34; with exception &#34;%s&#34;&#39; % (classname, str(vc), str(e)))
                #t = torch.zeros(4, num_frames, input_size, input_size)  # skip me (should never get here)
                raise

        if training or validation:
            return (t, json.dumps(lbl) if not asjson else lbl)  # json to use default torch collate_fn
        else:
            return t

    def totensor(self, v=None, training=False, validation=False, show=False, doflip=False, asjson=False):
        &#34;&#34;&#34;Return captured lambda function if v=None, else return tensor&#34;&#34;&#34;    
        assert v is None or isinstance(v, vipy.video.Scene), &#34;Invalid input&#34;
        f = (lambda v, num_frames=self._num_frames, input_size=self._input_size, mean=self._mean, std=self._std, training=training, validation=validation, show=show, classname=self.__class__.__name__:
             PIP_370k._totensor(v, training, validation, input_size, num_frames, mean, std, noflip=[&#39;car_turns_left&#39;, &#39;car_turns_right&#39;, &#39;vehicle_turns_left&#39;, &#39;vehicle_turns_right&#39;, &#39;motorcycle_turns_left&#39;, &#39;motorcycle_turns_right&#39;], show=show, doflip=doflip, asjson=asjson, classname=classname))
        return f(v) if v is not None else f


class CAP(PIP_370k, pl.LightningModule, ActivityRecognition):
    def __init__(self, modelfile=None, deterministic=True, pretrained=None, mlbl=None, mlfl=None):
        pl.LightningModule.__init__(self)
        ActivityRecognition.__init__(self)  

        self._input_size = 112
        self._num_frames = 16        
        self._mean = [0.485, 0.456, 0.406]
        self._std = [0.229, 0.224, 0.225]
        self._mlfl = True
        self._mlbl = False
        if deterministic:
            np.random.seed(42)
        
        # Generated using vipy.dataset.Dataset.multilabel_inverse_frequency_weight()
        # WARNING: this was truncated to one
        self._class_to_weight = {&#39;person_talks_on_phone&#39;: 0.0071500571732126764, &#39;car_moves&#39;: 0.008414980303528122, &#39;person_talks_to_person&#39;: 0.009391174775630284, &#39;person_opens_facility_door&#39;: 0.011485169340670632, &#39;person_enters_scene_through_structure&#39;: 0.011577007698961155, &#39;hand_interacts_with_person&#39;: 0.012750528623581633, &#39;person_puts_down_object&#39;: 0.012799173860425943, &#39;person_stands_up&#39;: 0.012843569626160192, &#39;person_exits_scene_through_structure&#39;: 0.014980961043628886, &#39;person_texts_on_phone&#39;: 0.01499085586892757, &#39;person_closes_facility_door&#39;: 0.016113488440289907, &#39;person_sits_down&#39;: 0.017908321006202585, &#39;person_opens_car_door&#39;: 0.01857279522480972, &#39;person_stands_up_from_floor&#39;: 0.019361727910618066, &#39;person_picks_up_object&#39;: 0.019615611319993825, &#39;car_stops&#39;: 0.02019106261740665, &#39;person_closes_car_door&#39;: 0.021202018411369025, &#39;car_starts&#39;: 0.021670313551423053, &#39;person_carries_heavy_object&#39;: 0.02274928459304244, &#39;person_unloads_car&#39;: 0.02351927526525588, &#39;car_turns_right&#39;: 0.024701873211435525, &#39;person_reads_document&#39;: 0.02499051143387395, &#39;car_reverses&#39;: 0.026255434156376472, &#39;person_transfers_object_to_person&#39;: 0.026580220081877853, &#39;person_picks_up_object_from_floor&#39;: 0.029207808500200837, &#39;person_embraces_person&#39;: 0.02945420882971027, &#39;person_interacts_with_laptop&#39;: 0.029911212318566305, &#39;person_exits_car&#39;: 0.030878325568415993, &#39;person_closes_car_trunk&#39;: 0.03114216254299982, &#39;person_loads_car&#39;: 0.03139933495976206, &#39;person_opens_car_trunk&#39;: 0.03154267942003807, &#39;person_enters_car&#39;: 0.03164745668042723, &#39;car_picks_up_person&#39;: 0.033681585653241426, &#39;car_turns_left&#39;: 0.035125658444433834, &#39;car_drops_off_person&#39;: 0.03586046019364838, &#39;person_steals_object&#39;: 0.0415729855145331, &#39;person_picks_up_object_from_table&#39;: 0.04418573296650998, &#39;person_drops_object&#39;: 0.048065094048302635, &#39;person_abandons_package&#39;: 0.04923985842254567, &#39;person_jumps&#39;: 0.05345098137582828, &#39;person_takes_phone_from_pocket&#39;: 0.05462388692326024, &#39;person_trips_on_object_on_floor&#39;: 0.05498089026389937, &#39;person_puts_phone_into_pocket&#39;: 0.056183850529544055, &#39;person_drinks_from_mug&#39;: 0.05666046130633482, &#39;person_drinks_from_bottle&#39;: 0.057329881619530565, &#39;person_sweeps_floor&#39;: 0.06141749219518637, &#39;person_uses_television_remote&#39;: 0.06148643566096271, &#39;person_wipes_mouth_with_napkin&#39;: 0.0616838064677993, &#39;person_sprays_from_bottle&#39;: 0.06264264051441124, &#39;person_salutes&#39;: 0.0628090316681978, &#39;person_puts_down_object_on_table&#39;: 0.06571369688838084, &#39;person_looks_at_wristwatch&#39;: 0.06672561082585902, &#39;person_puts_hands_in_back_pockets&#39;: 0.06787709488015589, &#39;person_punches&#39;: 0.06849983752132058, &#39;person_drinks_from_straw&#39;: 0.06898414063150268, &#39;person_strokes_chin&#39;: 0.06910772776183577, &#39;person_exercises_with_jumping_jacks&#39;: 0.06940183386381935, &#39;person_opens_laptop&#39;: 0.07058098348291678, &#39;person_grabs_person_by_bicep&#39;: 0.07060789551353987, &#39;person_grabs_person_by_forearm&#39;: 0.07071022579689283, &#39;person_strokes_hair&#39;: 0.0711640253790199, &#39;person_grabs_person_by_hair&#39;: 0.07126797516777103, &#39;person_closes_laptop&#39;: 0.07141913528726114, &#39;person_cleans_table_with_rag&#39;: 0.07177957626166052, &#39;person_searches_in_bag&#39;: 0.07240861269366403, &#39;person_exercises_with_pushup&#39;: 0.07245544215462119, &#39;person_picks_up_object_from_bed&#39;: 0.07261587289604508, &#39;person_rides_bicycle&#39;: 0.07343091926113407, &#39;person_brushes_teeth&#39;: 0.07613928807717861, &#39;person_gestures_raise_hand&#39;: 0.07666348171707751, &#39;person_sits_crisscross&#39;: 0.0777406880176164, &#39;person_gestures_be_quiet&#39;: 0.0778337212322166, &#39;person_gestures_cut&#39;: 0.07796334578685198, &#39;person_shoves_person&#39;: 0.07798921962892592, &#39;person_squats&#39;: 0.07838299609818057, &#39;person_puts_on_hat&#39;: 0.07850485876811529, &#39;person_crosses_arms&#39;: 0.07882076865889508, &#39;person_lies_down_on_bed&#39;: 0.0791809589015662, &#39;person_takes_off_hat&#39;: 0.07954505147236295, &#39;person_waves_at_person&#39;: 0.08075149917222121, &#39;person_stirs_mug&#39;: 0.08080499469999347, &#39;person_gestures_behind_me&#39;: 0.08104660431869777, &#39;person_exercises_with_situp&#39;: 0.08108438212368972, &#39;person_points_at_person&#39;: 0.0813710070044297, &#39;person_claps_hands&#39;: 0.0813710070044297, &#39;person_pats_head&#39;: 0.0825550859557632, &#39;person_gestures_listen_closely&#39;: 0.08286354585573379, &#39;person_bumps_into_wall&#39;: 0.0831459710290662, &#39;person_gestures_come_here&#39;: 0.08328790679388197, &#39;person_eats_snack_from_bag&#39;: 0.08403922538881889, &#39;person_puts_on_glasses&#39;: 0.08409178869330584, &#39;person_gestures_blow_kiss&#39;: 0.08417884023439623, &#39;person_picks_up_object_from_countertop&#39;: 0.08545575718410064, &#39;person_gestures_stop&#39;: 0.08591091430900676, &#39;person_takes_off_glasses&#39;: 0.08607984438930144, &#39;person_takes_off_facemask&#39;: 0.08657028030837606, &#39;person_gestures_lower_hand&#39;: 0.08671760324527736, &#39;person_puts_on_facemask&#39;: 0.0874573927099682, &#39;person_pours_into_bowl&#39;: 0.08757414051616512, &#39;person_gestures_watch_closely&#39;: 0.08822794900516465, &#39;person_gestures_number_five&#39;: 0.08909798356438287, &#39;person_gestures_number_three&#39;: 0.08913053671877248, &#39;person_gestures_number_one&#39;: 0.08932635627948746, &#39;person_gestures_arms_x&#39;: 0.08935907655651291, &#39;person_gestures_thumbs_up&#39;: 0.08974733507235413, &#39;person_gestures_number_four&#39;: 0.08981969035319597, &#39;person_searches_in_box&#39;: 0.09051958404425983, &#39;person_karate_kicks&#39;: 0.09061798903721408, &#39;person_gestures_heart&#39;: 0.09078908783002615, &#39;person_gestures_call_me&#39;: 0.09171063120273695, &#39;person_karate_chop&#39;: 0.09211588142846427, &#39;person_covers_face_with_hands&#39;: 0.09212623829277955, &#39;person_drinks_from_cup&#39;: 0.09237747667762447, &#39;person_bounces_ball_on_floor&#39;: 0.09314634555146249, &#39;person_takes_off_headphones&#39;: 0.09357509742972007, &#39;person_puts_on_headphones&#39;: 0.09422567748137516, &#39;person_puts_down_object_on_countertop&#39;: 0.09437718336136768, &#39;person_bows&#39;: 0.0948116125142947, &#39;person_searches_in_couch&#39;: 0.09669055846186297, &#39;person_gestures_thumbs_down&#39;: 0.09682838430256516, &#39;person_exercises_with_lunges&#39;: 0.09696408981583088, &#39;person_picks_up_object_from_couch&#39;: 0.09775818840195175, &#39;person_hugs_stuffed_animal&#39;: 0.09944976722351416, &#39;person_reads_book&#39;: 0.09957154244868582, &#39;person_sneezees into arm&#39;: 0.10051239314918431, &#39;person_takes_object_from_bag&#39;: 0.10070841311819703, &#39;person_puts_down_object_on_couch&#39;: 0.10077857312928755, &#39;person_sneezes_into_hand&#39;: 0.10219953037255143, &#39;person_lies_down_on_floor&#39;: 0.10227736307765319, &#39;person_tucks_in_shirt&#39;: 0.10246240170736202, &#39;person_puts_feet_up&#39;: 0.10246595377450825, &#39;person_yawns&#39;: 0.10284787750290855, &#39;person_crosses_legs&#39;: 0.10306306675085773, &#39;person_laughs_with_person&#39;: 0.10310178497269419, &#39;person_takes_object_from_cabinet&#39;: 0.10344711927898777, &#39;person_clips_fingernails&#39;: 0.10358822887442899, &#39;person_closes_cabinet&#39;: 0.10364868957787093, &#39;person_opens_refrigerator&#39;: 0.10400807518189813, &#39;person_puts_object_in_cabinet&#39;: 0.10437215722553372, &#39;person_applies_deodorant&#39;: 0.10438608429579814, &#39;person_reads_magazine&#39;: 0.10452025664065136, &#39;person_lies_down_on_couch&#39;: 0.10479046066920758, &#39;person_rubs_neck&#39;: 0.10515098232727597, &#39;person_dances_in_place&#39;: 0.10528096409904394, &#39;person_looks_at_hands_in_lap&#39;: 0.10560618138496981, &#39;person_shrugs&#39;: 0.10615764969507409, &#39;person_opens_curtains&#39;: 0.1064289157366757, &#39;person_carries_groceries&#39;: 0.10677998502422664, &#39;person_closes_curtains&#39;: 0.10701196252783704, &#39;person_puts_on_scarf&#39;: 0.10707652735115192, &#39;person_holds_object_above_head&#39;: 0.10746708325959484, &#39;person_puts_object_into_bag&#39;: 0.10770431743897584, &#39;person_touches_earlobe&#39;: 0.10789486023851405, &#39;person_crawls&#39;: 0.10814051028495372, &#39;person_puts_down_object_on_bed&#39;: 0.10908517308037075, &#39;person_swats_bug&#39;: 0.10929671998175639, &#39;person_rubs_foot&#39;: 0.1093457099951951, &#39;person_rubs_eyes&#39;: 0.10944382189290276, &#39;person_taps_object_with_finger&#39;: 0.10959132030515735, &#39;person_twirls&#39;: 0.10993703424933766, &#39;person_files_fingernails&#39;: 0.10993703424933766, &#39;person_gestures_swipe_up&#39;: 0.11013924537090604, &#39;person_eats_with_hands&#39;: 0.11072316058030798, &#39;person_gestures_swipe_left&#39;: 0.11113247931061442, &#39;person_closes_refrigerator&#39;: 0.1111555983642474, &#39;person_scratches_face&#39;: 0.11180122777235575, &#39;person_gestures_peace&#39;: 0.11185248922479608, &#39;person_searches_under_couch&#39;: 0.11190379770609186, &#39;person_gestures_swipe_down&#39;: 0.11198350367806151, &#39;person_tears_paper&#39;: 0.11205800597118983, &#39;person_cracks_knuckles&#39;: 0.11216104781576104, &#39;person_folds_towel&#39;: 0.11236065501421866, &#39;person_takes_off_scarf&#39;: 0.11262669819417719, &#39;person_takes_medicine_pills&#39;: 0.11262709095072956, &#39;person_untucks_shirt&#39;: 0.11283546669716941, &#39;person_folds_pants&#39;: 0.11297390288995361, &#39;person_searches_in_kitchen_drawer&#39;: 0.11309702318000939, &#39;person_gestures_swipe_right&#39;: 0.1132382679012143, &#39;person_eats_with_utensil&#39;: 0.11360054487971481, &#39;person_waves_hand_over_object&#39;: 0.11367673765110917, &#39;person_squeezes_object&#39;: 0.11372973379919825, &#39;person_folds_shirt&#39;: 0.11375160594315399, &#39;person_licks_fingers&#39;: 0.11442320778577873, &#39;person_folds_socks&#39;: 0.11491150791364328, &#39;person_puts_down_person&#39;: 0.11519465416162661, &#39;person_grabs_person_by_shoulder&#39;: 0.11572593880421266, &#39;person_carries_furniture&#39;: 0.11583583998066489, &#39;person_carries_laundry_basket&#39;: 0.11628835960688708, &#39;person_opens_cabinet&#39;: 0.1163328202475424, &#39;person_crumples_paper&#39;: 0.11633298950847891, &#39;person_spins_person_around&#39;: 0.11644404725502637, &#39;person_picks_up_object_from_cabinet&#39;: 0.11722742863973104, &#39;person_brushes_hair&#39;: 0.11752581209517853, &#39;person_puts_fingers_in_ear&#39;: 0.11756639951772542, &#39;person_wiggles_hips&#39;: 0.11819296463143424, &#39;person_nudges_person_with_elbow&#39;: 0.11864035680381292, &#39;person_touches_face_of_person&#39;: 0.1199952183961044, &#39;person_touches_back_of_person&#39;: 0.1201040449053256, &#39;person_puts_down_object_into_cabinet&#39;: 0.12100708283694457, &#39;person_brushes_hair_of_person&#39;: 0.12197513949964013, &#39;person_searches_in_backback&#39;: 0.12213018201245313, &#39;person_blows_nose&#39;: 0.12264971292070401, &#39;person_pounds_shoulders_of_person&#39;: 0.12281303232588005, &#39;person_takes_off_shoes&#39;: 0.12350532456683978, &#39;person_jumps_on_couch&#39;: 0.12378164652675162, &#39;person_mops&#39;: 0.1252311493836141, &#39;person_snaps_fingers&#39;: 0.1256180633363956, &#39;person_puts_on_shoes&#39;: 0.12636147834728292, &#39;person_washes_hands&#39;: 0.12684469836917986, &#39;person_puts_on_sunglasses&#39;: 0.12747187073013944, &#39;person_flosses&#39;: 0.1277895646931798, &#39;person_kneels&#39;: 0.1278423621928503, &#39;person_reads_newspaper&#39;: 0.12825987329089392, &#39;person_kisses_cheek_of_person&#39;: 0.12886966666628646, &#39;person_puts_object_into_backpack&#39;: 0.12991164284856274, &#39;person_polishes_car_with_rag&#39;: 0.13003746215313447, &#39;person_unloads_clothes_from_suitcase&#39;: 0.1302110517855032, &#39;person_dries_hair_with_towel&#39;: 0.1302457442601603, &#39;person_dries_hands_with_towel&#39;: 0.1306844570815548, &#39;person_turns_off_faucet&#39;: 0.13197629371214256, &#39;person_turns_on_faucet&#39;: 0.13294031635069914, &#39;person_puts_on_jacket&#39;: 0.1332673096496579, &#39;person_searches_under_bed&#39;: 0.13337904811332982, &#39;person_dries_dish&#39;: 0.1335343175960496, &#39;person_transfers_object_to_car&#39;: 0.13530242872949544, &#39;person_drums_on_chest&#39;: 0.13582977672565716, &#39;person_stretches_back&#39;: 0.13635674842915196, &#39;person_puts_on_gloves&#39;: 0.13785597770489835, &#39;person_gestures_hang_loose&#39;: 0.1381371908263195, &#39;person_applies_lip_makeup&#39;: 0.13821545552367154, &#39;person_picks_up_object_from_shelf&#39;: 0.13901394290468203, &#39;person_takes_object_from_backpack&#39;: 0.1390887336531267, &#39;person_slaps_hands_on_thighs&#39;: 0.13932054768662494, &#39;person_takes_off_sunglasses&#39;: 0.13955965617807797, &#39;person_paints_fingernails&#39;: 0.141502482018144, &#39;person_opens_home_window&#39;: 0.14158460766063857, &#39;person_applies_eye_makeup&#39;: 0.14191406573547427, &#39;person_closes_home_window&#39;: 0.14205455016553906, &#39;person_climbs_on_chair&#39;: 0.14286416865055518, &#39;person_opens_dresser_drawer&#39;: 0.14392346843615356, &#39;person_opens_closet_door&#39;: 0.14409349025356186, &#39;person_discards_trash&#39;: 0.14619789236857958, &#39;person_puts_down_object_on_shelf&#39;: 0.14623046508025042, &#39;person_climbs_off_chair&#39;: 0.1466640130633881, &#39;person_pulls_out_chair&#39;: 0.1467564560629468, &#39;person_stretches_arms_over_head&#39;: 0.14774021316767652, &#39;person_takes_off_gloves&#39;: 0.14792482083182573, &#39;person_hikes_up_pants&#39;: 0.14811795931953872, &#39;person_closes_closet_door&#39;: 0.14838824756647218, &#39;person_puts_on_wristwatch&#39;: 0.14846758639707738, &#39;person_prays&#39;: 0.149479337622108, &#39;person_opens_suitcase&#39;: 0.14999956703824965, &#39;person_washes_dish&#39;: 0.15061335914443671, &#39;person_interacts_with_tablet&#39;: 0.15067960407614595, &#39;person_sticks_out_tongue&#39;: 0.15114639343202, &#39;person_puts_up_picture_frame&#39;: 0.15124009857363935, &#39;person_stretches_arms_to_side&#39;: 0.1517687347875614, &#39;person_ties_jacket_around_waist&#39;: 0.15205524466480033, &#39;person_closes_dresser_drawer&#39;: 0.15256427704770498, &#39;person_takes_object_from_kitchen_drawer&#39;: 0.15292174958247323, &#39;person_covers_with_blanket&#39;: 0.1532186939054849, &#39;person_applies_foundation_makeup&#39;: 0.15362108249324954, &#39;person_takes_down_picture_frame&#39;: 0.15371788216715832, &#39;person_takes_selfie&#39;: 0.1544966934764283, &#39;person_walks_around_car&#39;: 0.1545946001262866, &#39;person_jumps_on_bed&#39;: 0.15469263094437558, &#39;person_zips_up_jacket&#39;: 0.15477837356130825, &#39;person_closes_suitcase&#39;: 0.1548318044264797, &#39;person_sets_upright_glass&#39;: 0.15528343666408673, &#39;person_takes_off_jacket&#39;: 0.1557468846718362, &#39;person_puts_object_into_purse&#39;: 0.15597843925785185, &#39;person_searches_in_purse&#39;: 0.1564786908269918, &#39;person_puts_on_socks&#39;: 0.15697187998993098, &#39;person_unties_jacket_around_waist&#39;: 0.1575376799198842, &#39;person_takes_off_backpack&#39;: 0.15780303861823264, &#39;person_arranges_flowers_in_vase&#39;: 0.15789662071150826, &#39;person_opens_kitchen_drawer&#39;: 0.15796859678500738, &#39;person_unzips_jacket&#39;: 0.15821465973786641, &#39;person_puts_on_backpack&#39;: 0.1591174653417599, &#39;person_closes_kitchen_drawer&#39;: 0.16220003797595112, &#39;person_puts_on_necklace&#39;: 0.1626663379799898, &#39;person_puts_object_into_kitchen_drawer&#39;: 0.16363350184412317, &#39;person_takes_off_socks&#39;: 0.1637836332966157, &#39;person_hits_person_with_pillow&#39;: 0.16383497582221643, &#39;person_climbs_on_couch&#39;: 0.16385203176578586, &#39;person_throws_object_into_air&#39;: 0.16542943627126713, &#39;person_takes_object_from_purse&#39;: 0.16550222455853475, &#39;person_kicks_car_tires&#39;: 0.1662919420581324, &#39;person_loads_clothes_into_suitcase&#39;: 0.16665572681128546, &#39;person_takes_object_from_basket&#39;: 0.16704418245100291, &#39;person_takes_off_necklace&#39;: 0.16720375531136414, &#39;person_burns_hand&#39;: 0.16766342199263248, &#39;person_walks_tiptoe&#39;: 0.16766342199263248, &#39;person_applies_sunscreen&#39;: 0.1694099159717224, &#39;person_pushes_in_chair&#39;: 0.17093473022331301, &#39;person_nods_head&#39;: 0.1730143822689931, &#39;person_puts_on_ring&#39;: 0.17320954516305123, &#39;person_puts_object_into_basket&#39;: 0.17345866516751823, &#39;person_covers_friend_with_blanket&#39;: 0.17375375997099735, &#39;person_shakes_head&#39;: 0.1741258236968453, &#39;person_takes_off_wristwatch&#39;: 0.1751258284273369, &#39;person_lights_candle&#39;: 0.17639210339788883, &#39;person_picks_up_person&#39;: 0.1775475101887047, &#39;person_shades_eyes&#39;: 0.17898039545068253, &#39;person_uncovers_friend_with_blanket&#39;: 0.17990433554519192, &#39;person_closes_clothes_washer&#39;: 0.18070391036983724, &#39;person_blows_into_hands&#39;: 0.18097201706178062, &#39;person_opens_clothes_washer&#39;: 0.18097201706178062, &#39;person_puts_on_belt&#39;: 0.1826837651926084, &#39;person_bumps_into_table&#39;: 0.18273429138522868, &#39;person_climbs_off_couch&#39;: 0.18411341811266432, &#39;person_uncovers_with_blanket&#39;: 0.1846559266696029, &#39;person_eats_apple&#39;: 0.1855135201515439, &#39;person_opens_box&#39;: 0.18707843481539896, &#39;person_takes_off_ring&#39;: 0.18750982244372041, &#39;person_throws_object_to_ground&#39;: 0.1885241723332923, &#39;person_locks_door_with_keys&#39;: 0.1891087434102948, &#39;person_unlocks_door_with_keys&#39;: 0.18954955633199708, &#39;person_vacuums_carpet&#39;: 0.18954955633199708, &#39;person_opens_microwave&#39;: 0.18967163518640828, &#39;person_extinguishes_candle&#39;: 0.18984457509671618, &#39;person_applies_facial_moisturizer&#39;: 0.19043737626797835, &#39;person_eats_banana&#39;: 0.1929986384487977, &#39;person_stubs_toe&#39;: 0.19395487244437154, &#39;person_puts_on_shirt&#39;: 0.194933929667213, &#39;person_closes_box&#39;: 0.195472979967372, &#39;person_puts_down_object_on_floor&#39;: 0.19599558450709312, &#39;person_throws_object_on_table&#39;: 0.19898065171230037, &#39;person_opens_jar&#39;: 0.20028758538528754, &#39;person_drinks_from_beverage_can&#39;: 0.20218778029011394, &#39;person_closes_microwave&#39;: 0.20322926171591793, &#39;person_exercises_with_plank&#39;: 0.2032303156689395, &#39;person_takes_clothes_from_dresser&#39;: 0.2043134664985597, &#39;person_carries_bicycle&#39;: 0.20500023445317672, &#39;person_dusts_furniture&#39;: 0.20813094360488035, &#39;person_closes_jar&#39;: 0.20921979330984586, &#39;person_knocks_over_glass&#39;: 0.21030196465455195, &#39;person_takes_off_belt&#39;: 0.21048341587513394, &#39;person_closes_door_with_foot&#39;: 0.21066518048297084, &#39;person_puts_clothes_into_dresser&#39;: 0.21066518048297084, &#39;person_kicks_object_to_person&#39;: 0.21194637619398804, &#39;person_pours_coffee_into_mug&#39;: 0.2139914728063862, &#39;person_buttons_shirt&#39;: 0.21535941093384248, &#39;person_falls_from_chair&#39;: 0.21573745621114188, &#39;person_puts_on_hoodie&#39;: 0.21625599556698347, &#39;person_opens_beverage_can&#39;: 0.21703761476804292, &#39;person_puts_on_earrings&#39;: 0.21786207374692124, &#39;person_climbs_on_table&#39;: 0.22102665713111078, &#39;person_climbs_off_table&#39;: 0.22195290532393117, &#39;person_takes_off_earrings&#39;: 0.2233976913912823, &#39;person_waters_houseplant&#39;: 0.2236024555447115, &#39;person_opens_oven_door&#39;: 0.22407580298906604, &#39;person_wraps_box&#39;: 0.22567093339433883, &#39;person_loads_clotheswasher&#39;: 0.22927657800684234, &#39;person_closes_oven_door&#39;: 0.2313013332980745, &#39;person_closes_door_with_hip&#39;: 0.23233359904693357, &#39;person_unbuttons_shirt&#39;: 0.2332220640528492, &#39;person_pushes_wheeled_cart&#39;: 0.2375367857831356, &#39;person_pulls_wheeled_cart&#39;: 0.23963681630577627, &#39;person_dries_hair_with_hairdryer&#39;: 0.24010854232212622, &#39;person_takes_off_shirt&#39;: 0.24034510246234506, &#39;person_irons_clothes&#39;: 0.2405821291906117, &#39;person_bumps_into_person&#39;: 0.2419661565158503, &#39;person_purchases_from_machine&#39;: 0.24256228370916671, &#39;person_purchases_from_cashier&#39;: 0.2500715967813632, &#39;person_takes_off_hoodie&#39;: 0.2512361266727912, &#39;person_flips_up_car_wipers&#39;: 0.25625029306647085, &#39;person_throws_object_to_person&#39;: 0.2584219057195765, &#39;person_climbs_up_stairs&#39;: 0.25924578002048915, &#39;person_drinks_from_shotglass&#39;: 0.26007492430626894, &#39;person_puts_on_pants&#39;: 0.26180106773743944, &#39;person_flips_down_car_wipers&#39;: 0.26203037486496267, &#39;person_spreads_tablecloth&#39;: 0.26287745581818994, &#39;person_takes_clothes_from_closet&#39;: 0.26373003135057327, &#39;person_folds_tablecloth&#39;: 0.26458815509683326, &#39;person_pours_liquid_into_cup&#39;: 0.26716946140944486, &#39;person_takes_selfie_with_person&#39;: 0.26837214411361965, &#39;person_washes_window&#39;: 0.2688077926543526, &#39;person_closes_car_hood&#39;: 0.271357373747809, &#39;person_climbs_down_stairs&#39;: 0.2719624069111263, &#39;person_covers_with_bedsheets&#39;: 0.2733971404050831, &#39;person_opens_car_hood&#39;: 0.274101437077843, &#39;person_puts_on_necktie&#39;: 0.2741340156665674, &#39;person_tickles_person&#39;: 0.275961854071584, &#39;person_uncovers_with_bedsheets&#39;: 0.27733168911014544, &#39;person_catches_dropped_object&#39;: 0.2784711362375529, &#39;person_lifts_dummbells&#39;: 0.2788003188563203, &#39;person_climbs_up_ladder&#39;: 0.28008068771444344, &#39;person_sets_table&#39;: 0.2810487085245164, &#39;person_puts_object_into_microwave&#39;: 0.2823920373735829, &#39;person_folds_blanket&#39;: 0.28267703244412545, &#39;person_interacts_with_handheld_game&#39;: 0.2839933399293134, &#39;person_looks_over_shoulder&#39;: 0.28532196374184826, &#39;person_sets_upright_furniture&#39;: 0.28532196374184826, &#39;person_catches_object_from_person&#39;: 0.2870003282344473, &#39;person_knocks_over_furniture&#39;: 0.2870003282344473, &#39;person_walks&#39;: 0.2893834863573906, &#39;person_puts_hair_in_ponytail&#39;: 0.290763145410346, &#39;person_climbs_down_ladder&#39;: 0.290763145410346, &#39;person_takes_off_pants&#39;: 0.2939159987943136, &#39;person_puts_clothes_into_closet&#39;: 0.2953393208223732, &#39;person_searches_in_cabinet&#39;: 0.2956973078779155, &#39;person_throws_object_on_bed&#39;: 0.29677649513294435, &#39;person_washes_face&#39;: 0.2971379768566142, &#39;person_carries_person_over_shoulder&#39;: 0.2972615412890275, &#39;person_takes_object_from_microwave&#39;: 0.2981634008906294, &#39;person_opens_can_with_can_opener&#39;: 0.29969321744383326, &#39;person_unfolds_blanket&#39;: 0.30117318394972875, &#39;person_dries_face_with_towel&#39;: 0.3041774052360103, &#39;person_loads_groceries_into_refrigerator&#39;: 0.306085670011644, &#39;person_puts_on_apron&#39;: 0.3076296078175035, &#39;person_makes_bed&#39;: 0.3099749415492761, &#39;person_takes_off_hairtie&#39;: 0.3119568785157037, &#39;person_unloads_clotheswasher&#39;: 0.31356076992195403, &#39;person_takes_off_apron&#39;: 0.31518123901715794, &#39;person_turns_off_fan&#39;: 0.3159977707244563, &#39;person_turns_on_lamp&#39;: 0.31681854415490945, &#39;person_turns_on_fan&#39;: 0.31788266098916557, &#39;person_opens_sliding_door&#39;: 0.31805773011640187, &#39;person_closes_sliding_door&#39;: 0.32141011725860374, &#39;person_takes_off_necktie&#39;: 0.324401966754362, &#39;person_hugs_person_from_behind&#39;: 0.324401966754362, &#39;person_loads_groceries_into_cabinet&#39;: 0.3257013070751405, &#39;person_carries_person_on_back&#39;: 0.3283314656787083, &#39;person_braids_hair_of_person&#39;: 0.3323573283368941, &#39;person_turns_on_stovetop&#39;: 0.3346368710552541, &#39;person_falls_from_bed&#39;: 0.3350905862080302, &#39;person_uses_bodyweight_scale&#39;: 0.3426267963473037, &#39;person_turns_off_lamp&#39;: 0.3431086905756403, &#39;person_vapes&#39;: 0.3435919422525074, &#39;person_turns_off_stovetop&#39;: 0.3460287645379862, &#39;person_closes_gate&#39;: 0.34701319914549117, &#39;person_opens_gate&#39;: 0.3475075199419947, &#39;person_puts_object_into_refrigerator&#39;: 0.3530701457147036, &#39;person_puts_object_into_oven&#39;: 0.3572873745649558, &#39;person_unscrews_lid_from_bottle&#39;: 0.35748569200453895, &#39;person_applies_shaving_cream&#39;: 0.35927876141278386, &#39;person_takes_object_from_refrigerator&#39;: 0.3595562632398506, &#39;person_shaves_face&#39;: 0.3646491464862186, &#39;person_takes_object_from_oven&#39;: 0.36930257807780786, &#39;person_screws_lid_to_bottle&#39;: 0.3758864083193841, &#39;person_loads_dryer&#39;: 0.3947415517787706, &#39;person_falls_while_standing&#39;: 0.4050786163671186, &#39;person_inserts_trashbag_into_trashcan&#39;: 0.47599885750525034, &#39;person_removes_trashbag_from_trashcan&#39;: 0.47707269147228026, &#39;person_gestures_zoom_out&#39;: 0.5082297479151672, &#39;person_gestures_zoom_in&#39;: 0.5291763101936665, &#39;person_unloads_dryer&#39;: 0.5421117311095116, &#39;person_handstand&#39;: 0.566009928072576, &#39;person_pets_dog&#39;: 0.6402894461923366, &#39;person_plugs_into_electrical_socket&#39;: 0.648803933508724, &#39;person_somersaults&#39;: 0.648803933508724, &#39;person_unplugs_from_electrical_socket&#39;: 0.6629083668458703, &#39;person_carries_person_on_shoulders&#39;: 0.6891250819188708, &#39;person_adjusts_thermostat&#39;: 0.7370099063422364, &#39;person_hugs_dog&#39;: 0.7719945537951908, &#39;person_attaches_leash_to_dog&#39;: 0.786936383868646, &#39;person_opens_dishwasher&#39;: 0.7920463603872736, &#39;person_closes_dishwasher&#39;: 0.7946263159585677, &#39;person_pets_cat&#39;: 0.8297628537390485, &#39;person_searches_jewelry_box&#39;: 0.8354461609564392, &#39;person_hugs_cat&#39;: 0.847049579858612, &#39;person_opens_jewelry_box&#39;: 0.8559658912255448, &#39;person_unattaches_leash_from_dog&#39;: 0.8620151201387994, &#39;person_unloads_dishwasher&#39;: 0.8712509964260009, &#39;person_loads_dishwasher&#39;: 0.8743737598540511, &#39;person_closes_jewelry_box&#39;: 0.9205670905633218, &#39;person_puts_object_into_toaster&#39;: 0.9642303517758113, &#39;person_takes_object_from_toaster&#39;: 0.9797199959810451, &#39;person_puts_on_boots&#39;: 0.9957154244868583, &#39;person_takes_off_boots&#39;: 1, &#39;person_unloads_box_onto_floor&#39;: 1, &#39;person_spills_on_table&#39;: 1, &#39;person_cleans_dryer_lint_trap&#39;: 1, &#39;person_exits_pool&#39;: 1, &#39;person_unloads_box_onto_table&#39;: 1, &#39;person_spills_on_floor&#39;: 1, &#39;person_texts_on_phone_while_sitting&#39;: 1, &#39;person_closes_mailbox&#39;: 1, &#39;person_opens_mailbox&#39;: 1, &#39;person_puts_object_into_box&#39;: 1, &#39;person_takes_object_from_box&#39;: 1, &#39;person_takes_down_smoke_detector&#39;: 1, &#39;person_puts_up_smoke_detector&#39;: 1, &#39;person_trips_on_stair&#39;: 1, &#39;person_jumps_into_pool&#39;: 1, &#39;person_falls_into_pool&#39;: 1, &#39;person_pulls_wheeled_trashcan&#39;: 1, &#39;person_uncrates_dog&#39;: 1, &#39;person_crates_dog&#39;: 1, &#39;person_pushes_wheeled_trashcan&#39;: 1, &#39;person_leaves_scene_through_structure&#39;: 1, &#39;person_crawls_out_from_under_vehicle&#39;: 1, &#39;person_feeds_dog&#39;: 1, &#39;person_feeds_cat&#39;: 1, &#39;person_points_to_dog&#39;: 1, &#39;person_throws_object_to_dog&#39;: 1, &#39;person_shakes_hand&#39;: 1, &#39;person_cleans_eyeglasses&#39;: 1, &#39;person_holds_hand&#39;: 1, &#39;person_embraces_sitting_person&#39;: 1}

        
        # Generated using vipy.dataset.Dataset.class_to_index()
        self._class_to_index = {&#39;car_drops_off_person&#39;: 0, &#39;car_moves&#39;: 1, &#39;car_picks_up_person&#39;: 2, &#39;car_reverses&#39;: 3, &#39;car_starts&#39;: 4, &#39;car_stops&#39;: 5, &#39;car_turns_left&#39;: 6, &#39;car_turns_right&#39;: 7, &#39;hand_interacts_with_person&#39;: 8, &#39;person_abandons_package&#39;: 9, &#39;person_adjusts_thermostat&#39;: 10, &#39;person_applies_deodorant&#39;: 11, &#39;person_applies_eye_makeup&#39;: 12, &#39;person_applies_facial_moisturizer&#39;: 13, &#39;person_applies_foundation_makeup&#39;: 14, &#39;person_applies_lip_makeup&#39;: 15, &#39;person_applies_shaving_cream&#39;: 16, &#39;person_applies_sunscreen&#39;: 17, &#39;person_arranges_flowers_in_vase&#39;: 18, &#39;person_attaches_leash_to_dog&#39;: 19, &#39;person_blows_into_hands&#39;: 20, &#39;person_blows_nose&#39;: 21, &#39;person_bounces_ball_on_floor&#39;: 22, &#39;person_bows&#39;: 23, &#39;person_braids_hair_of_person&#39;: 24, &#39;person_brushes_hair&#39;: 25, &#39;person_brushes_hair_of_person&#39;: 26, &#39;person_brushes_teeth&#39;: 27, &#39;person_bumps_into_person&#39;: 28, &#39;person_bumps_into_table&#39;: 29, &#39;person_bumps_into_wall&#39;: 30, &#39;person_burns_hand&#39;: 31, &#39;person_buttons_shirt&#39;: 32, &#39;person_carries_bicycle&#39;: 33, &#39;person_carries_furniture&#39;: 34, &#39;person_carries_groceries&#39;: 35, &#39;person_carries_heavy_object&#39;: 36, &#39;person_carries_laundry_basket&#39;: 37, &#39;person_carries_person_on_back&#39;: 38, &#39;person_carries_person_on_shoulders&#39;: 39, &#39;person_carries_person_over_shoulder&#39;: 40, &#39;person_catches_dropped_object&#39;: 41, &#39;person_catches_object_from_person&#39;: 42, &#39;person_claps_hands&#39;: 43, &#39;person_cleans_dryer_lint_trap&#39;: 44, &#39;person_cleans_eyeglasses&#39;: 45, &#39;person_cleans_table_with_rag&#39;: 46, &#39;person_climbs_down_ladder&#39;: 47, &#39;person_climbs_down_stairs&#39;: 48, &#39;person_climbs_off_chair&#39;: 49, &#39;person_climbs_off_couch&#39;: 50, &#39;person_climbs_off_table&#39;: 51, &#39;person_climbs_on_chair&#39;: 52, &#39;person_climbs_on_couch&#39;: 53, &#39;person_climbs_on_table&#39;: 54, &#39;person_climbs_up_ladder&#39;: 55, &#39;person_climbs_up_stairs&#39;: 56, &#39;person_clips_fingernails&#39;: 57, &#39;person_closes_box&#39;: 58, &#39;person_closes_cabinet&#39;: 59, &#39;person_closes_car_door&#39;: 60, &#39;person_closes_car_hood&#39;: 61, &#39;person_closes_car_trunk&#39;: 62, &#39;person_closes_closet_door&#39;: 63, &#39;person_closes_clothes_washer&#39;: 64, &#39;person_closes_curtains&#39;: 65, &#39;person_closes_dishwasher&#39;: 66, &#39;person_closes_door_with_foot&#39;: 67, &#39;person_closes_door_with_hip&#39;: 68, &#39;person_closes_dresser_drawer&#39;: 69, &#39;person_closes_facility_door&#39;: 70, &#39;person_closes_gate&#39;: 71, &#39;person_closes_home_window&#39;: 72, &#39;person_closes_jar&#39;: 73, &#39;person_closes_jewelry_box&#39;: 74, &#39;person_closes_kitchen_drawer&#39;: 75, &#39;person_closes_laptop&#39;: 76, &#39;person_closes_mailbox&#39;: 77, &#39;person_closes_microwave&#39;: 78, &#39;person_closes_oven_door&#39;: 79, &#39;person_closes_refrigerator&#39;: 80, &#39;person_closes_sliding_door&#39;: 81, &#39;person_closes_suitcase&#39;: 82, &#39;person_covers_face_with_hands&#39;: 83, &#39;person_covers_friend_with_blanket&#39;: 84, &#39;person_covers_with_bedsheets&#39;: 85, &#39;person_covers_with_blanket&#39;: 86, &#39;person_cracks_knuckles&#39;: 87, &#39;person_crates_dog&#39;: 88, &#39;person_crawls&#39;: 89, &#39;person_crawls_out_from_under_vehicle&#39;: 90, &#39;person_crosses_arms&#39;: 91, &#39;person_crosses_legs&#39;: 92, &#39;person_crumples_paper&#39;: 93, &#39;person_dances_in_place&#39;: 94, &#39;person_discards_trash&#39;: 95, &#39;person_dries_dish&#39;: 96, &#39;person_dries_face_with_towel&#39;: 97, &#39;person_dries_hair_with_hairdryer&#39;: 98, &#39;person_dries_hair_with_towel&#39;: 99, &#39;person_dries_hands_with_towel&#39;: 100, &#39;person_drinks_from_beverage_can&#39;: 101, &#39;person_drinks_from_bottle&#39;: 102, &#39;person_drinks_from_cup&#39;: 103, &#39;person_drinks_from_mug&#39;: 104, &#39;person_drinks_from_shotglass&#39;: 105, &#39;person_drinks_from_straw&#39;: 106, &#39;person_drops_object&#39;: 107, &#39;person_drums_on_chest&#39;: 108, &#39;person_dusts_furniture&#39;: 109, &#39;person_eats_apple&#39;: 110, &#39;person_eats_banana&#39;: 111, &#39;person_eats_snack_from_bag&#39;: 112, &#39;person_eats_with_hands&#39;: 113, &#39;person_eats_with_utensil&#39;: 114, &#39;person_embraces_person&#39;: 115, &#39;person_embraces_sitting_person&#39;: 116, &#39;person_enters_car&#39;: 117, &#39;person_enters_scene_through_structure&#39;: 118, &#39;person_exercises_with_jumping_jacks&#39;: 119, &#39;person_exercises_with_lunges&#39;: 120, &#39;person_exercises_with_plank&#39;: 121, &#39;person_exercises_with_pushup&#39;: 122, &#39;person_exercises_with_situp&#39;: 123, &#39;person_exits_car&#39;: 124, &#39;person_exits_pool&#39;: 125, &#39;person_exits_scene_through_structure&#39;: 126, &#39;person_extinguishes_candle&#39;: 127, &#39;person_falls_from_bed&#39;: 128, &#39;person_falls_from_chair&#39;: 129, &#39;person_falls_into_pool&#39;: 130, &#39;person_falls_while_standing&#39;: 131, &#39;person_feeds_cat&#39;: 132, &#39;person_feeds_dog&#39;: 133, &#39;person_files_fingernails&#39;: 134, &#39;person_flips_down_car_wipers&#39;: 135, &#39;person_flips_up_car_wipers&#39;: 136, &#39;person_flosses&#39;: 137, &#39;person_folds_blanket&#39;: 138, &#39;person_folds_pants&#39;: 139, &#39;person_folds_shirt&#39;: 140, &#39;person_folds_socks&#39;: 141, &#39;person_folds_tablecloth&#39;: 142, &#39;person_folds_towel&#39;: 143, &#39;person_gestures_arms_x&#39;: 144, &#39;person_gestures_be_quiet&#39;: 145, &#39;person_gestures_behind_me&#39;: 146, &#39;person_gestures_blow_kiss&#39;: 147, &#39;person_gestures_call_me&#39;: 148, &#39;person_gestures_come_here&#39;: 149, &#39;person_gestures_cut&#39;: 150, &#39;person_gestures_hang_loose&#39;: 151, &#39;person_gestures_heart&#39;: 152, &#39;person_gestures_listen_closely&#39;: 153, &#39;person_gestures_lower_hand&#39;: 154, &#39;person_gestures_number_five&#39;: 155, &#39;person_gestures_number_four&#39;: 156, &#39;person_gestures_number_one&#39;: 157, &#39;person_gestures_number_three&#39;: 158, &#39;person_gestures_peace&#39;: 159, &#39;person_gestures_raise_hand&#39;: 160, &#39;person_gestures_stop&#39;: 161, &#39;person_gestures_swipe_down&#39;: 162, &#39;person_gestures_swipe_left&#39;: 163, &#39;person_gestures_swipe_right&#39;: 164, &#39;person_gestures_swipe_up&#39;: 165, &#39;person_gestures_thumbs_down&#39;: 166, &#39;person_gestures_thumbs_up&#39;: 167, &#39;person_gestures_watch_closely&#39;: 168, &#39;person_gestures_zoom_in&#39;: 169, &#39;person_gestures_zoom_out&#39;: 170, &#39;person_grabs_person_by_bicep&#39;: 171, &#39;person_grabs_person_by_forearm&#39;: 172, &#39;person_grabs_person_by_hair&#39;: 173, &#39;person_grabs_person_by_shoulder&#39;: 174, &#39;person_handstand&#39;: 175, &#39;person_hikes_up_pants&#39;: 176, &#39;person_hits_person_with_pillow&#39;: 177, &#39;person_holds_hand&#39;: 178, &#39;person_holds_object_above_head&#39;: 179, &#39;person_hugs_cat&#39;: 180, &#39;person_hugs_dog&#39;: 181, &#39;person_hugs_person_from_behind&#39;: 182, &#39;person_hugs_stuffed_animal&#39;: 183, &#39;person_inserts_trashbag_into_trashcan&#39;: 184, &#39;person_interacts_with_handheld_game&#39;: 185, &#39;person_interacts_with_laptop&#39;: 186, &#39;person_interacts_with_tablet&#39;: 187, &#39;person_irons_clothes&#39;: 188, &#39;person_jumps&#39;: 189, &#39;person_jumps_into_pool&#39;: 190, &#39;person_jumps_on_bed&#39;: 191, &#39;person_jumps_on_couch&#39;: 192, &#39;person_karate_chop&#39;: 193, &#39;person_karate_kicks&#39;: 194, &#39;person_kicks_car_tires&#39;: 195, &#39;person_kicks_object_to_person&#39;: 196, &#39;person_kisses_cheek_of_person&#39;: 197, &#39;person_kneels&#39;: 198, &#39;person_knocks_over_furniture&#39;: 199, &#39;person_knocks_over_glass&#39;: 200, &#39;person_laughs_with_person&#39;: 201, &#39;person_leaves_scene_through_structure&#39;: 202, &#39;person_licks_fingers&#39;: 203, &#39;person_lies_down_on_bed&#39;: 204, &#39;person_lies_down_on_couch&#39;: 205, &#39;person_lies_down_on_floor&#39;: 206, &#39;person_lifts_dummbells&#39;: 207, &#39;person_lights_candle&#39;: 208, &#39;person_loads_car&#39;: 209, &#39;person_loads_clothes_into_suitcase&#39;: 210, &#39;person_loads_clotheswasher&#39;: 211, &#39;person_loads_dishwasher&#39;: 212, &#39;person_loads_dryer&#39;: 213, &#39;person_loads_groceries_into_cabinet&#39;: 214, &#39;person_loads_groceries_into_refrigerator&#39;: 215, &#39;person_locks_door_with_keys&#39;: 216, &#39;person_looks_at_hands_in_lap&#39;: 217, &#39;person_looks_at_wristwatch&#39;: 218, &#39;person_looks_over_shoulder&#39;: 219, &#39;person_makes_bed&#39;: 220, &#39;person_mops&#39;: 221, &#39;person_nods_head&#39;: 222, &#39;person_nudges_person_with_elbow&#39;: 223, &#39;person_opens_beverage_can&#39;: 224, &#39;person_opens_box&#39;: 225, &#39;person_opens_cabinet&#39;: 226, &#39;person_opens_can_with_can_opener&#39;: 227, &#39;person_opens_car_door&#39;: 228, &#39;person_opens_car_hood&#39;: 229, &#39;person_opens_car_trunk&#39;: 230, &#39;person_opens_closet_door&#39;: 231, &#39;person_opens_clothes_washer&#39;: 232, &#39;person_opens_curtains&#39;: 233, &#39;person_opens_dishwasher&#39;: 234, &#39;person_opens_dresser_drawer&#39;: 235, &#39;person_opens_facility_door&#39;: 236, &#39;person_opens_gate&#39;: 237, &#39;person_opens_home_window&#39;: 238, &#39;person_opens_jar&#39;: 239, &#39;person_opens_jewelry_box&#39;: 240, &#39;person_opens_kitchen_drawer&#39;: 241, &#39;person_opens_laptop&#39;: 242, &#39;person_opens_mailbox&#39;: 243, &#39;person_opens_microwave&#39;: 244, &#39;person_opens_oven_door&#39;: 245, &#39;person_opens_refrigerator&#39;: 246, &#39;person_opens_sliding_door&#39;: 247, &#39;person_opens_suitcase&#39;: 248, &#39;person_paints_fingernails&#39;: 249, &#39;person_pats_head&#39;: 250, &#39;person_pets_cat&#39;: 251, &#39;person_pets_dog&#39;: 252, &#39;person_picks_up_object&#39;: 253, &#39;person_picks_up_object_from_bed&#39;: 254, &#39;person_picks_up_object_from_cabinet&#39;: 255, &#39;person_picks_up_object_from_couch&#39;: 256, &#39;person_picks_up_object_from_countertop&#39;: 257, &#39;person_picks_up_object_from_floor&#39;: 258, &#39;person_picks_up_object_from_shelf&#39;: 259, &#39;person_picks_up_object_from_table&#39;: 260, &#39;person_picks_up_person&#39;: 261, &#39;person_plugs_into_electrical_socket&#39;: 262, &#39;person_points_at_person&#39;: 263, &#39;person_points_to_dog&#39;: 264, &#39;person_polishes_car_with_rag&#39;: 265, &#39;person_pounds_shoulders_of_person&#39;: 266, &#39;person_pours_coffee_into_mug&#39;: 267, &#39;person_pours_into_bowl&#39;: 268, &#39;person_pours_liquid_into_cup&#39;: 269, &#39;person_prays&#39;: 270, &#39;person_pulls_out_chair&#39;: 271, &#39;person_pulls_wheeled_cart&#39;: 272, &#39;person_pulls_wheeled_trashcan&#39;: 273, &#39;person_punches&#39;: 274, &#39;person_purchases_from_cashier&#39;: 275, &#39;person_purchases_from_machine&#39;: 276, &#39;person_pushes_in_chair&#39;: 277, &#39;person_pushes_wheeled_cart&#39;: 278, &#39;person_pushes_wheeled_trashcan&#39;: 279, &#39;person_puts_clothes_into_closet&#39;: 280, &#39;person_puts_clothes_into_dresser&#39;: 281, &#39;person_puts_down_object&#39;: 282, &#39;person_puts_down_object_into_cabinet&#39;: 283, &#39;person_puts_down_object_on_bed&#39;: 284, &#39;person_puts_down_object_on_couch&#39;: 285, &#39;person_puts_down_object_on_countertop&#39;: 286, &#39;person_puts_down_object_on_floor&#39;: 287, &#39;person_puts_down_object_on_shelf&#39;: 288, &#39;person_puts_down_object_on_table&#39;: 289, &#39;person_puts_down_person&#39;: 290, &#39;person_puts_feet_up&#39;: 291, &#39;person_puts_fingers_in_ear&#39;: 292, &#39;person_puts_hair_in_ponytail&#39;: 293, &#39;person_puts_hands_in_back_pockets&#39;: 294, &#39;person_puts_object_in_cabinet&#39;: 295, &#39;person_puts_object_into_backpack&#39;: 296, &#39;person_puts_object_into_bag&#39;: 297, &#39;person_puts_object_into_basket&#39;: 298, &#39;person_puts_object_into_box&#39;: 299, &#39;person_puts_object_into_kitchen_drawer&#39;: 300, &#39;person_puts_object_into_microwave&#39;: 301, &#39;person_puts_object_into_oven&#39;: 302, &#39;person_puts_object_into_purse&#39;: 303, &#39;person_puts_object_into_refrigerator&#39;: 304, &#39;person_puts_object_into_toaster&#39;: 305, &#39;person_puts_on_apron&#39;: 306, &#39;person_puts_on_backpack&#39;: 307, &#39;person_puts_on_belt&#39;: 308, &#39;person_puts_on_boots&#39;: 309, &#39;person_puts_on_earrings&#39;: 310, &#39;person_puts_on_facemask&#39;: 311, &#39;person_puts_on_glasses&#39;: 312, &#39;person_puts_on_gloves&#39;: 313, &#39;person_puts_on_hat&#39;: 314, &#39;person_puts_on_headphones&#39;: 315, &#39;person_puts_on_hoodie&#39;: 316, &#39;person_puts_on_jacket&#39;: 317, &#39;person_puts_on_necklace&#39;: 318, &#39;person_puts_on_necktie&#39;: 319, &#39;person_puts_on_pants&#39;: 320, &#39;person_puts_on_ring&#39;: 321, &#39;person_puts_on_scarf&#39;: 322, &#39;person_puts_on_shirt&#39;: 323, &#39;person_puts_on_shoes&#39;: 324, &#39;person_puts_on_socks&#39;: 325, &#39;person_puts_on_sunglasses&#39;: 326, &#39;person_puts_on_wristwatch&#39;: 327, &#39;person_puts_phone_into_pocket&#39;: 328, &#39;person_puts_up_picture_frame&#39;: 329, &#39;person_puts_up_smoke_detector&#39;: 330, &#39;person_reads_book&#39;: 331, &#39;person_reads_document&#39;: 332, &#39;person_reads_magazine&#39;: 333, &#39;person_reads_newspaper&#39;: 334, &#39;person_removes_trashbag_from_trashcan&#39;: 335, &#39;person_rides_bicycle&#39;: 336, &#39;person_rubs_eyes&#39;: 337, &#39;person_rubs_foot&#39;: 338, &#39;person_rubs_neck&#39;: 339, &#39;person_salutes&#39;: 340, &#39;person_scratches_face&#39;: 341, &#39;person_screws_lid_to_bottle&#39;: 342, &#39;person_searches_in_backback&#39;: 343, &#39;person_searches_in_bag&#39;: 344, &#39;person_searches_in_box&#39;: 345, &#39;person_searches_in_cabinet&#39;: 346, &#39;person_searches_in_couch&#39;: 347, &#39;person_searches_in_kitchen_drawer&#39;: 348, &#39;person_searches_in_purse&#39;: 349, &#39;person_searches_jewelry_box&#39;: 350, &#39;person_searches_under_bed&#39;: 351, &#39;person_searches_under_couch&#39;: 352, &#39;person_sets_table&#39;: 353, &#39;person_sets_upright_furniture&#39;: 354, &#39;person_sets_upright_glass&#39;: 355, &#39;person_shades_eyes&#39;: 356, &#39;person_shakes_hand&#39;: 357, &#39;person_shakes_head&#39;: 358, &#39;person_shaves_face&#39;: 359, &#39;person_shoves_person&#39;: 360, &#39;person_shrugs&#39;: 361, &#39;person_sits_crisscross&#39;: 362, &#39;person_sits_down&#39;: 363, &#39;person_slaps_hands_on_thighs&#39;: 364, &#39;person_snaps_fingers&#39;: 365, &#39;person_sneezees into arm&#39;: 366, &#39;person_sneezes_into_hand&#39;: 367, &#39;person_somersaults&#39;: 368, &#39;person_spills_on_floor&#39;: 369, &#39;person_spills_on_table&#39;: 370, &#39;person_spins_person_around&#39;: 371, &#39;person_sprays_from_bottle&#39;: 372, &#39;person_spreads_tablecloth&#39;: 373, &#39;person_squats&#39;: 374, &#39;person_squeezes_object&#39;: 375, &#39;person_stands_up&#39;: 376, &#39;person_stands_up_from_floor&#39;: 377, &#39;person_steals_object&#39;: 378, &#39;person_sticks_out_tongue&#39;: 379, &#39;person_stirs_mug&#39;: 380, &#39;person_stretches_arms_over_head&#39;: 381, &#39;person_stretches_arms_to_side&#39;: 382, &#39;person_stretches_back&#39;: 383, &#39;person_strokes_chin&#39;: 384, &#39;person_strokes_hair&#39;: 385, &#39;person_stubs_toe&#39;: 386, &#39;person_swats_bug&#39;: 387, &#39;person_sweeps_floor&#39;: 388, &#39;person_takes_clothes_from_closet&#39;: 389, &#39;person_takes_clothes_from_dresser&#39;: 390, &#39;person_takes_down_picture_frame&#39;: 391, &#39;person_takes_down_smoke_detector&#39;: 392, &#39;person_takes_medicine_pills&#39;: 393, &#39;person_takes_object_from_backpack&#39;: 394, &#39;person_takes_object_from_bag&#39;: 395, &#39;person_takes_object_from_basket&#39;: 396, &#39;person_takes_object_from_box&#39;: 397, &#39;person_takes_object_from_cabinet&#39;: 398, &#39;person_takes_object_from_kitchen_drawer&#39;: 399, &#39;person_takes_object_from_microwave&#39;: 400, &#39;person_takes_object_from_oven&#39;: 401, &#39;person_takes_object_from_purse&#39;: 402, &#39;person_takes_object_from_refrigerator&#39;: 403, &#39;person_takes_object_from_toaster&#39;: 404, &#39;person_takes_off_apron&#39;: 405, &#39;person_takes_off_backpack&#39;: 406, &#39;person_takes_off_belt&#39;: 407, &#39;person_takes_off_boots&#39;: 408, &#39;person_takes_off_earrings&#39;: 409, &#39;person_takes_off_facemask&#39;: 410, &#39;person_takes_off_glasses&#39;: 411, &#39;person_takes_off_gloves&#39;: 412, &#39;person_takes_off_hairtie&#39;: 413, &#39;person_takes_off_hat&#39;: 414, &#39;person_takes_off_headphones&#39;: 415, &#39;person_takes_off_hoodie&#39;: 416, &#39;person_takes_off_jacket&#39;: 417, &#39;person_takes_off_necklace&#39;: 418, &#39;person_takes_off_necktie&#39;: 419, &#39;person_takes_off_pants&#39;: 420, &#39;person_takes_off_ring&#39;: 421, &#39;person_takes_off_scarf&#39;: 422, &#39;person_takes_off_shirt&#39;: 423, &#39;person_takes_off_shoes&#39;: 424, &#39;person_takes_off_socks&#39;: 425, &#39;person_takes_off_sunglasses&#39;: 426, &#39;person_takes_off_wristwatch&#39;: 427, &#39;person_takes_phone_from_pocket&#39;: 428, &#39;person_takes_selfie&#39;: 429, &#39;person_takes_selfie_with_person&#39;: 430, &#39;person_talks_on_phone&#39;: 431, &#39;person_talks_to_person&#39;: 432, &#39;person_taps_object_with_finger&#39;: 433, &#39;person_tears_paper&#39;: 434, &#39;person_texts_on_phone&#39;: 435, &#39;person_texts_on_phone_while_sitting&#39;: 436, &#39;person_throws_object_into_air&#39;: 437, &#39;person_throws_object_on_bed&#39;: 438, &#39;person_throws_object_on_table&#39;: 439, &#39;person_throws_object_to_dog&#39;: 440, &#39;person_throws_object_to_ground&#39;: 441, &#39;person_throws_object_to_person&#39;: 442, &#39;person_tickles_person&#39;: 443, &#39;person_ties_jacket_around_waist&#39;: 444, &#39;person_touches_back_of_person&#39;: 445, &#39;person_touches_earlobe&#39;: 446, &#39;person_touches_face_of_person&#39;: 447, &#39;person_transfers_object_to_car&#39;: 448, &#39;person_transfers_object_to_person&#39;: 449, &#39;person_trips_on_object_on_floor&#39;: 450, &#39;person_trips_on_stair&#39;: 451, &#39;person_tucks_in_shirt&#39;: 452, &#39;person_turns_off_fan&#39;: 453, &#39;person_turns_off_faucet&#39;: 454, &#39;person_turns_off_lamp&#39;: 455, &#39;person_turns_off_stovetop&#39;: 456, &#39;person_turns_on_fan&#39;: 457, &#39;person_turns_on_faucet&#39;: 458, &#39;person_turns_on_lamp&#39;: 459, &#39;person_turns_on_stovetop&#39;: 460, &#39;person_twirls&#39;: 461, &#39;person_unattaches_leash_from_dog&#39;: 462, &#39;person_unbuttons_shirt&#39;: 463, &#39;person_uncovers_friend_with_blanket&#39;: 464, &#39;person_uncovers_with_bedsheets&#39;: 465, &#39;person_uncovers_with_blanket&#39;: 466, &#39;person_uncrates_dog&#39;: 467, &#39;person_unfolds_blanket&#39;: 468, &#39;person_unloads_box_onto_floor&#39;: 469, &#39;person_unloads_box_onto_table&#39;: 470, &#39;person_unloads_car&#39;: 471, &#39;person_unloads_clothes_from_suitcase&#39;: 472, &#39;person_unloads_clotheswasher&#39;: 473, &#39;person_unloads_dishwasher&#39;: 474, &#39;person_unloads_dryer&#39;: 475, &#39;person_unlocks_door_with_keys&#39;: 476, &#39;person_unplugs_from_electrical_socket&#39;: 477, &#39;person_unscrews_lid_from_bottle&#39;: 478, &#39;person_unties_jacket_around_waist&#39;: 479, &#39;person_untucks_shirt&#39;: 480, &#39;person_unzips_jacket&#39;: 481, &#39;person_uses_bodyweight_scale&#39;: 482, &#39;person_uses_television_remote&#39;: 483, &#39;person_vacuums_carpet&#39;: 484, &#39;person_vapes&#39;: 485, &#39;person_walks&#39;: 486, &#39;person_walks_around_car&#39;: 487, &#39;person_walks_tiptoe&#39;: 488, &#39;person_washes_dish&#39;: 489, &#39;person_washes_face&#39;: 490, &#39;person_washes_hands&#39;: 491, &#39;person_washes_window&#39;: 492, &#39;person_waters_houseplant&#39;: 493, &#39;person_waves_at_person&#39;: 494, &#39;person_waves_hand_over_object&#39;: 495, &#39;person_wiggles_hips&#39;: 496, &#39;person_wipes_mouth_with_napkin&#39;: 497, &#39;person_wraps_box&#39;: 498, &#39;person_yawns&#39;: 499, &#39;person_zips_up_jacket&#39;: 500}

        
        self._verb_to_noun = {k:set([&#39;car&#39;,&#39;vehicle&#39;,&#39;motorcycle&#39;,&#39;bus&#39;,&#39;truck&#39;]) if (k.startswith(&#39;car&#39;) or k.startswith(&#39;motorcycle&#39;) or k.startswith(&#39;vehicle&#39;)) else set([&#39;person&#39;]) for k in self.classlist()}

        # Generated using vipy.dataset.Dataset.class_to_shortlabel()        
        self._class_to_shortlabel =  {&#39;person_exits_scene_through_structure&#39;: &#39;Leaves scene&#39;, &#39;person_stands_up&#39;: &#39;Stands&#39;, &#39;person_talks_on_phone&#39;: &#39;Talks on phone&#39;, &#39;person_talks_to_person&#39;: &#39;Talks to person&#39;, &#39;person_enters_scene_through_structure&#39;: &#39;Comes into scene&#39;, &#39;person_closes_car_door&#39;: &#39;Unloads&#39;, &#39;person_picks_up_object&#39;: &#39;Picks up&#39;, &#39;person_enters_car&#39;: &#39;Opens&#39;, &#39;person_opens_facility_door&#39;: &#39;Opens door&#39;, &#39;person_sits_down&#39;: &#39;Sits down&#39;, &#39;person_opens_car_trunk&#39;: &#39;Closing&#39;, &#39;person_exits_car&#39;: &#39;Opens&#39;, &#39;person_texts_on_phone&#39;: &#39;Texts on phone&#39;, &#39;person_closes_facility_door&#39;: &#39;Closes door&#39;, &#39;person_opens_car_door&#39;: &#39;Opens&#39;, &#39;person_puts_down_object&#39;: &#39;Puts down&#39;, &#39;hand_interacts_with_person&#39;: &#39;Hold hands&#39;, &#39;person_closes_car_trunk&#39;: &#39;Unloading&#39;, &#39;person_purchases_from_cashier&#39;: &#39;Purchases (cashier)&#39;, &#39;person_carries_heavy_object&#39;: &#39;Carries heavy object&#39;, &#39;person_abandons_package&#39;: &#39;Abandons&#39;, &#39;person_reads_document&#39;: &#39;Reads&#39;, &#39;person_rides_bicycle&#39;: &#39;Rides&#39;, &#39;person_transfers_object_to_person&#39;: &#39;Transfers&#39;, &#39;person_embraces_person&#39;: &#39;Embraces person&#39;, &#39;person_unloads_car&#39;: &#39;Unloads&#39;, &#39;person_loads_car&#39;: &#39;Loads&#39;, &#39;person_interacts_with_laptop&#39;: &#39;Interacts with laptop&#39;, &#39;car_turns_right&#39;: &#39;Turns right&#39;, &#39;car_turns_left&#39;: &#39;Turns left&#39;, &#39;car_stops&#39;: &#39;Stops&#39;, &#39;car_reverses&#39;: &#39;Reverses&#39;, &#39;car_starts&#39;: &#39;Starts&#39;, &#39;car_drops_off_person&#39;: &#39;Stops&#39;, &#39;car_picks_up_person&#39;: &#39;Stops&#39;, &#39;person_steals_object&#39;: &#39;Steals&#39;, &#39;person_walks&#39;: &#39;Walks&#39;, &#39;person_purchases_from_machine&#39;: &#39;Purchases (machine)&#39;, &#39;person_transfers_object_to_car&#39;: &#39;Transferring&#39;, &#39;car_moves&#39;: &#39;moves&#39;, &#39;person_scratches_face&#39;: &#39;Scratches&#39;, &#39;person_drops_object&#39;: &#39;Drops object&#39;, &#39;person_picks_up_object_from_floor&#39;: &#39;Picks up object from floor&#39;, &#39;person_interacts_with_handheld_game&#39;: &#39;Uses&#39;, &#39;person_picks_up_object_from_countertop&#39;: &#39;Picks up object from countertop&#39;, &#39;person_puts_down_object_on_countertop&#39;: &#39;Puts down object on countertop&#39;, &#39;person_gestures_raise_hand&#39;: &#39;Gestures raise hand&#39;, &#39;person_gestures_lower_hand&#39;: &#39;Gestures lower hand&#39;, &#39;person_unlocks_door_with_keys&#39;: &#39;Unlocks door&#39;, &#39;person_locks_door_with_keys&#39;: &#39;Locks door&#39;, &#39;person_gestures_come_here&#39;: &#39;Come here sign&#39;, &#39;person_gestures_peace&#39;: &#39;Peace sign&#39;, &#39;person_nudges_person_with_elbow&#39;: &#39;Nudges with elbow&#39;, &#39;person_touches_back_of_person&#39;: &#39;Touches back of person&#39;, &#39;person_touches_face_of_person&#39;: &#39;Touches face of person&#39;, &#39;person_picks_up_object_from_bed&#39;: &#39;Picks up object from bed&#39;, &#39;person_puts_down_object_on_bed&#39;: &#39;Puts down object on bed&#39;, &#39;person_gestures_thumbs_down&#39;: &#39;Gestures thumbs down&#39;, &#39;person_waves_hand_over_object&#39;: &#39;Waves over&#39;, &#39;person_opens_laptop&#39;: &#39;Opens laptop&#39;, &#39;person_closes_laptop&#39;: &#39;Closes laptop&#39;, &#39;person_bumps_into_wall&#39;: &#39;Bumps into&#39;, &#39;person_waves_at_person&#39;: &#39;Waves at person&#39;, &#39;person_opens_car_hood&#39;: &#39;Opens&#39;, &#39;person_closes_car_hood&#39;: &#39;Closes&#39;, &#39;person_exercises_with_lunges&#39;: &#39;Exercises with lunges&#39;, &#39;person_reads_newspaper&#39;: &#39;Reads&#39;, &#39;person_points_at_person&#39;: &#39;Points to&#39;, &#39;person_gestures_thumbs_up&#39;: &#39;Gestures thumbs up&#39;, &#39;person_squeezes_object&#39;: &#39;Squeezes&#39;, &#39;person_cracks_knuckles&#39;: &#39;Cracks knuckles&#39;, &#39;person_closes_door_with_foot&#39;: &#39;Closes&#39;, &#39;person_gestures_swipe_left&#39;: &#39;Gestures swipe left&#39;, &#39;person_gestures_swipe_right&#39;: &#39;Gestures swipe right&#39;, &#39;person_opens_gate&#39;: &#39;Opens&#39;, &#39;person_closes_gate&#39;: &#39;Closes&#39;, &#39;person_trips_on_object_on_floor&#39;: &#39;Trips&#39;, &#39;person_stands_up_from_floor&#39;: &#39;Stands up from floor&#39;, &#39;person_carries_laundry_basket&#39;: &#39;Carries laundry basket&#39;, &#39;person_squats&#39;: &#39;Squats&#39;, &#39;person_picks_up_object_from_couch&#39;: &#39;Picks up object from couch&#39;, &#39;person_puts_down_object_on_couch&#39;: &#39;Puts down object on couch&#39;, &#39;person_picks_up_object_from_cabinet&#39;: &#39;Picks up&#39;, &#39;person_puts_down_object_into_cabinet&#39;: &#39;Puts down&#39;, &#39;person_exercises_with_jumping_jacks&#39;: &#39;Exercises with jumping jacks&#39;, &#39;person_reads_magazine&#39;: &#39;Reads&#39;, &#39;person_closes_door_with_hip&#39;: &#39;Closes&#39;, &#39;person_carries_groceries&#39;: &#39;Carries groceries&#39;, &#39;person_looks_at_hands_in_lap&#39;: &#39;Looks at&#39;, &#39;person_interacts_with_tablet&#39;: &#39;Uses&#39;, &#39;person_polishes_car_with_rag&#39;: &#39;Polishes&#39;, &#39;person_kisses_cheek_of_person&#39;: &#39;Kisses cheek of person&#39;, &#39;person_exercises_with_situp&#39;: &#39;Exercises with situp&#39;, &#39;person_carries_furniture&#39;: &#39;Carries&#39;, &#39;person_carries_bicycle&#39;: &#39;Carries&#39;, &#39;person_exercises_with_pushup&#39;: &#39;Exercises with pushup&#39;, &#39;person_bows&#39;: &#39;Bows&#39;, &#39;person_sits_crisscross&#39;: &#39;Sits&#39;, &#39;person_reads_book&#39;: &#39;Reads book&#39;, &#39;person_gestures_swipe_up&#39;: &#39;Gestures swipe up&#39;, &#39;person_gestures_swipe_down&#39;: &#39;Gestures swipe down&#39;, &#39;person_looks_at_wristwatch&#39;: &#39;Looks at wristwatch&#39;, &#39;person_kicks_car_tires&#39;: &#39;Kicks&#39;, &#39;person_taps_object_with_finger&#39;: &#39;Taps&#39;, &#39;person_pounds_shoulders_of_person&#39;: &#39;Pounds shoulders of person&#39;, &#39;person_pushes_wheeled_cart&#39;: &#39;Pushes&#39;, &#39;person_pulls_wheeled_cart&#39;: &#39;Pulls&#39;, &#39;person_flips_up_car_wipers&#39;: &#39;Flips up&#39;, &#39;person_flips_down_car_wipers&#39;: &#39;Flips down&#39;, &#39;person_walks_around_car&#39;: &#39;Walks around&#39;, &#39;person_opens_sliding_door&#39;: &#39;Opens&#39;, &#39;person_closes_sliding_door&#39;: &#39;Closes&#39;, &#39;person_drinks_from_bottle&#39;: &#39;Drinks from bottle&#39;, &#39;person_shoves_person&#39;: &#39;Shoves&#39;, &#39;person_jumps&#39;: &#39;Jumps&#39;, &#39;person_eats_snack_from_bag&#39;: &#39;Eats snack from bag&#39;, &#39;person_takes_off_glasses&#39;: &#39;Takes off glasses&#39;, &#39;person_puts_on_glasses&#39;: &#39;Puts on glasses&#39;, &#39;person_lies_down_on_couch&#39;: &#39;Lies down&#39;, &#39;person_lies_down_on_bed&#39;: &#39;Lies down on bed&#39;, &#39;person_puts_on_hat&#39;: &#39;Puts on hat&#39;, &#39;person_takes_off_hat&#39;: &#39;Takes off hat&#39;, &#39;person_puts_on_shoes&#39;: &#39;Puts on shoes&#39;, &#39;person_takes_off_shoes&#39;: &#39;Takes off shoes&#39;, &#39;person_laughs_with_person&#39;: &#39;Laughs&#39;, &#39;person_lies_down_on_floor&#39;: &#39;Lies down&#39;, &#39;person_puts_on_scarf&#39;: &#39;Puts on scarf&#39;, &#39;person_takes_off_scarf&#39;: &#39;Takes off scarf&#39;, &#39;person_takes_off_headphones&#39;: &#39;Takes off headphones&#39;, &#39;person_puts_on_headphones&#39;: &#39;Puts on headphones&#39;, &#39;person_takes_phone_from_pocket&#39;: &#39;Takes phone from pocket&#39;, &#39;person_puts_phone_into_pocket&#39;: &#39;Puts phone in&#39;, &#39;person_puts_on_gloves&#39;: &#39;Puts on gloves&#39;, &#39;person_takes_off_gloves&#39;: &#39;Takes off gloves&#39;, &#39;person_wipes_mouth_with_napkin&#39;: &#39;Wipes mouth with napkin&#39;, &#39;person_drinks_from_straw&#39;: &#39;Drinks&#39;, &#39;person_grabs_person_by_forearm&#39;: &#39;Grabs&#39;, &#39;person_puts_on_jacket&#39;: &#39;Puts on jacket&#39;, &#39;person_zips_up_jacket&#39;: &#39;Zips up jacket&#39;, &#39;person_unzips_jacket&#39;: &#39;Unzips jacket&#39;, &#39;person_takes_off_jacket&#39;: &#39;Takes off jacket&#39;, &#39;person_grabs_person_by_bicep&#39;: &#39;Grabs&#39;, &#39;person_grabs_person_by_shoulder&#39;: &#39;Grabs&#39;, &#39;person_spins_person_around&#39;: &#39;Spins around&#39;, &#39;person_grabs_person_by_hair&#39;: &#39;Grabs&#39;, &#39;person_carries_person_over_shoulder&#39;: &#39;Carries&#39;, &#39;person_puts_down_person&#39;: &#39;Puts down&#39;, &#39;person_picks_up_person&#39;: &#39;Picks up&#39;, &#39;person_searches_in_backback&#39;: &#39;Searches in backback&#39;, &#39;person_takes_object_from_backpack&#39;: &#39;Takes object from backpack&#39;, &#39;person_puts_object_into_backpack&#39;: &#39;Puts object into backpack&#39;, &#39;person_searches_in_purse&#39;: &#39;Searches&#39;, &#39;person_puts_object_into_purse&#39;: &#39;Puts object into purse&#39;, &#39;person_takes_object_from_purse&#39;: &#39;Takes from&#39;, &#39;person_takes_off_facemask&#39;: &#39;Takes off facemask&#39;, &#39;person_puts_on_facemask&#39;: &#39;Puts on facemask&#39;, &#39;person_sweeps_floor&#39;: &#39;Sweeps&#39;, &#39;person_vacuums_carpet&#39;: &#39;Vacuums carpet&#39;, &#39;person_tucks_in_shirt&#39;: &#39;Tucks in shirt&#39;, &#39;person_untucks_shirt&#39;: &#39;Untucks&#39;, &#39;person_applies_deodorant&#39;: &#39;Uses deodorant&#39;, &#39;person_claps_hands&#39;: &#39;Claps&#39;, &#39;person_puts_feet_up&#39;: &#39;Puts feet up&#39;, &#39;person_climbs_up_stairs&#39;: &#39;Climbs up stairs&#39;, &#39;person_climbs_down_stairs&#39;: &#39;Climbs down stairs&#39;, &#39;person_kneels&#39;: &#39;Kneels&#39;, &#39;person_jumps_into_pool&#39;: &#39;Jumps into pool&#39;, &#39;person_exits_pool&#39;: &#39;Exits pool&#39;, &#39;person_puts_on_belt&#39;: &#39;Puts on belt&#39;, &#39;person_takes_off_belt&#39;: &#39;Takes off&#39;, &#39;person_picks_up_object_from_table&#39;: &#39;Picks up object from table&#39;, &#39;person_drinks_from_shotglass&#39;: &#39;Drinks from shotglass&#39;, &#39;person_puts_down_object_on_table&#39;: &#39;Puts down object on table&#39;, &#39;person_karate_kicks&#39;: &#39;Karate kicks&#39;, &#39;person_karate_chop&#39;: &#39;Karate chop&#39;, &#39;person_hikes_up_pants&#39;: &#39;Adjusts pants&#39;, &#39;person_opens_curtains&#39;: &#39;Opens curtains&#39;, &#39;person_closes_curtains&#39;: &#39;Closes curtains&#39;, &#39;person_puts_on_necklace&#39;: &#39;Puts on necklace&#39;, &#39;person_takes_off_necklace&#39;: &#39;Takes off&#39;, &#39;person_slaps_hands_on_thighs&#39;: &#39;Slaps&#39;, &#39;person_flosses&#39;: &#39;Flosses&#39;, &#39;person_plugs_into_electrical_socket&#39;: &#39;Plugs&#39;, &#39;person_unplugs_from_electrical_socket&#39;: &#39;Unplugs&#39;, &#39;person_brushes_teeth&#39;: &#39;Brushes&#39;, &#39;person_puts_on_socks&#39;: &#39;Puts on socks&#39;, &#39;person_takes_off_socks&#39;: &#39;Takes off socks&#39;, &#39;person_puts_on_sunglasses&#39;: &#39;Puts on sunglasses&#39;, &#39;person_takes_off_sunglasses&#39;: &#39;Takes off&#39;, &#39;person_punches&#39;: &#39;Punches&#39;, &#39;person_brushes_hair&#39;: &#39;Brushes hair&#39;, &#39;person_crawls&#39;: &#39;Crawls&#39;, &#39;person_applies_sunscreen&#39;: &#39;Uses sunscreen&#39;, &#39;person_drinks_from_mug&#39;: &#39;Drinks from mug&#39;, &#39;person_stretches_back&#39;: &#39;Stretches back&#39;, &#39;person_stretches_arms_to_side&#39;: &#39;Stretches arms to side&#39;, &#39;person_stretches_arms_over_head&#39;: &#39;Stretches arms over head&#39;, &#39;person_unscrews_lid_from_bottle&#39;: &#39;Unscrews lid from bottle&#39;, &#39;person_pours_liquid_into_cup&#39;: &#39;Pours liquid into cup&#39;, &#39;person_screws_lid_to_bottle&#39;: &#39;Screws lid to bottle&#39;, &#39;person_ties_jacket_around_waist&#39;: &#39;Ties&#39;, &#39;person_unties_jacket_around_waist&#39;: &#39;Unties&#39;, &#39;person_crosses_arms&#39;: &#39;Crosses arms&#39;, &#39;person_opens_home_window&#39;: &#39;Opens home window&#39;, &#39;person_closes_home_window&#39;: &#39;Closes home window&#39;, &#39;person_puts_on_wristwatch&#39;: &#39;Puts on wristwatch&#39;, &#39;person_takes_off_wristwatch&#39;: &#39;Takes off&#39;, &#39;person_turns_off_fan&#39;: &#39;Turns off fan&#39;, &#39;person_turns_on_fan&#39;: &#39;Turns on fan&#39;, &#39;person_puts_on_ring&#39;: &#39;Puts on ring&#39;, &#39;person_takes_off_ring&#39;: &#39;Takes off&#39;, &#39;person_opens_closet_door&#39;: &#39;Opens&#39;, &#39;person_closes_closet_door&#39;: &#39;Closes&#39;, &#39;person_pulls_wheeled_trashcan&#39;: &#39;Pulls trashcan&#39;, &#39;person_pushes_wheeled_trashcan&#39;: &#39;Pushes trashcan&#39;, &#39;person_drums_on_chest&#39;: &#39;Drums&#39;, &#39;person_eats_with_hands&#39;: &#39;Eats with hands&#39;, &#39;person_snaps_fingers&#39;: &#39;Snaps&#39;, &#39;person_folds_socks&#39;: &#39;Folds socks&#39;, &#39;person_dries_dish&#39;: &#39;Dries dish&#39;, &#39;person_puts_on_earrings&#39;: &#39;Puts on earrings&#39;, &#39;person_takes_off_earrings&#39;: &#39;Takes off&#39;, &#39;person_folds_pants&#39;: &#39;Folds pants&#39;, &#39;person_sprays_from_bottle&#39;: &#39;Sprays from bottle&#39;, &#39;person_opens_microwave&#39;: &#39;Opens microwave&#39;, &#39;person_puts_object_into_microwave&#39;: &#39;Puts object into microwave&#39;, &#39;person_closes_microwave&#39;: &#39;Closes microwave&#39;, &#39;person_takes_object_from_microwave&#39;: &#39;Takes object from microwave&#39;, &#39;person_unloads_dishwasher&#39;: &#39;Unloads dishwasher&#39;, &#39;person_washes_hands&#39;: &#39;Washes hands&#39;, &#39;person_dries_hands_with_towel&#39;: &#39;Dries hands with towel&#39;, &#39;person_washes_dish&#39;: &#39;Washes dish&#39;, &#39;person_applies_lip_makeup&#39;: &#39;Applies&#39;, &#39;person_opens_refrigerator&#39;: &#39;Opens refrigerator&#39;, &#39;person_takes_object_from_refrigerator&#39;: &#39;Takes object from refrigerator&#39;, &#39;person_closes_refrigerator&#39;: &#39;Closes refrigerator&#39;, &#39;person_puts_object_into_refrigerator&#39;: &#39;Puts object into refrigerator&#39;, &#39;person_eats_with_utensil&#39;: &#39;Eats with utensil&#39;, &#39;person_opens_cabinet&#39;: &#39;Opens cabinet&#39;, &#39;person_closes_cabinet&#39;: &#39;Closes cabinet&#39;, &#39;person_licks_fingers&#39;: &#39;Licks&#39;, &#39;person_applies_facial_moisturizer&#39;: &#39;Uses moisturizer&#39;, &#39;person_unloads_clotheswasher&#39;: &#39;Unloads&#39;, &#39;person_loads_dryer&#39;: &#39;Loads&#39;, &#39;person_takes_clothes_from_dresser&#39;: &#39;Takes from&#39;, &#39;person_puts_clothes_into_dresser&#39;: &#39;Puts into&#39;, &#39;person_removes_trashbag_from_trashcan&#39;: &#39;Removes trashbag from trashcan&#39;, &#39;person_inserts_trashbag_into_trashcan&#39;: &#39;Inserts trashbag into trashcan&#39;, &#39;person_turns_on_lamp&#39;: &#39;Turns on&#39;, &#39;person_turns_off_lamp&#39;: &#39;Turns off&#39;, &#39;person_throws_object_to_person&#39;: &#39;Throws object to person&#39;, &#39;person_catches_object_from_person&#39;: &#39;Catches&#39;, &#39;person_waters_houseplant&#39;: &#39;Waters&#39;, &#39;person_applies_foundation_makeup&#39;: &#39;Applies&#39;, &#39;person_folds_shirt&#39;: &#39;Folds shirt&#39;, &#39;person_takes_medicine_pills&#39;: &#39;Takes pills&#39;, &#39;person_drinks_from_cup&#39;: &#39;Drinks from cup&#39;, &#39;person_opens_mailbox&#39;: &#39;Opens&#39;, &#39;person_closes_mailbox&#39;: &#39;Closes mailbox&#39;, &#39;person_blows_into_hands&#39;: &#39;Blows&#39;, &#39;person_puts_fingers_in_ear&#39;: &#39;Put into ears&#39;, &#39;person_cleans_table_with_rag&#39;: &#39;Cleans table with rag&#39;, &#39;person_loads_dishwasher&#39;: &#39;Loads&#39;, &#39;person_discards_trash&#39;: &#39;Discards trash&#39;, &#39;person_lights_candle&#39;: &#39;Lights candle&#39;, &#39;person_extinguishes_candle&#39;: &#39;Extinguishes candle&#39;, &#39;person_falls_into_pool&#39;: &#39;Falls into pool&#39;, &#39;person_yawns&#39;: &#39;Yawns&#39;, &#39;person_opens_dresser_drawer&#39;: &#39;Opens&#39;, &#39;person_closes_dresser_drawer&#39;: &#39;Closes&#39;, &#39;person_pulls_out_chair&#39;: &#39;Pulls out chair&#39;, &#39;person_pushes_in_chair&#39;: &#39;Pushes in chair&#39;, &#39;person_kicks_object_to_person&#39;: &#39;Kicks object&#39;, &#39;person_puts_clothes_into_closet&#39;: &#39;Puts into&#39;, &#39;person_takes_clothes_from_closet&#39;: &#39;Takes clothes from closet&#39;, &#39;person_looks_over_shoulder&#39;: &#39;Looks over&#39;, &#39;person_pets_dog&#39;: &#39;Pets dog&#39;, &#39;person_nods_head&#39;: &#39;Nods head&#39;, &#39;person_shakes_head&#39;: &#39;Shakes head&#39;, &#39;person_applies_eye_makeup&#39;: &#39;Applies&#39;, &#39;person_loads_clotheswasher&#39;: &#39;Loads&#39;, &#39;person_unbuttons_shirt&#39;: &#39;Unbuttons&#39;, &#39;person_buttons_shirt&#39;: &#39;Buttons shirt&#39;, &#39;person_opens_jar&#39;: &#39;Opens&#39;, &#39;person_closes_jar&#39;: &#39;Closes&#39;, &#39;person_falls_from_bed&#39;: &#39;Falls from bed&#39;, &#39;person_falls_from_chair&#39;: &#39;Falls from chair&#39;, &#39;person_hugs_cat&#39;: &#39;Hugs cat&#39;, &#39;person_prays&#39;: &#39;Prays&#39;, &#39;person_takes_selfie&#39;: &#39;Takes selfie&#39;, &#39;person_puts_hair_in_ponytail&#39;: &#39;Puts hair in ponytail&#39;, &#39;person_takes_off_hairtie&#39;: &#39;Removes hairtie&#39;, &#39;person_hugs_dog&#39;: &#39;Hugs dog&#39;, &#39;person_pets_cat&#39;: &#39;Pets cat&#39;, &#39;person_picks_up_object_from_shelf&#39;: &#39;Picks up object from shelf&#39;, &#39;person_puts_down_object_on_shelf&#39;: &#39;Puts down object on shelf&#39;, &#39;person_shrugs&#39;: &#39;Shrugs&#39;, &#39;person_opens_can_with_can_opener&#39;: &#39;Opens&#39;, &#39;person_rubs_neck&#39;: &#39;Rubs&#39;, &#39;person_takes_selfie_with_person&#39;: &#39;Takes selfie&#39;, &#39;person_wiggles_hips&#39;: &#39;Wiggles&#39;, &#39;person_dances_in_place&#39;: &#39;Dances in place&#39;, &#39;person_trips_on_stair&#39;: &#39;Trips on stair&#39;, &#39;person_sneezees into arm&#39;: &#39;Sneezees into arm&#39;, &#39;person_throws_object_on_table&#39;: &#39;Throws&#39;, &#39;person_touches_earlobe&#39;: &#39;Touches ear&#39;, &#39;person_twirls&#39;: &#39;Twirls&#39;, &#39;person_unloads_dryer&#39;: &#39;Unloads&#39;, &#39;person_shades_eyes&#39;: &#39;Shades eyes&#39;, &#39;person_washes_face&#39;: &#39;Washes&#39;, &#39;person_dries_face_with_towel&#39;: &#39;Dries&#39;, &#39;person_throws_object_to_ground&#39;: &#39;Throws&#39;, &#39;person_tickles_person&#39;: &#39;Tickles&#39;, &#39;person_bumps_into_person&#39;: &#39;Bumps into person&#39;, &#39;person_throws_object_on_bed&#39;: &#39;Throws&#39;, &#39;person_rubs_eyes&#39;: &#39;Rubs&#39;, &#39;person_stubs_toe&#39;: &#39;Stubs toe&#39;, &#39;person_sneezes_into_hand&#39;: &#39;Sneezes into hand&#39;, &#39;person_vapes&#39;: &#39;Vapes&#39;, &#39;person_tears_paper&#39;: &#39;Tears paper&#39;, &#39;person_spills_on_table&#39;: &#39;Spills on table&#39;, &#39;person_sets_upright_glass&#39;: &#39;Sets upright glass&#39;, &#39;person_covers_friend_with_blanket&#39;: &#39;Covers friend with blanket&#39;, &#39;person_uncovers_friend_with_blanket&#39;: &#39;Uncovers friend with blanket&#39;, &#39;person_takes_off_boots&#39;: &#39;Takes off boots&#39;, &#39;person_puts_on_boots&#39;: &#39;Puts on boots&#39;, &#39;person_uses_television_remote&#39;: &#39;Uses television remote&#39;, &#39;person_sets_table&#39;: &#39;Sets table&#39;, &#39;person_puts_on_apron&#39;: &#39;Puts on apron&#39;, &#39;person_takes_off_apron&#39;: &#39;Takes off apron&#39;, &#39;person_turns_on_stovetop&#39;: &#39;Turns on stovetop&#39;, &#39;person_turns_off_stovetop&#39;: &#39;Turns off stovetop&#39;, &#39;person_spreads_tablecloth&#39;: &#39;Spreads tablecloth&#39;, &#39;person_folds_tablecloth&#39;: &#39;Folds tablecloth&#39;, &#39;person_folds_blanket&#39;: &#39;Folds blanket&#39;, &#39;person_unfolds_blanket&#39;: &#39;Unfolds blanket&#39;, &#39;person_covers_with_blanket&#39;: &#39;Covers with blanket&#39;, &#39;person_uncovers_with_blanket&#39;: &#39;Uncovers with blanket&#39;, &#39;person_crumples_paper&#39;: &#39;Crumples paper&#39;, &#39;person_sets_upright_furniture&#39;: &#39;Sets upright furniture&#39;, &#39;person_knocks_over_furniture&#39;: &#39;Knock over furniture&#39;, &#39;person_knocks_over_glass&#39;: &#39;Knocks over glass&#39;, &#39;person_loads_groceries_into_refrigerator&#39;: &#39;Loads groceries (refrigerator)&#39;, &#39;person_uncovers_with_bedsheets&#39;: &#39;Uncovers with bedsheets&#39;, &#39;person_covers_with_bedsheets&#39;: &#39;Covers with bedsheets&#39;, &#39;person_clips_fingernails&#39;: &#39;Clips fingernails&#39;, &#39;person_loads_groceries_into_cabinet&#39;: &#39;Loads groceries (cabinet)&#39;, &#39;person_files_fingernails&#39;: &#39;Files fingernails&#39;, &#39;person_loads_clothes_into_suitcase&#39;: &#39;Loads clothes into suitcase&#39;, &#39;person_mops&#39;: &#39;Mops&#39;, &#39;person_spills_on_floor&#39;: &#39;Spills on floor&#39;, &#39;person_washes_window&#39;: &#39;Washes window&#39;, &#39;person_unloads_clothes_from_suitcase&#39;: &#39;Unloads clothes from suitcase&#39;, &#39;person_puts_on_backpack&#39;: &#39;Puts on backpack&#39;, &#39;person_takes_off_backpack&#39;: &#39;Takes off backpack&#39;, &#39;person_opens_jewelry_box&#39;: &#39;Opens jewelry box&#39;, &#39;person_searches_jewelry_box&#39;: &#39;Searches jewelry box&#39;, &#39;person_closes_jewelry_box&#39;: &#39;Closes jewelry box&#39;, &#39;person_lifts_dummbells&#39;: &#39;Lifts dumbbells&#39;, &#39;person_swats_bug&#39;: &#39;Swats bug&#39;, &#39;person_irons_clothes&#39;: &#39;Irons clothes&#39;, &#39;person_uses_bodyweight_scale&#39;: &#39;Uses scale&#39;, &#39;person_makes_bed&#39;: &#39;Makes bed&#39;, &#39;person_unloads_box_onto_floor&#39;: &#39;Unloads box (floor)&#39;, &#39;person_unloads_box_onto_table&#39;: &#39;Unloads box (table)&#39;, &#39;person_searches_in_cabinet&#39;: &#39;Searches (cabinet)&#39;, &#39;person_dusts_furniture&#39;: &#39;Dusts furniture&#39;, &#39;person_handstand&#39;: &#39;Handstands&#39;, &#39;person_opens_beverage_can&#39;: &#39;Opens beverage can&#39;, &#39;person_drinks_from_beverage_can&#39;: &#39;Drinks from beverage can&#39;, &#39;person_somersaults&#39;: &#39;Somersaults&#39;, &#39;person_jumps_on_couch&#39;: &#39;Jumps on couch&#39;, &#39;person_climbs_up_ladder&#39;: &#39;Climbs up ladder&#39;, &#39;person_climbs_down_ladder&#39;: &#39;Climbs down ladder&#39;, &#39;person_covers_face_with_hands&#39;: &#39;Covers face&#39;, &#39;person_climbs_on_table&#39;: &#39;Climbs on table&#39;, &#39;person_climbs_off_table&#39;: &#39;Climbs off table&#39;, &#39;person_arranges_flowers_in_vase&#39;: &#39;Arranges flowers&#39;, &#39;person_crosses_legs&#39;: &#39;Crosses legs&#39;, &#39;person_climbs_on_chair&#39;: &#39;Climbs on chair&#39;, &#39;person_climbs_off_chair&#39;: &#39;Climbs off chair&#39;, &#39;person_bumps_into_table&#39;: &#39;Bumps into&#39;, &#39;person_climbs_on_couch&#39;: &#39;Climbs on couch&#39;, &#39;person_climbs_off_couch&#39;: &#39;Climbs off couch&#39;, &#39;person_opens_box&#39;: &#39;Opens box&#39;, &#39;person_closes_box&#39;: &#39;Closes box&#39;, &#39;person_cleans_dryer_lint_trap&#39;: &#39;Cleans&#39;, &#39;person_strokes_hair&#39;: &#39;Strokes hair&#39;, &#39;person_puts_on_shirt&#39;: &#39;Puts on shirt&#39;, &#39;person_takes_off_shirt&#39;: &#39;Takes off&#39;, &#39;person_applies_shaving_cream&#39;: &#39;Applies&#39;, &#39;person_shaves_face&#39;: &#39;Shaves&#39;, &#39;person_jumps_on_bed&#39;: &#39;Jumps on bed&#39;, &#39;person_puts_hands_in_back_pockets&#39;: &#39;Puts hands in&#39;, &#39;person_searches_in_bag&#39;: &#39;Searches in bag&#39;, &#39;person_folds_towel&#39;: &#39;Folds towel&#39;, &#39;person_puts_on_pants&#39;: &#39;Puts on pants&#39;, &#39;person_takes_object_from_toaster&#39;: &#39;Takes from&#39;, &#39;person_puts_object_into_toaster&#39;: &#39;Puts into&#39;, &#39;person_searches_under_couch&#39;: &#39;Searches&#39;, &#39;person_puts_up_picture_frame&#39;: &#39;Puts up picture frame&#39;, &#39;person_takes_down_picture_frame&#39;: &#39;Takes down picture frame&#39;, &#39;person_strokes_chin&#39;: &#39;Strokes chin&#39;, &#39;person_hugs_stuffed_animal&#39;: &#39;Hugs stuffy&#39;, &#39;person_crawls_out_from_under_vehicle&#39;: &#39;Crawls&#39;, &#39;person_puts_on_hoodie&#39;: &#39;Puts on hoodie&#39;, &#39;person_takes_off_hoodie&#39;: &#39;Takes off hoodie&#39;, &#39;person_searches_in_box&#39;: &#39;Searches in box&#39;, &#39;person_adjusts_thermostat&#39;: &#39;Adjusts&#39;, &#39;person_stirs_mug&#39;: &#39;Stirs&#39;, &#39;person_opens_oven_door&#39;: &#39;Opens oven door&#39;, &#39;person_closes_oven_door&#39;: &#39;Closes oven door&#39;, &#39;person_burns_hand&#39;: &#39;Burns hand&#39;, &#39;person_searches_in_couch&#39;: &#39;Searches couch&#39;, &#39;person_salutes&#39;: &#39;Salutes&#39;, &#39;person_crates_dog&#39;: &#39;Crates dog&#39;, &#39;person_uncrates_dog&#39;: &#39;Uncrates dog&#39;, &#39;person_bounces_ball_on_floor&#39;: &#39;Bounces ball on floor&#39;, &#39;person_takes_off_pants&#39;: &#39;Takes off pants&#39;, &#39;person_puts_object_into_oven&#39;: &#39;Puts object into oven&#39;, &#39;person_takes_object_from_oven&#39;: &#39;Takes object from oven&#39;, &#39;person_searches_under_bed&#39;: &#39;Searches&#39;, &#39;person_attaches_leash_to_dog&#39;: &#39;Attaches leash to dog&#39;, &#39;person_unattaches_leash_from_dog&#39;: &#39;Unattaches leash&#39;, &#39;person_takes_down_smoke_detector&#39;: &#39;Takes down&#39;, &#39;person_puts_up_smoke_detector&#39;: &#39;Puts up&#39;, &#39;person_holds_object_above_head&#39;: &#39;Holds object up&#39;, &#39;person_rubs_foot&#39;: &#39;Rubs foot&#39;, &#39;person_gestures_listen_closely&#39;: &#39;Gestures listen closely&#39;, &#39;person_gestures_blow_kiss&#39;: &#39;Gestures blow kiss&#39;, &#39;person_dries_hair_with_hairdryer&#39;: &#39;Blow dries hair&#39;, &#39;person_gestures_cut&#39;: &#39;Gestures cut&#39;, &#39;person_gestures_watch_closely&#39;: &#39;Gestures watch closely&#39;, &#39;person_gestures_be_quiet&#39;: &#39;Gestures be quiet&#39;, &#39;person_blows_nose&#39;: &#39;Blows nose&#39;, &#39;person_sticks_out_tongue&#39;: &#39;Sticks out tongue&#39;, &#39;person_dries_hair_with_towel&#39;: &#39;Towel dries hair&#39;, &#39;person_gestures_arms_x&#39;: &#39;Gestures arms X&#39;, &#39;person_gestures_behind_me&#39;: &#39;Gestures behind me&#39;, &#39;person_pats_head&#39;: &#39;Pats head&#39;, &#39;person_puts_on_necktie&#39;: &#39;Puts on necktie&#39;, &#39;person_takes_off_necktie&#39;: &#39;Takes off necktie&#39;, &#39;person_paints_fingernails&#39;: &#39;Paints fingernails&#39;, &#39;person_wraps_box&#39;: &#39;Wraps box&#39;, &#39;person_carries_person_on_shoulders&#39;: &#39;Carries person on shoulders&#39;, &#39;person_braids_hair_of_person&#39;: &#39;Braids hair&#39;, &#39;person_carries_person_on_back&#39;: &#39;Carries person on back&#39;, &#39;person_gestures_hang_loose&#39;: &#39;Gestures hang loose&#39;, &#39;person_throws_object_into_air&#39;: &#39;Throws object into air&#39;, &#39;person_gestures_zoom_in&#39;: &#39;Gestures zoom in&#39;, &#39;person_gestures_zoom_out&#39;: &#39;Gestures zoom out&#39;, &#39;person_gestures_number_four&#39;: &#39;Gestures number four&#39;, &#39;person_brushes_hair_of_person&#39;: &#39;Brushes hair of person&#39;, &#39;person_gestures_heart&#39;: &#39;Gestures heart&#39;, &#39;person_gestures_number_one&#39;: &#39;Gestures number one&#39;, &#39;person_gestures_number_five&#39;: &#39;Gestures number five&#39;, &#39;person_gestures_number_three&#39;: &#39;Gestures number three&#39;, &#39;person_gestures_call_me&#39;: &#39;Gestures call me&#39;, &#39;person_gestures_stop&#39;: &#39;Gestures stop&#39;, &#39;person_walks_tiptoe&#39;: &#39;Walks tiptoe&#39;, &#39;person_pours_coffee_into_mug&#39;: &#39;Pours coffee&#39;, &#39;person_hugs_person_from_behind&#39;: &#39;Hugs from behind&#39;, &#39;person_opens_clothes_washer&#39;: &#39;Opens clothes washer&#39;, &#39;person_closes_clothes_washer&#39;: &#39;Closes clothes washer&#39;, &#39;person_pours_into_bowl&#39;: &#39;Pours into bowl&#39;, &#39;person_searches_in_kitchen_drawer&#39;: &#39;Searches in kitchen drawer&#39;, &#39;person_puts_down_object_on_floor&#39;: &#39;Puts down object on floor&#39;, &#39;person_falls_while_standing&#39;: &#39;Falls while standing&#39;, &#39;person_opens_suitcase&#39;: &#39;Opens suitcase&#39;, &#39;person_closes_suitcase&#39;: &#39;Closes suitcase&#39;, &#39;person_opens_kitchen_drawer&#39;: &#39;Opens kitchen drawer&#39;, &#39;person_closes_kitchen_drawer&#39;: &#39;Closes kitchen drawer&#39;, &#39;person_turns_off_faucet&#39;: &#39;Turns off faucet&#39;, &#39;person_takes_object_from_cabinet&#39;: &#39;Takes object from cabinet&#39;, &#39;person_takes_object_from_kitchen_drawer&#39;: &#39;Takes object from kitchen drawer&#39;, &#39;person_puts_object_into_kitchen_drawer&#39;: &#39;Puts object into kitchen drawer&#39;, &#39;person_turns_on_faucet&#39;: &#39;Turns on faucet&#39;, &#39;person_texts_on_phone_while_sitting&#39;: &#39;Texts on phone while sitting&#39;, &#39;person_exercises_with_plank&#39;: &#39;Exercises with plank&#39;, &#39;person_takes_object_from_bag&#39;: &#39;Takes object from bag&#39;, &#39;person_puts_object_in_cabinet&#39;: &#39;Puts object in cabinet&#39;, &#39;person_hits_person_with_pillow&#39;: &#39;Hits with pillow&#39;, &#39;person_takes_object_from_basket&#39;: &#39;Takes object from basket&#39;, &#39;person_puts_object_into_basket&#39;: &#39;Puts object into basket&#39;, &#39;person_catches_dropped_object&#39;: &#39;Catches dropped object&#39;, &#39;person_eats_apple&#39;: &#39;Eats apple&#39;, &#39;person_eats_banana&#39;: &#39;Eats banana&#39;, &#39;person_puts_object_into_bag&#39;: &#39;Puts object into bag&#39;, &#39;person_opens_dishwasher&#39;: &#39;Opens dishwasher&#39;, &#39;person_closes_dishwasher&#39;: &#39;Closes dishwasher&#39;, &#39;person_leaves_scene_through_structure&#39;: &#39;Leaves scene&#39;, &#39;person_feeds_cat&#39;: &#39;Feeds cat&#39;, &#39;person_points_to_dog&#39;: &#39;Points to dog&#39;, &#39;person_throws_object_to_dog&#39;: &#39;Throws object to dog&#39;, &#39;person_feeds_dog&#39;: &#39;Feeds dog&#39;, &#39;person_embraces_sitting_person&#39;: &#39;Embraces sitting person&#39;, &#39;person_holds_hand&#39;: &#39;Holds hand&#39;, &#39;person_shakes_hand&#39;: &#39;Shakes hand&#39;, &#39;person_takes_object_from_box&#39;: &#39;Takes object from box&#39;, &#39;person_puts_object_into_box&#39;: &#39;Puts object into box&#39;, &#39;person_cleans_eyeglasses&#39;: &#39;Cleans eyeglasses&#39;}

        if modelfile is not None:
            self._load_trained(modelfile)
        else:
            self._load_pretrained()
            self.net.fc = nn.Linear(self.net.fc.in_features, self.num_classes())
        

    #---- &lt;LIGHTNING&gt;
    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x[&#39;val_loss&#39;] for x in outputs]).mean()
        self.log(&#39;val_loss&#39;, avg_loss, on_epoch=True, prog_bar=False, logger=True)
        self.log(&#39;avg_val_loss&#39;, avg_loss, on_epoch=True, prog_bar=True, logger=True)                
        #return {&#39;val_loss&#39;: avg_loss, &#39;avg_val_loss&#39;: avg_loss}   # as of 9.1, this does not return anything
    #---- &lt;/LIGHTNING&gt;
        
    def totensor(self, v=None, training=False, validation=False, show=False, doflip=False, asjson=False):
        &#34;&#34;&#34;Return captured lambda function if v=None, else return tensor&#34;&#34;&#34;    
        assert v is None or isinstance(v, vipy.video.Scene), &#34;Invalid input&#34;
        f = (lambda v, num_frames=self._num_frames, input_size=self._input_size, mean=self._mean, std=self._std, training=training, validation=validation, show=show, classname=self.__class__.__name__:
             PIP_370k._totensor(v, training, validation, input_size, num_frames, mean, std, noflip=[&#39;car_turns_left&#39;, &#39;car_turns_right&#39;, &#39;vehicle_turns_left&#39;, &#39;vehicle_turns_right&#39;, &#39;motorcycle_turns_left&#39;, &#39;motorcycle_turns_right&#39;], show=show, doflip=doflip, asjson=asjson, classname=classname))
        return f(v) if v is not None else f
    

class ActivityTracker(PIP_370k):
    &#34;&#34;&#34;Video Activity detection.
        
    Args (__call__):
        vi [generator of `vipy.video.Scene`]:  The input video to be updated in place with detections.  This is a generator which is output from heyvi.detection.MultiscaleVideoTracker.__call__
        activityiou [float]: The minimum temporal iou for activity assignment
        mirror [bool]:  If true, encode using the mean of a video encoding and the mirrored video encoding.  This is slower as it requires 2x GPU forward passes
        minprob [float]: The minimum probability for new activity detection
        trackconf [float]: The minimum object detection confidence for new tracks
        maxdets [int]:  The maximum number of allowable detections per frame.  If there are more detections per frame tha maxdets, sort them by confidence and use only the top maxdets best
        avgdets [int]:  The number of allowable detections per frame if throttled
        buffered [bool]:  If true, then buffer streams.  This is useful for activity detection on live streams.            
        finalized [bool, int]:  If False then do not finalize(), If True finalize() only at the end, If int, then finalize every int frames.  This is useful for streaming activity detection on unbounded inputs. 
        
    Returns:
        The input video is updated in place.
    
    &#34;&#34;&#34;    
    def __init__(self, stride=3, activities=None, gpus=None, batchsize=None, mlbl=False, mlfl=False, modelfile=None):
        assert modelfile is not None, &#34;Contact &lt;info@visym.com&gt; for access to non-public model files&#34;

        super().__init__(pretrained=False, modelfile=modelfile, mlbl=mlbl, mlfl=mlfl)
        self._stride = stride
        self._allowable_activities = {k:v for (k,v) in [(a,a) if not isinstance(a, tuple) else a for a in activities]} if activities is not None else {k:k for k in self.classlist()}
        self._batchsize_per_gpu = batchsize
        self._gpus = gpus

        if gpus is not None:
            assert torch.cuda.is_available()
            assert batchsize is not None
            self._devices = [&#39;cuda:%d&#39; % k for k in gpus]
            self._gpus = [copy.deepcopy(self.net).to(d, non_blocking=False) for d in self._devices]  
            for m in self._gpus:
                m.eval()
        torch.set_grad_enabled(False)

    def temporal_stride(self, s=None):
        if s is not None:
            self._stride = s
            return self
        else:
            return self._stride

    def forward(self, x):
        &#34;&#34;&#34;Overload forward for multi-gpu batch.  Don&#39;t use torch DataParallel!&#34;&#34;&#34;
        if self._gpus is None:
            return super().forward(x)  # cpu
        else:
            x_forward = None            
            for b in x.pin_memory().split(self._batchsize_per_gpu*len(self._gpus)):  # pinned copy
                n_todevice = np.sum(np.array([1 if k&lt;len(b) else 0 for k in range(int(len(self._devices)*np.ceil(len(b)/len(self._devices))))]).reshape(-1, len(self._devices)), axis=0).tolist()
                todevice = [t.to(d, non_blocking=True) for (t,d) in zip(b.split(n_todevice), self._devices) if len(t)&gt;0]   # async device copy
                ondevice = [m(t) for (m,t) in zip(self._gpus, todevice)]   # async
                fromdevice = torch.cat([t.cpu() for t in ondevice], dim=0)
                x_forward = fromdevice if x_forward is None else torch.cat((x_forward, fromdevice), dim=0)
                del ondevice, todevice, fromdevice, b  # force garbage collection of GPU memory
            del x  # force garbage collection
            return x_forward

    def lrt(self, x_logits, lrt_threshold=None):
        &#34;&#34;&#34;top-k with likelihood ratio test with background null hypothesis&#34;&#34;&#34;
        j_bg_person = self._class_to_index[&#39;person&#39;] if &#39;person&#39; in self._class_to_index else self._class_to_index[&#39;person_walks&#39;]  # FIXME
        j_bg_vehicle = self._class_to_index[&#39;vehicle&#39;] if &#39;vehicle&#39; in self._class_to_index else self._class_to_index[&#39;car_moves&#39;]  # FIXME

        yh = x_logits.detach().cpu().numpy()
        yh_softmax = F.softmax(x_logits.detach().cpu(), dim=1)
        p_null = np.maximum(yh[:, j_bg_person], yh[:, j_bg_vehicle]).reshape(yh.shape[0], 1)
        lr = yh - p_null   # ~= log likelihood ratio
        f_logistic = lambda x,b,s=1.0: float(1.0 / (1.0 + np.exp(-s*(x + b))))
        return [sorted([(self.index_to_class(j), float(s[j]), float(t[j]), f_logistic(s[j], 1.0)*f_logistic(t[j], 0.0), float(sm[j])) for j in range(len(s)) if (lrt_threshold is None or t[j] &gt;= lrt_threshold)], key=lambda x: x[3], reverse=True) for (s,t,sm) in zip(yh, lr, yh_softmax)]


    def finalize(self, vo, trackconf=None, activityconf=None, startframe=None, endframe=None):
        &#34;&#34;&#34;In place filtering of video to finalize&#34;&#34;&#34;
        assert isinstance(vo, vipy.video.Scene)

        tofinalize = set([ai for (ai,a) in vo.activities().items() if (endframe is None or a.endframe() &lt;= endframe) and (startframe is None or a.endframe() &gt;= startframe)])
        tofinalize = tofinalize.union([ti for (ti,t) in vo.tracks().items() if ((endframe is None or t.endframe() &lt;= endframe) and (startframe is None or t.endframe() &gt;= startframe)) or any([ti == vo.activities(id=ai).actorid() for ai in tofinalize])])

        # Bad tracks:  Remove low confidence or too short non-moving tracks, and associated activities
        # - will throw exception that &#39;vo referenced before assignment&#39; if one loop did not succceed
        if trackconf is not None:
            vo.trackfilter(lambda t: t.id() not in tofinalize or len(t)&gt;=vo.framerate() and (t.confidence() &gt;= trackconf or t.startbox().iou(t.endbox()) == 0)).activityfilter(lambda a: a.id() not in tofinalize or a.actorid() in vo.tracks())  
        
        # Activity probability:  noun_probability*verb probability
        nounconf = {k:t.confidence(samples=8) for (k,t) in vo.tracks().items() if t.id() in tofinalize}   # 
        vo.activitymap(lambda a: a.confidence(nounconf[a.actorid()]*a.confidence()) if a.id() in tofinalize else a)
        
        # Missing objects:  Significantly reduce confidence of complex classes (yuck)
        vo.activitymap(lambda a: a.confidence(0.01*a.confidence()) if (a.id() in tofinalize and a.category() in [&#39;person_purchases&#39;]) else a) 
        
        # Vehicle turns:  High confidence vehicle turns must be a minimum angle
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                      (a.category() in [&#39;vehicle_turns_left&#39;, &#39;vehicle_turns_right&#39;]) and
                                                                      (abs(vo.track(a.actorid()).bearing_change(a.startframe(), a.endframe(), dt=vo.framerate(), samples=5)) &lt; (np.pi/16))) else a) 
        
        # Vehicle turns:  U-turn can only be distinguished from left/right turn at the end of a track by looking at the turn angle
        vo.activitymap(lambda a: a.category(&#39;vehicle_makes_u_turn&#39;).shortlabel(&#39;u turn&#39;) if (a.id() in tofinalize and
                                                                                             (a.category() in [&#39;vehicle_turns_left&#39;, &#39;vehicle_turns_right&#39;]) and
                                                                                             (abs(vo.track(a.actorid()).bearing_change(a.startframe(), a.endframe(), dt=vo.framerate(), samples=5)) &gt; (np.pi-(np.pi/2)))) else a)
        
        # Background activities:  Use logistic confidence on logit due to lack of background class &#34;person stands&#34;, otherwise every standing person is using a phone
        f_logistic = lambda x,b,s=1.0: float(1.0 / (1.0 + np.exp(-s*(x + b))))
        vo.activitymap(lambda a: a.confidence(a.confidence()*f_logistic(a.attributes[&#39;logit&#39;], -1.5)) if a.id() in tofinalize else a)

        # Complex activities: remove steal/abandon and replace with picks up / puts down
        vo.activityfilter(lambda a: a.category() not in [&#39;person_steals_object&#39;, &#39;person_abandons_package&#39;])
        newlist = [vo.add(vipy.activity.Activity(startframe=a.startframe(), endframe=a.endframe(), category=&#39;person_steals_object&#39;, shortlabel=&#39;steals&#39;, confidence=0.5*a.confidence(), framerate=vo.framerate(), actorid=a.actorid(), attributes={&#39;pip&#39;:&#39;person_picks_up_object&#39;}))
                   for a in vo.activitylist() if a.category() == &#39;person_picks_up_object&#39;]
        newlist = [vo.add(vipy.activity.Activity(startframe=a.startframe(), endframe=a.endframe(), category=&#39;person_abandons_package&#39;, shortlabel=&#39;abandons&#39;, confidence=0.5*a.confidence(), framerate=vo.framerate(), actorid=a.actorid(), attributes={&#39;pip&#39;:&#39;person_puts_down_object&#39;}))
                   for a in vo.activitylist() if a.category() == &#39;person_puts_down_object&#39;]
            
        # Vehicle/person interaction: &#39;vehicle_drops_off_person&#39;/&#39;vehicle_picks_up_person&#39;  must be followed by car driving away/pulling up, must be accompanied by person track start/end
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                      a.category() == &#39;vehicle_drops_off_person&#39; and
                                                                      (not vo.track(a.actorid()).ismoving(a.middleframe(), a.endframe()+10*vo.framerate()) or
                                                                       not any([t.category() == &#39;person&#39; and t.segment_maxiou(vo.track(a._actorid), t.startframe(), t.startframe()+1) &gt; 0 for t in vo.tracks().values()]))) else a)
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                      a.category() == &#39;vehicle_picks_up_person&#39; and
                                                                      (not vo.track(a.actorid()).ismoving(a.startframe()-10*vo.framerate(), a.middleframe()) or
                                                                       not any([t.category() == &#39;person&#39; and t.segment_maxiou(vo.track(a._actorid), t.endframe()-1, t.endframe()) &gt; 0 for t in vo.tracks().values()]))) else a)
        
        # Person/Bicycle track: riding must be accompanied by an associated moving bicycle track
        vo.activityfilter(lambda a: a.id() not in tofinalize or a.category() != &#39;person_rides_bicycle&#39;)
        bikelist = [vo.add(vipy.activity.Activity(startframe=t.startframe(), endframe=t.endframe(), category=&#39;person_rides_bicycle&#39;, shortlabel=&#39;rides&#39;, confidence=t.confidence(samples=8), framerate=vo.framerate(), actorid=t.id(), attributes={&#39;pip&#39;:&#39;person_rides_bicycle&#39;}))
                    for (tk,t) in vo.tracks().items() if (t.id() in tofinalize and t.category() == &#39;bicycle&#39; and t.ismoving())]
        
        # Person/Vehicle track: person/vehicle interaction must be accompanied by an associated stopped vehicle track
        dstbox = {k:vo.track(a.actorid()).boundingbox(a.startframe(), a.endframe()) for (k,a) in vo.activities().items() if (a.id() in tofinalize and a.category().startswith(&#39;person&#39;) and (&#39;vehicle&#39; in a.category() or &#39;trunk&#39; in a.category()))}  # precompute            
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                      (a.category().startswith(&#39;person&#39;) and (&#39;vehicle&#39; in a.category() or &#39;trunk&#39; in a.category())) and
                                                                      not any([t.category() == &#39;vehicle&#39; and 
                                                                               t.during(a.startframe()) and
                                                                               not t.ismoving(a.startframe(), a.endframe()) and
                                                                               t[a.startframe()].hasintersection(dstbox[a._id])
                                                                               for t in vo.tracks().values()])) else a)
        
        # Vehicle/Person track: vehicle/person interaction must be accompanied by an associated person track
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and (a.category().startswith(&#39;vehicle&#39;) and (&#39;person&#39; in a.category())) and not any([t.category() == &#39;person&#39; and t.segment_maxiou(vo.track(a._actorid), a._startframe, a._endframe) &gt; 0 for t in vo.tracks().values()])) else a)
        
        # Person track: enter/exit scene cannot be at the image boundary
        boundary = vo.framebox().dilate(0.9)
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and a.category() == &#39;person_enters_scene_through_structure&#39; and vo.track(a.actorid())[max(a.startframe(), vo.track(a.actorid()).startframe())].cover(boundary) &lt; 1) else a)
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and a.category() == &#39;person_exits_scene_through_structure&#39; and vo.track(a.actorid())[min(a.endframe(), vo.track(a.actorid()).endframe())].cover(boundary) &lt; 1) else a)
        
        # Activity union:  Temporal gaps less than support should be merged into one activity detection for a single track
        # Activity union:  &#34;Brief&#34; breaks (&lt;5 seconds) of confident activities should be merged into one activity detection for a single track
        briefmerge = set([&#39;person_reads_document&#39;, &#39;person_interacts_with_laptop&#39;, &#39;person_talks_to_person&#39;, &#39;person_purchases&#39;, &#39;person_steals_object&#39;, &#39;person_talks_on_phone&#39;, &#39;person_texts_on_phone&#39;, &#39;person_rides_bicycle&#39;, &#39;person_carries_heavy_object&#39;, &#39;person&#39;, &#39;person_walks&#39;, &#39;vehicle&#39;, &#39;car_moves&#39;])  
        merged = set([])
        mergeable_dets = [a for a in vo.activities().values() if a.id() in tofinalize and a.confidence() &gt; 0.2]  # only mergeable detections
        mergeable_dets.sort(key=lambda a: a.startframe())  # in-place
        for a in mergeable_dets:
            for o in mergeable_dets:
                if ((o._startframe &gt;= a._startframe) and (a._id != o._id) and (o._actorid == a._actorid) and (o._label == a._label) and (o._id not in merged) and (a._id not in merged) and
                    ((a.temporal_distance(o) &lt;= self.temporal_support() or (a.category() in briefmerge and a.temporal_distance(o) &lt; 5*vo.framerate())))):
                    a.union(o)  # in-place update
                    merged.add(o.id())
        vo.activityfilter(lambda a: a.id() not in tofinalize or a.id() not in merged)

        # Group activity: Must be accompanied by a friend with the same activity detection
        categories = [&#39;person_embraces_person&#39;, &#39;hand_interacts_with_person&#39;, &#39;person_talks_to_person&#39;, &#39;person_transfers_object&#39;]           
        dstbox = {k:vo.track(a.actorid()).boundingbox(a.startframe(), a.endframe()) for (k,a) in vo.activities().items() if a.id() in tofinalize and a.category() in categories}  # precompute
        srcbox = {k:bb.clone().maxsquare().dilate(1.2) for (k,bb) in dstbox.items()}                            
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                      a._label in categories and
                                                                      not any([(af._label == a._label and
                                                                                af._id != a._id and
                                                                                af._actorid != a._actorid and 
                                                                                srcbox[a._id].hasintersection(dstbox[af._id]))
                                                                               for af in vo.activities().values() if af.during_interval(a._startframe, a._endframe, inclusive=True)])) else a)
        
        # Activity group suppression:  Group activities may have at most one activity detection of this type per group in a spatial region surrounding the actor
        tosuppress = set([&#39;hand_interacts_with_person&#39;, &#39;person_embraces_person&#39;, &#39;person_transfers_object&#39;, &#39;person_steals_object&#39;, &#39;person_purchases&#39;, &#39;person_talks_to_person&#39;])
        suppressed = set([])
        activitybox = {a.id():vo.track(a.actorid()).boundingbox(a.startframe(), a.endframe()) for a in vo.activities().values() if a.id() in tofinalize and a.category() in tosuppress}
        activitybox = {k:bb.dilate(1.2).maxsquare() if bb is not None else bb for (k,bb) in activitybox.items()}
        candidates = [a for a in vo.activities().values() if a.id() in tofinalize]
        for a in sorted(candidates, key=lambda a: a.confidence(), reverse=True):  # decreasing confidence
            if a.category() in tosuppress:
                for o in candidates:  # other activities
                    if (o._actorid != a._actorid and  # different tracks
                        o._label == a._label and  # same category
                        o.confidence() &lt;= a.confidence() and   # lower confidence
                        o._id not in suppressed and  # not already suppressed
                        o.during_interval(a.startframe(), a.endframe()) and # overlaps temporally by at least one frame
                        (activitybox[a._id] is not None and activitybox[o._id] is not None) and   # has valid tracks
                        activitybox[a._id].hasintersection(activitybox[o._id]) and  # has coarse overlap 
                        vo.track(a.actorid()).clone().maxsquare().dilate(1.2).segment_maxiou(vo.track(o.actorid()), a.startframe(), a.endframe()) &gt; 0):  # has fine overlap &#34;close by&#34;
                        suppressed.add(o.id())  # greedy non-maximum suppression of lower confidence activity detection
        vo.activityfilter(lambda a: a.id() not in tofinalize or a.id() not in suppressed)

        # Activity duration
        vo.activitymap(lambda a: a.padto(5) if a.id() in tofinalize and a.category() in [&#39;person_talks_to_person&#39;, &#39;person_interacts_with_laptop&#39;, &#39;person_reads_document&#39;, &#39;person_purchases&#39;] else a)   
        vo.activitymap(lambda a: a.duration(2, centered=False) if a.id() in tofinalize and a.category() in [&#39;person_opens_vehicle_door&#39;, &#39;person_closes_vehicle_door&#39;] else a)
        vo.activitymap(lambda a: a.duration(2, centered=True) if a.id() in tofinalize and a.category() in [&#39;person_enters_scene_through_structure&#39;, &#39;person_exits_scene_through_structure&#39;] else a)
        vo.activitymap(lambda a: a.startframe(0) if a.id() in tofinalize and a.startframe() &lt; 0 else a)

        # Activity confidence
        if activityconf is not None:
            vo.activityfilter(lambda a: a.id() not in tofinalize or a.confidence() &gt;= activityconf)
    
        return vo
            
    def __call__(self, vi, activityiou=0.1, mirror=False, minprob=0.04, trackconf=0.2, maxdets=105, avgdets=70, throttle=True, buffered=True, finalized=True):
        (n,m,dt) = (self.temporal_support(), self.temporal_stride(), 1)  
        aa = self._allowable_activities  # dictionary mapping of allowable classified activities to output names        
        f_encode = self.totensor(training=False, validation=False, show=False, doflip=False)  # video -&gt; tensor CxNxHxW
        f_mirror = lambda t: (t, torch.from_numpy(np.copy(np.flip(np.asarray(t), axis=3))))  # CxNxHxW -&gt; CxNxHx(-W), np.flip is much faster than torch.flip, faster than encode mirror=True, np.flip returns a view which must be copied
        f_totensor = lambda v: (f_encode(v.clone(sharedarray=True) if mirror else v),) if (not mirror or v.actor().category() != &#39;person&#39;) else f_mirror(f_encode(v.clone(sharedarray=True)))  # do not mirror vehicle activities
        f_totensorlist = lambda V: [t for v in V for t in f_totensor(v)]        
        def f_reduce(T,V):
            j = sum([v.actor().category() == &#39;person&#39; for v in V])  # person mirrored, vehicle not mirrored
            (tm, t) = torch.split(T, (2*j, len(T)-2*j), dim=0)  # assumes sorted order, person first, only person/vehicle
            return torch.cat((torch.mean(tm.view(-1, 2, tm.shape[1]), dim=1), t), dim=0) if j&gt;0 else T  # mean over mirror augmentation
        
        try:
            with torch.no_grad():                                
                vp = next(vi)  # peek in generator to create clip
                vi = itertools.chain([vp], vi)  # unpeek
                sw = vipy.util.Stopwatch() if throttle else None  # real-time framerate estimate
                framerate = vp.framerate()
                for (k, (vo,vc)) in enumerate(zip(vi, vp.stream(buffered=buffered).clip(n, m, continuous=True, activities=False, delay=dt))):
                    videotracks = [] if vc is None else [vt for vt in vc.trackfilter(lambda t: len(t)&gt;=4 and (t.category() == &#39;person&#39; or (t.category() == &#39;vehicle&#39; and vo.track(t.id()).ismoving(k-10*n+dt, k+dt)))).tracksplit()]  # vehicle moved recently?
                    if throttle:
                        videotracks.sort(key=lambda v: v.actor().confidence(last=1))  # in-place                                            
                        numdets = (maxdets if ((avgdets is None) or (sw.duration()&lt;=60) or ((sw.duration()&gt;60) and ((k/sw.duration())/vp.framerate())&gt;0.8)) else
                                   (avgdets if ((k/sw.duration())/vp.framerate())&gt;0.67 else int(avgdets//2)))   # real-time throttle schedule
                        videotracks = videotracks[-numdets:] if (numdets is not None and len(videotracks)&gt;numdets) else videotracks   # select only the most confident for detection                
                    videotracks.sort(key=lambda v: v.actor().category())  # in-place, for grouping mirrored encoding: person&lt;vehicle
                
                    if len(videotracks)&gt;0 and (k+dt &gt; n):
                        logits = self.forward(torch.stack(f_totensorlist(videotracks))) # augmented logits in track index order, copy
                        logits = f_reduce(logits, videotracks) if mirror else logits  # reduced logits in track index order
                        (actorid, actorcategory) = ([t.actorid() for t in videotracks], [t.actor().category() for t in videotracks])
                        dets = [vipy.activity.Activity(category=aa[category], shortlabel=self._class_to_shortlabel[category], startframe=k-n+dt, endframe=k+dt, confidence=sm, framerate=framerate, actorid=actorid[j], attributes={&#39;pip&#39;:category, &#39;logit&#39;:float(conf)}) 
                                for (j, category_conf_lr_prob_sm) in enumerate(self.lrt(logits))  # likelihood ratio test
                                for (category, conf, lr, prob, sm) in category_conf_lr_prob_sm   
                                if ((category in aa) and   # requested activities only
                                    (actorcategory[j] in self._verb_to_noun[category]) and   # noun matching with category renaming dictionary
                                    sm&gt;=minprob)]   # minimum probability for new activity detection
                        vo.assign(k+dt, dets, activityiou=activityiou, activitymerge=False, activitynms=True)   # assign new activity detections by non-maximum suppression (merge happens at the end)
                        del logits, dets, videotracks  # torch garabage collection

                    if not isinstance(finalized, bool) and k &gt; 0 and k%finalized == 0:
                        self.finalize(vo, trackconf=trackconf, startframe=k-finalized-5, endframe=k-5)  
                        
                    yield vo

        except Exception as e:                
            raise

        finally:
            if not (finalized is False):
                self.finalize(vo, trackconf=trackconf) if finalized == True else self.finalize(vo, trackconf=trackconf, startframe=(k//finalized)*finalized-4, endframe=k)


class ActivityTrackerCap(ActivityTracker, CAP):
    pass

    </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="heyvi.recognition.ActivityRecognition"><code class="flex name class">
<span>class <span class="ident">ActivityRecognition</span></span>
<span>(</span><span>pretrained=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L30-L84" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ActivityRecognition(object):
    def __init__(self, pretrained=True):
        self.net =  None
        self._class_to_index = {}
        self._num_frames = 0

    def class_to_index(self, c=None):
        return self._class_to_index if c is None else self._class_to_index[c]
    
    def index_to_class(self, index=None):
        d = {v:k for (k,v) in self.class_to_index().items()}
        return d if index is None else d[index]
    
    def classlist(self):
        return [k for (k,v) in sorted(list(self.class_to_index().items()), key=lambda x: x[0])]  # sorted in index order

    def num_classes(self):
        return len(self.classlist())

    def fromindex(self, k):
        index_to_class = {v:k for (k,v) in self.class_to_index().items()}
        assert k in index_to_class, &#34;Invalid class index &#39;%s&#39;&#34; % (str(k))
        return index_to_class[k]

    def label_confidence(self, video=None, tensor=None, threshold=None):
        raise
        logits = self.__call__(video, tensor)
        conf = [[(self.index_to_class(j), s[j]) for j in i[::-1] if threshold is None or s[j]&gt;threshold] for (s,i) in zip(logits, np.argsort(logits, axis=1))]
        return conf if len(logits) &gt; 1 else conf[0]

    def activity(self, video, threshold=None):
        (c,s) = zip(*self.label_confidence(video=video, threshold=None))
        return vipy.activity.Activity(startframe=0, endframe=self._num_frames, category=c[0], actorid=video.actorid(), confidence=s[0]) if (threshold is None or s[0]&gt;threshold) else None
            
    def top1(self, video=None, tensor=None, threshold=None):
        raise
        return self.topk(k=1, video=video, tensor=tensor, threshold=threshold)

    def topk(self, k, video=None, tensor=None, threshold=None):
        raise
        logits = self.__call__(video, tensor)
        topk = [[self.index_to_class(j) for j in i[-k:][::-1] if threshold is None or s[j] &gt;= threshold] for (s,i) in zip(logits, np.argsort(logits, axis=1))]
        return topk if len(topk) &gt; 1 else topk[0]

    def temporal_support(self):
        return self._num_frames

    def totensor(self, training=False):
        raise

    def binary_vector(self, categories):
        y = np.zeros(len(self.classlist())).astype(np.float32)
        for c in tolist(categories):
            y[self.class_to_index(c)] = 1
        return torch.from_numpy(y).type(torch.FloatTensor)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="heyvi.recognition.CAP" href="#heyvi.recognition.CAP">CAP</a></li>
<li><a title="heyvi.recognition.PIP_250k" href="#heyvi.recognition.PIP_250k">PIP_250k</a></li>
<li><a title="heyvi.recognition.PIP_370k" href="#heyvi.recognition.PIP_370k">PIP_370k</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="heyvi.recognition.ActivityRecognition.activity"><code class="name flex">
<span>def <span class="ident">activity</span></span>(<span>self, video, threshold=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L60-L62" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def activity(self, video, threshold=None):
    (c,s) = zip(*self.label_confidence(video=video, threshold=None))
    return vipy.activity.Activity(startframe=0, endframe=self._num_frames, category=c[0], actorid=video.actorid(), confidence=s[0]) if (threshold is None or s[0]&gt;threshold) else None</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityRecognition.binary_vector"><code class="name flex">
<span>def <span class="ident">binary_vector</span></span>(<span>self, categories)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L80-L84" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def binary_vector(self, categories):
    y = np.zeros(len(self.classlist())).astype(np.float32)
    for c in tolist(categories):
        y[self.class_to_index(c)] = 1
    return torch.from_numpy(y).type(torch.FloatTensor)</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityRecognition.class_to_index"><code class="name flex">
<span>def <span class="ident">class_to_index</span></span>(<span>self, c=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L36-L37" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def class_to_index(self, c=None):
    return self._class_to_index if c is None else self._class_to_index[c]</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityRecognition.classlist"><code class="name flex">
<span>def <span class="ident">classlist</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L43-L44" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def classlist(self):
    return [k for (k,v) in sorted(list(self.class_to_index().items()), key=lambda x: x[0])]  # sorted in index order</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityRecognition.fromindex"><code class="name flex">
<span>def <span class="ident">fromindex</span></span>(<span>self, k)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L49-L52" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def fromindex(self, k):
    index_to_class = {v:k for (k,v) in self.class_to_index().items()}
    assert k in index_to_class, &#34;Invalid class index &#39;%s&#39;&#34; % (str(k))
    return index_to_class[k]</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityRecognition.index_to_class"><code class="name flex">
<span>def <span class="ident">index_to_class</span></span>(<span>self, index=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L39-L41" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def index_to_class(self, index=None):
    d = {v:k for (k,v) in self.class_to_index().items()}
    return d if index is None else d[index]</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityRecognition.label_confidence"><code class="name flex">
<span>def <span class="ident">label_confidence</span></span>(<span>self, video=None, tensor=None, threshold=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L54-L58" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def label_confidence(self, video=None, tensor=None, threshold=None):
    raise
    logits = self.__call__(video, tensor)
    conf = [[(self.index_to_class(j), s[j]) for j in i[::-1] if threshold is None or s[j]&gt;threshold] for (s,i) in zip(logits, np.argsort(logits, axis=1))]
    return conf if len(logits) &gt; 1 else conf[0]</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityRecognition.num_classes"><code class="name flex">
<span>def <span class="ident">num_classes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L46-L47" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def num_classes(self):
    return len(self.classlist())</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityRecognition.temporal_support"><code class="name flex">
<span>def <span class="ident">temporal_support</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L74-L75" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def temporal_support(self):
    return self._num_frames</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityRecognition.top1"><code class="name flex">
<span>def <span class="ident">top1</span></span>(<span>self, video=None, tensor=None, threshold=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L64-L66" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def top1(self, video=None, tensor=None, threshold=None):
    raise
    return self.topk(k=1, video=video, tensor=tensor, threshold=threshold)</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityRecognition.topk"><code class="name flex">
<span>def <span class="ident">topk</span></span>(<span>self, k, video=None, tensor=None, threshold=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L68-L72" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def topk(self, k, video=None, tensor=None, threshold=None):
    raise
    logits = self.__call__(video, tensor)
    topk = [[self.index_to_class(j) for j in i[-k:][::-1] if threshold is None or s[j] &gt;= threshold] for (s,i) in zip(logits, np.argsort(logits, axis=1))]
    return topk if len(topk) &gt; 1 else topk[0]</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityRecognition.totensor"><code class="name flex">
<span>def <span class="ident">totensor</span></span>(<span>self, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L77-L78" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def totensor(self, training=False):
    raise</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="heyvi.recognition.ActivityTracker"><code class="flex name class">
<span>class <span class="ident">ActivityTracker</span></span>
<span>(</span><span>stride=3, activities=None, gpus=None, batchsize=None, mlbl=False, mlfl=False, modelfile=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Video Activity detection.</p>
<p>Args (<strong>call</strong>):
vi [generator of <code>vipy.video.Scene</code>]:
The input video to be updated in place with detections.
This is a generator which is output from heyvi.detection.MultiscaleVideoTracker.<strong>call</strong>
activityiou [float]: The minimum temporal iou for activity assignment
mirror [bool]:
If true, encode using the mean of a video encoding and the mirrored video encoding.
This is slower as it requires 2x GPU forward passes
minprob [float]: The minimum probability for new activity detection
trackconf [float]: The minimum object detection confidence for new tracks
maxdets [int]:
The maximum number of allowable detections per frame.
If there are more detections per frame tha maxdets, sort them by confidence and use only the top maxdets best
avgdets [int]:
The number of allowable detections per frame if throttled
buffered [bool]:
If true, then buffer streams.
This is useful for activity detection on live streams.
<br>
finalized [bool, int]:
If False then do not finalize(), If True finalize() only at the end, If int, then finalize every int frames.
This is useful for streaming activity detection on unbounded inputs. </p>
<h2 id="returns">Returns</h2>
<p>The input video is updated in place.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L446-L699" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ActivityTracker(PIP_370k):
    &#34;&#34;&#34;Video Activity detection.
        
    Args (__call__):
        vi [generator of `vipy.video.Scene`]:  The input video to be updated in place with detections.  This is a generator which is output from heyvi.detection.MultiscaleVideoTracker.__call__
        activityiou [float]: The minimum temporal iou for activity assignment
        mirror [bool]:  If true, encode using the mean of a video encoding and the mirrored video encoding.  This is slower as it requires 2x GPU forward passes
        minprob [float]: The minimum probability for new activity detection
        trackconf [float]: The minimum object detection confidence for new tracks
        maxdets [int]:  The maximum number of allowable detections per frame.  If there are more detections per frame tha maxdets, sort them by confidence and use only the top maxdets best
        avgdets [int]:  The number of allowable detections per frame if throttled
        buffered [bool]:  If true, then buffer streams.  This is useful for activity detection on live streams.            
        finalized [bool, int]:  If False then do not finalize(), If True finalize() only at the end, If int, then finalize every int frames.  This is useful for streaming activity detection on unbounded inputs. 
        
    Returns:
        The input video is updated in place.
    
    &#34;&#34;&#34;    
    def __init__(self, stride=3, activities=None, gpus=None, batchsize=None, mlbl=False, mlfl=False, modelfile=None):
        assert modelfile is not None, &#34;Contact &lt;info@visym.com&gt; for access to non-public model files&#34;

        super().__init__(pretrained=False, modelfile=modelfile, mlbl=mlbl, mlfl=mlfl)
        self._stride = stride
        self._allowable_activities = {k:v for (k,v) in [(a,a) if not isinstance(a, tuple) else a for a in activities]} if activities is not None else {k:k for k in self.classlist()}
        self._batchsize_per_gpu = batchsize
        self._gpus = gpus

        if gpus is not None:
            assert torch.cuda.is_available()
            assert batchsize is not None
            self._devices = [&#39;cuda:%d&#39; % k for k in gpus]
            self._gpus = [copy.deepcopy(self.net).to(d, non_blocking=False) for d in self._devices]  
            for m in self._gpus:
                m.eval()
        torch.set_grad_enabled(False)

    def temporal_stride(self, s=None):
        if s is not None:
            self._stride = s
            return self
        else:
            return self._stride

    def forward(self, x):
        &#34;&#34;&#34;Overload forward for multi-gpu batch.  Don&#39;t use torch DataParallel!&#34;&#34;&#34;
        if self._gpus is None:
            return super().forward(x)  # cpu
        else:
            x_forward = None            
            for b in x.pin_memory().split(self._batchsize_per_gpu*len(self._gpus)):  # pinned copy
                n_todevice = np.sum(np.array([1 if k&lt;len(b) else 0 for k in range(int(len(self._devices)*np.ceil(len(b)/len(self._devices))))]).reshape(-1, len(self._devices)), axis=0).tolist()
                todevice = [t.to(d, non_blocking=True) for (t,d) in zip(b.split(n_todevice), self._devices) if len(t)&gt;0]   # async device copy
                ondevice = [m(t) for (m,t) in zip(self._gpus, todevice)]   # async
                fromdevice = torch.cat([t.cpu() for t in ondevice], dim=0)
                x_forward = fromdevice if x_forward is None else torch.cat((x_forward, fromdevice), dim=0)
                del ondevice, todevice, fromdevice, b  # force garbage collection of GPU memory
            del x  # force garbage collection
            return x_forward

    def lrt(self, x_logits, lrt_threshold=None):
        &#34;&#34;&#34;top-k with likelihood ratio test with background null hypothesis&#34;&#34;&#34;
        j_bg_person = self._class_to_index[&#39;person&#39;] if &#39;person&#39; in self._class_to_index else self._class_to_index[&#39;person_walks&#39;]  # FIXME
        j_bg_vehicle = self._class_to_index[&#39;vehicle&#39;] if &#39;vehicle&#39; in self._class_to_index else self._class_to_index[&#39;car_moves&#39;]  # FIXME

        yh = x_logits.detach().cpu().numpy()
        yh_softmax = F.softmax(x_logits.detach().cpu(), dim=1)
        p_null = np.maximum(yh[:, j_bg_person], yh[:, j_bg_vehicle]).reshape(yh.shape[0], 1)
        lr = yh - p_null   # ~= log likelihood ratio
        f_logistic = lambda x,b,s=1.0: float(1.0 / (1.0 + np.exp(-s*(x + b))))
        return [sorted([(self.index_to_class(j), float(s[j]), float(t[j]), f_logistic(s[j], 1.0)*f_logistic(t[j], 0.0), float(sm[j])) for j in range(len(s)) if (lrt_threshold is None or t[j] &gt;= lrt_threshold)], key=lambda x: x[3], reverse=True) for (s,t,sm) in zip(yh, lr, yh_softmax)]


    def finalize(self, vo, trackconf=None, activityconf=None, startframe=None, endframe=None):
        &#34;&#34;&#34;In place filtering of video to finalize&#34;&#34;&#34;
        assert isinstance(vo, vipy.video.Scene)

        tofinalize = set([ai for (ai,a) in vo.activities().items() if (endframe is None or a.endframe() &lt;= endframe) and (startframe is None or a.endframe() &gt;= startframe)])
        tofinalize = tofinalize.union([ti for (ti,t) in vo.tracks().items() if ((endframe is None or t.endframe() &lt;= endframe) and (startframe is None or t.endframe() &gt;= startframe)) or any([ti == vo.activities(id=ai).actorid() for ai in tofinalize])])

        # Bad tracks:  Remove low confidence or too short non-moving tracks, and associated activities
        # - will throw exception that &#39;vo referenced before assignment&#39; if one loop did not succceed
        if trackconf is not None:
            vo.trackfilter(lambda t: t.id() not in tofinalize or len(t)&gt;=vo.framerate() and (t.confidence() &gt;= trackconf or t.startbox().iou(t.endbox()) == 0)).activityfilter(lambda a: a.id() not in tofinalize or a.actorid() in vo.tracks())  
        
        # Activity probability:  noun_probability*verb probability
        nounconf = {k:t.confidence(samples=8) for (k,t) in vo.tracks().items() if t.id() in tofinalize}   # 
        vo.activitymap(lambda a: a.confidence(nounconf[a.actorid()]*a.confidence()) if a.id() in tofinalize else a)
        
        # Missing objects:  Significantly reduce confidence of complex classes (yuck)
        vo.activitymap(lambda a: a.confidence(0.01*a.confidence()) if (a.id() in tofinalize and a.category() in [&#39;person_purchases&#39;]) else a) 
        
        # Vehicle turns:  High confidence vehicle turns must be a minimum angle
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                      (a.category() in [&#39;vehicle_turns_left&#39;, &#39;vehicle_turns_right&#39;]) and
                                                                      (abs(vo.track(a.actorid()).bearing_change(a.startframe(), a.endframe(), dt=vo.framerate(), samples=5)) &lt; (np.pi/16))) else a) 
        
        # Vehicle turns:  U-turn can only be distinguished from left/right turn at the end of a track by looking at the turn angle
        vo.activitymap(lambda a: a.category(&#39;vehicle_makes_u_turn&#39;).shortlabel(&#39;u turn&#39;) if (a.id() in tofinalize and
                                                                                             (a.category() in [&#39;vehicle_turns_left&#39;, &#39;vehicle_turns_right&#39;]) and
                                                                                             (abs(vo.track(a.actorid()).bearing_change(a.startframe(), a.endframe(), dt=vo.framerate(), samples=5)) &gt; (np.pi-(np.pi/2)))) else a)
        
        # Background activities:  Use logistic confidence on logit due to lack of background class &#34;person stands&#34;, otherwise every standing person is using a phone
        f_logistic = lambda x,b,s=1.0: float(1.0 / (1.0 + np.exp(-s*(x + b))))
        vo.activitymap(lambda a: a.confidence(a.confidence()*f_logistic(a.attributes[&#39;logit&#39;], -1.5)) if a.id() in tofinalize else a)

        # Complex activities: remove steal/abandon and replace with picks up / puts down
        vo.activityfilter(lambda a: a.category() not in [&#39;person_steals_object&#39;, &#39;person_abandons_package&#39;])
        newlist = [vo.add(vipy.activity.Activity(startframe=a.startframe(), endframe=a.endframe(), category=&#39;person_steals_object&#39;, shortlabel=&#39;steals&#39;, confidence=0.5*a.confidence(), framerate=vo.framerate(), actorid=a.actorid(), attributes={&#39;pip&#39;:&#39;person_picks_up_object&#39;}))
                   for a in vo.activitylist() if a.category() == &#39;person_picks_up_object&#39;]
        newlist = [vo.add(vipy.activity.Activity(startframe=a.startframe(), endframe=a.endframe(), category=&#39;person_abandons_package&#39;, shortlabel=&#39;abandons&#39;, confidence=0.5*a.confidence(), framerate=vo.framerate(), actorid=a.actorid(), attributes={&#39;pip&#39;:&#39;person_puts_down_object&#39;}))
                   for a in vo.activitylist() if a.category() == &#39;person_puts_down_object&#39;]
            
        # Vehicle/person interaction: &#39;vehicle_drops_off_person&#39;/&#39;vehicle_picks_up_person&#39;  must be followed by car driving away/pulling up, must be accompanied by person track start/end
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                      a.category() == &#39;vehicle_drops_off_person&#39; and
                                                                      (not vo.track(a.actorid()).ismoving(a.middleframe(), a.endframe()+10*vo.framerate()) or
                                                                       not any([t.category() == &#39;person&#39; and t.segment_maxiou(vo.track(a._actorid), t.startframe(), t.startframe()+1) &gt; 0 for t in vo.tracks().values()]))) else a)
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                      a.category() == &#39;vehicle_picks_up_person&#39; and
                                                                      (not vo.track(a.actorid()).ismoving(a.startframe()-10*vo.framerate(), a.middleframe()) or
                                                                       not any([t.category() == &#39;person&#39; and t.segment_maxiou(vo.track(a._actorid), t.endframe()-1, t.endframe()) &gt; 0 for t in vo.tracks().values()]))) else a)
        
        # Person/Bicycle track: riding must be accompanied by an associated moving bicycle track
        vo.activityfilter(lambda a: a.id() not in tofinalize or a.category() != &#39;person_rides_bicycle&#39;)
        bikelist = [vo.add(vipy.activity.Activity(startframe=t.startframe(), endframe=t.endframe(), category=&#39;person_rides_bicycle&#39;, shortlabel=&#39;rides&#39;, confidence=t.confidence(samples=8), framerate=vo.framerate(), actorid=t.id(), attributes={&#39;pip&#39;:&#39;person_rides_bicycle&#39;}))
                    for (tk,t) in vo.tracks().items() if (t.id() in tofinalize and t.category() == &#39;bicycle&#39; and t.ismoving())]
        
        # Person/Vehicle track: person/vehicle interaction must be accompanied by an associated stopped vehicle track
        dstbox = {k:vo.track(a.actorid()).boundingbox(a.startframe(), a.endframe()) for (k,a) in vo.activities().items() if (a.id() in tofinalize and a.category().startswith(&#39;person&#39;) and (&#39;vehicle&#39; in a.category() or &#39;trunk&#39; in a.category()))}  # precompute            
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                      (a.category().startswith(&#39;person&#39;) and (&#39;vehicle&#39; in a.category() or &#39;trunk&#39; in a.category())) and
                                                                      not any([t.category() == &#39;vehicle&#39; and 
                                                                               t.during(a.startframe()) and
                                                                               not t.ismoving(a.startframe(), a.endframe()) and
                                                                               t[a.startframe()].hasintersection(dstbox[a._id])
                                                                               for t in vo.tracks().values()])) else a)
        
        # Vehicle/Person track: vehicle/person interaction must be accompanied by an associated person track
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and (a.category().startswith(&#39;vehicle&#39;) and (&#39;person&#39; in a.category())) and not any([t.category() == &#39;person&#39; and t.segment_maxiou(vo.track(a._actorid), a._startframe, a._endframe) &gt; 0 for t in vo.tracks().values()])) else a)
        
        # Person track: enter/exit scene cannot be at the image boundary
        boundary = vo.framebox().dilate(0.9)
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and a.category() == &#39;person_enters_scene_through_structure&#39; and vo.track(a.actorid())[max(a.startframe(), vo.track(a.actorid()).startframe())].cover(boundary) &lt; 1) else a)
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and a.category() == &#39;person_exits_scene_through_structure&#39; and vo.track(a.actorid())[min(a.endframe(), vo.track(a.actorid()).endframe())].cover(boundary) &lt; 1) else a)
        
        # Activity union:  Temporal gaps less than support should be merged into one activity detection for a single track
        # Activity union:  &#34;Brief&#34; breaks (&lt;5 seconds) of confident activities should be merged into one activity detection for a single track
        briefmerge = set([&#39;person_reads_document&#39;, &#39;person_interacts_with_laptop&#39;, &#39;person_talks_to_person&#39;, &#39;person_purchases&#39;, &#39;person_steals_object&#39;, &#39;person_talks_on_phone&#39;, &#39;person_texts_on_phone&#39;, &#39;person_rides_bicycle&#39;, &#39;person_carries_heavy_object&#39;, &#39;person&#39;, &#39;person_walks&#39;, &#39;vehicle&#39;, &#39;car_moves&#39;])  
        merged = set([])
        mergeable_dets = [a for a in vo.activities().values() if a.id() in tofinalize and a.confidence() &gt; 0.2]  # only mergeable detections
        mergeable_dets.sort(key=lambda a: a.startframe())  # in-place
        for a in mergeable_dets:
            for o in mergeable_dets:
                if ((o._startframe &gt;= a._startframe) and (a._id != o._id) and (o._actorid == a._actorid) and (o._label == a._label) and (o._id not in merged) and (a._id not in merged) and
                    ((a.temporal_distance(o) &lt;= self.temporal_support() or (a.category() in briefmerge and a.temporal_distance(o) &lt; 5*vo.framerate())))):
                    a.union(o)  # in-place update
                    merged.add(o.id())
        vo.activityfilter(lambda a: a.id() not in tofinalize or a.id() not in merged)

        # Group activity: Must be accompanied by a friend with the same activity detection
        categories = [&#39;person_embraces_person&#39;, &#39;hand_interacts_with_person&#39;, &#39;person_talks_to_person&#39;, &#39;person_transfers_object&#39;]           
        dstbox = {k:vo.track(a.actorid()).boundingbox(a.startframe(), a.endframe()) for (k,a) in vo.activities().items() if a.id() in tofinalize and a.category() in categories}  # precompute
        srcbox = {k:bb.clone().maxsquare().dilate(1.2) for (k,bb) in dstbox.items()}                            
        vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                      a._label in categories and
                                                                      not any([(af._label == a._label and
                                                                                af._id != a._id and
                                                                                af._actorid != a._actorid and 
                                                                                srcbox[a._id].hasintersection(dstbox[af._id]))
                                                                               for af in vo.activities().values() if af.during_interval(a._startframe, a._endframe, inclusive=True)])) else a)
        
        # Activity group suppression:  Group activities may have at most one activity detection of this type per group in a spatial region surrounding the actor
        tosuppress = set([&#39;hand_interacts_with_person&#39;, &#39;person_embraces_person&#39;, &#39;person_transfers_object&#39;, &#39;person_steals_object&#39;, &#39;person_purchases&#39;, &#39;person_talks_to_person&#39;])
        suppressed = set([])
        activitybox = {a.id():vo.track(a.actorid()).boundingbox(a.startframe(), a.endframe()) for a in vo.activities().values() if a.id() in tofinalize and a.category() in tosuppress}
        activitybox = {k:bb.dilate(1.2).maxsquare() if bb is not None else bb for (k,bb) in activitybox.items()}
        candidates = [a for a in vo.activities().values() if a.id() in tofinalize]
        for a in sorted(candidates, key=lambda a: a.confidence(), reverse=True):  # decreasing confidence
            if a.category() in tosuppress:
                for o in candidates:  # other activities
                    if (o._actorid != a._actorid and  # different tracks
                        o._label == a._label and  # same category
                        o.confidence() &lt;= a.confidence() and   # lower confidence
                        o._id not in suppressed and  # not already suppressed
                        o.during_interval(a.startframe(), a.endframe()) and # overlaps temporally by at least one frame
                        (activitybox[a._id] is not None and activitybox[o._id] is not None) and   # has valid tracks
                        activitybox[a._id].hasintersection(activitybox[o._id]) and  # has coarse overlap 
                        vo.track(a.actorid()).clone().maxsquare().dilate(1.2).segment_maxiou(vo.track(o.actorid()), a.startframe(), a.endframe()) &gt; 0):  # has fine overlap &#34;close by&#34;
                        suppressed.add(o.id())  # greedy non-maximum suppression of lower confidence activity detection
        vo.activityfilter(lambda a: a.id() not in tofinalize or a.id() not in suppressed)

        # Activity duration
        vo.activitymap(lambda a: a.padto(5) if a.id() in tofinalize and a.category() in [&#39;person_talks_to_person&#39;, &#39;person_interacts_with_laptop&#39;, &#39;person_reads_document&#39;, &#39;person_purchases&#39;] else a)   
        vo.activitymap(lambda a: a.duration(2, centered=False) if a.id() in tofinalize and a.category() in [&#39;person_opens_vehicle_door&#39;, &#39;person_closes_vehicle_door&#39;] else a)
        vo.activitymap(lambda a: a.duration(2, centered=True) if a.id() in tofinalize and a.category() in [&#39;person_enters_scene_through_structure&#39;, &#39;person_exits_scene_through_structure&#39;] else a)
        vo.activitymap(lambda a: a.startframe(0) if a.id() in tofinalize and a.startframe() &lt; 0 else a)

        # Activity confidence
        if activityconf is not None:
            vo.activityfilter(lambda a: a.id() not in tofinalize or a.confidence() &gt;= activityconf)
    
        return vo
            
    def __call__(self, vi, activityiou=0.1, mirror=False, minprob=0.04, trackconf=0.2, maxdets=105, avgdets=70, throttle=True, buffered=True, finalized=True):
        (n,m,dt) = (self.temporal_support(), self.temporal_stride(), 1)  
        aa = self._allowable_activities  # dictionary mapping of allowable classified activities to output names        
        f_encode = self.totensor(training=False, validation=False, show=False, doflip=False)  # video -&gt; tensor CxNxHxW
        f_mirror = lambda t: (t, torch.from_numpy(np.copy(np.flip(np.asarray(t), axis=3))))  # CxNxHxW -&gt; CxNxHx(-W), np.flip is much faster than torch.flip, faster than encode mirror=True, np.flip returns a view which must be copied
        f_totensor = lambda v: (f_encode(v.clone(sharedarray=True) if mirror else v),) if (not mirror or v.actor().category() != &#39;person&#39;) else f_mirror(f_encode(v.clone(sharedarray=True)))  # do not mirror vehicle activities
        f_totensorlist = lambda V: [t for v in V for t in f_totensor(v)]        
        def f_reduce(T,V):
            j = sum([v.actor().category() == &#39;person&#39; for v in V])  # person mirrored, vehicle not mirrored
            (tm, t) = torch.split(T, (2*j, len(T)-2*j), dim=0)  # assumes sorted order, person first, only person/vehicle
            return torch.cat((torch.mean(tm.view(-1, 2, tm.shape[1]), dim=1), t), dim=0) if j&gt;0 else T  # mean over mirror augmentation
        
        try:
            with torch.no_grad():                                
                vp = next(vi)  # peek in generator to create clip
                vi = itertools.chain([vp], vi)  # unpeek
                sw = vipy.util.Stopwatch() if throttle else None  # real-time framerate estimate
                framerate = vp.framerate()
                for (k, (vo,vc)) in enumerate(zip(vi, vp.stream(buffered=buffered).clip(n, m, continuous=True, activities=False, delay=dt))):
                    videotracks = [] if vc is None else [vt for vt in vc.trackfilter(lambda t: len(t)&gt;=4 and (t.category() == &#39;person&#39; or (t.category() == &#39;vehicle&#39; and vo.track(t.id()).ismoving(k-10*n+dt, k+dt)))).tracksplit()]  # vehicle moved recently?
                    if throttle:
                        videotracks.sort(key=lambda v: v.actor().confidence(last=1))  # in-place                                            
                        numdets = (maxdets if ((avgdets is None) or (sw.duration()&lt;=60) or ((sw.duration()&gt;60) and ((k/sw.duration())/vp.framerate())&gt;0.8)) else
                                   (avgdets if ((k/sw.duration())/vp.framerate())&gt;0.67 else int(avgdets//2)))   # real-time throttle schedule
                        videotracks = videotracks[-numdets:] if (numdets is not None and len(videotracks)&gt;numdets) else videotracks   # select only the most confident for detection                
                    videotracks.sort(key=lambda v: v.actor().category())  # in-place, for grouping mirrored encoding: person&lt;vehicle
                
                    if len(videotracks)&gt;0 and (k+dt &gt; n):
                        logits = self.forward(torch.stack(f_totensorlist(videotracks))) # augmented logits in track index order, copy
                        logits = f_reduce(logits, videotracks) if mirror else logits  # reduced logits in track index order
                        (actorid, actorcategory) = ([t.actorid() for t in videotracks], [t.actor().category() for t in videotracks])
                        dets = [vipy.activity.Activity(category=aa[category], shortlabel=self._class_to_shortlabel[category], startframe=k-n+dt, endframe=k+dt, confidence=sm, framerate=framerate, actorid=actorid[j], attributes={&#39;pip&#39;:category, &#39;logit&#39;:float(conf)}) 
                                for (j, category_conf_lr_prob_sm) in enumerate(self.lrt(logits))  # likelihood ratio test
                                for (category, conf, lr, prob, sm) in category_conf_lr_prob_sm   
                                if ((category in aa) and   # requested activities only
                                    (actorcategory[j] in self._verb_to_noun[category]) and   # noun matching with category renaming dictionary
                                    sm&gt;=minprob)]   # minimum probability for new activity detection
                        vo.assign(k+dt, dets, activityiou=activityiou, activitymerge=False, activitynms=True)   # assign new activity detections by non-maximum suppression (merge happens at the end)
                        del logits, dets, videotracks  # torch garabage collection

                    if not isinstance(finalized, bool) and k &gt; 0 and k%finalized == 0:
                        self.finalize(vo, trackconf=trackconf, startframe=k-finalized-5, endframe=k-5)  
                        
                    yield vo

        except Exception as e:                
            raise

        finally:
            if not (finalized is False):
                self.finalize(vo, trackconf=trackconf) if finalized == True else self.finalize(vo, trackconf=trackconf, startframe=(k//finalized)*finalized-4, endframe=k)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="heyvi.recognition.PIP_370k" href="#heyvi.recognition.PIP_370k">PIP_370k</a></li>
<li><a title="heyvi.recognition.PIP_250k" href="#heyvi.recognition.PIP_250k">PIP_250k</a></li>
<li>pytorch_lightning.core.lightning.LightningModule</li>
<li>abc.ABC</li>
<li>pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.grads.GradInformation</li>
<li>pytorch_lightning.core.saving.ModelIO</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
<li><a title="heyvi.recognition.ActivityRecognition" href="#heyvi.recognition.ActivityRecognition">ActivityRecognition</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="heyvi.recognition.ActivityTrackerCap" href="#heyvi.recognition.ActivityTrackerCap">ActivityTrackerCap</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="heyvi.recognition.ActivityTracker.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="heyvi.recognition.ActivityTracker.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="heyvi.recognition.ActivityTracker.finalize"><code class="name flex">
<span>def <span class="ident">finalize</span></span>(<span>self, vo, trackconf=None, activityconf=None, startframe=None, endframe=None)</span>
</code></dt>
<dd>
<div class="desc"><p>In place filtering of video to finalize</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L518-L647" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def finalize(self, vo, trackconf=None, activityconf=None, startframe=None, endframe=None):
    &#34;&#34;&#34;In place filtering of video to finalize&#34;&#34;&#34;
    assert isinstance(vo, vipy.video.Scene)

    tofinalize = set([ai for (ai,a) in vo.activities().items() if (endframe is None or a.endframe() &lt;= endframe) and (startframe is None or a.endframe() &gt;= startframe)])
    tofinalize = tofinalize.union([ti for (ti,t) in vo.tracks().items() if ((endframe is None or t.endframe() &lt;= endframe) and (startframe is None or t.endframe() &gt;= startframe)) or any([ti == vo.activities(id=ai).actorid() for ai in tofinalize])])

    # Bad tracks:  Remove low confidence or too short non-moving tracks, and associated activities
    # - will throw exception that &#39;vo referenced before assignment&#39; if one loop did not succceed
    if trackconf is not None:
        vo.trackfilter(lambda t: t.id() not in tofinalize or len(t)&gt;=vo.framerate() and (t.confidence() &gt;= trackconf or t.startbox().iou(t.endbox()) == 0)).activityfilter(lambda a: a.id() not in tofinalize or a.actorid() in vo.tracks())  
    
    # Activity probability:  noun_probability*verb probability
    nounconf = {k:t.confidence(samples=8) for (k,t) in vo.tracks().items() if t.id() in tofinalize}   # 
    vo.activitymap(lambda a: a.confidence(nounconf[a.actorid()]*a.confidence()) if a.id() in tofinalize else a)
    
    # Missing objects:  Significantly reduce confidence of complex classes (yuck)
    vo.activitymap(lambda a: a.confidence(0.01*a.confidence()) if (a.id() in tofinalize and a.category() in [&#39;person_purchases&#39;]) else a) 
    
    # Vehicle turns:  High confidence vehicle turns must be a minimum angle
    vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                  (a.category() in [&#39;vehicle_turns_left&#39;, &#39;vehicle_turns_right&#39;]) and
                                                                  (abs(vo.track(a.actorid()).bearing_change(a.startframe(), a.endframe(), dt=vo.framerate(), samples=5)) &lt; (np.pi/16))) else a) 
    
    # Vehicle turns:  U-turn can only be distinguished from left/right turn at the end of a track by looking at the turn angle
    vo.activitymap(lambda a: a.category(&#39;vehicle_makes_u_turn&#39;).shortlabel(&#39;u turn&#39;) if (a.id() in tofinalize and
                                                                                         (a.category() in [&#39;vehicle_turns_left&#39;, &#39;vehicle_turns_right&#39;]) and
                                                                                         (abs(vo.track(a.actorid()).bearing_change(a.startframe(), a.endframe(), dt=vo.framerate(), samples=5)) &gt; (np.pi-(np.pi/2)))) else a)
    
    # Background activities:  Use logistic confidence on logit due to lack of background class &#34;person stands&#34;, otherwise every standing person is using a phone
    f_logistic = lambda x,b,s=1.0: float(1.0 / (1.0 + np.exp(-s*(x + b))))
    vo.activitymap(lambda a: a.confidence(a.confidence()*f_logistic(a.attributes[&#39;logit&#39;], -1.5)) if a.id() in tofinalize else a)

    # Complex activities: remove steal/abandon and replace with picks up / puts down
    vo.activityfilter(lambda a: a.category() not in [&#39;person_steals_object&#39;, &#39;person_abandons_package&#39;])
    newlist = [vo.add(vipy.activity.Activity(startframe=a.startframe(), endframe=a.endframe(), category=&#39;person_steals_object&#39;, shortlabel=&#39;steals&#39;, confidence=0.5*a.confidence(), framerate=vo.framerate(), actorid=a.actorid(), attributes={&#39;pip&#39;:&#39;person_picks_up_object&#39;}))
               for a in vo.activitylist() if a.category() == &#39;person_picks_up_object&#39;]
    newlist = [vo.add(vipy.activity.Activity(startframe=a.startframe(), endframe=a.endframe(), category=&#39;person_abandons_package&#39;, shortlabel=&#39;abandons&#39;, confidence=0.5*a.confidence(), framerate=vo.framerate(), actorid=a.actorid(), attributes={&#39;pip&#39;:&#39;person_puts_down_object&#39;}))
               for a in vo.activitylist() if a.category() == &#39;person_puts_down_object&#39;]
        
    # Vehicle/person interaction: &#39;vehicle_drops_off_person&#39;/&#39;vehicle_picks_up_person&#39;  must be followed by car driving away/pulling up, must be accompanied by person track start/end
    vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                  a.category() == &#39;vehicle_drops_off_person&#39; and
                                                                  (not vo.track(a.actorid()).ismoving(a.middleframe(), a.endframe()+10*vo.framerate()) or
                                                                   not any([t.category() == &#39;person&#39; and t.segment_maxiou(vo.track(a._actorid), t.startframe(), t.startframe()+1) &gt; 0 for t in vo.tracks().values()]))) else a)
    vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                  a.category() == &#39;vehicle_picks_up_person&#39; and
                                                                  (not vo.track(a.actorid()).ismoving(a.startframe()-10*vo.framerate(), a.middleframe()) or
                                                                   not any([t.category() == &#39;person&#39; and t.segment_maxiou(vo.track(a._actorid), t.endframe()-1, t.endframe()) &gt; 0 for t in vo.tracks().values()]))) else a)
    
    # Person/Bicycle track: riding must be accompanied by an associated moving bicycle track
    vo.activityfilter(lambda a: a.id() not in tofinalize or a.category() != &#39;person_rides_bicycle&#39;)
    bikelist = [vo.add(vipy.activity.Activity(startframe=t.startframe(), endframe=t.endframe(), category=&#39;person_rides_bicycle&#39;, shortlabel=&#39;rides&#39;, confidence=t.confidence(samples=8), framerate=vo.framerate(), actorid=t.id(), attributes={&#39;pip&#39;:&#39;person_rides_bicycle&#39;}))
                for (tk,t) in vo.tracks().items() if (t.id() in tofinalize and t.category() == &#39;bicycle&#39; and t.ismoving())]
    
    # Person/Vehicle track: person/vehicle interaction must be accompanied by an associated stopped vehicle track
    dstbox = {k:vo.track(a.actorid()).boundingbox(a.startframe(), a.endframe()) for (k,a) in vo.activities().items() if (a.id() in tofinalize and a.category().startswith(&#39;person&#39;) and (&#39;vehicle&#39; in a.category() or &#39;trunk&#39; in a.category()))}  # precompute            
    vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                  (a.category().startswith(&#39;person&#39;) and (&#39;vehicle&#39; in a.category() or &#39;trunk&#39; in a.category())) and
                                                                  not any([t.category() == &#39;vehicle&#39; and 
                                                                           t.during(a.startframe()) and
                                                                           not t.ismoving(a.startframe(), a.endframe()) and
                                                                           t[a.startframe()].hasintersection(dstbox[a._id])
                                                                           for t in vo.tracks().values()])) else a)
    
    # Vehicle/Person track: vehicle/person interaction must be accompanied by an associated person track
    vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and (a.category().startswith(&#39;vehicle&#39;) and (&#39;person&#39; in a.category())) and not any([t.category() == &#39;person&#39; and t.segment_maxiou(vo.track(a._actorid), a._startframe, a._endframe) &gt; 0 for t in vo.tracks().values()])) else a)
    
    # Person track: enter/exit scene cannot be at the image boundary
    boundary = vo.framebox().dilate(0.9)
    vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and a.category() == &#39;person_enters_scene_through_structure&#39; and vo.track(a.actorid())[max(a.startframe(), vo.track(a.actorid()).startframe())].cover(boundary) &lt; 1) else a)
    vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and a.category() == &#39;person_exits_scene_through_structure&#39; and vo.track(a.actorid())[min(a.endframe(), vo.track(a.actorid()).endframe())].cover(boundary) &lt; 1) else a)
    
    # Activity union:  Temporal gaps less than support should be merged into one activity detection for a single track
    # Activity union:  &#34;Brief&#34; breaks (&lt;5 seconds) of confident activities should be merged into one activity detection for a single track
    briefmerge = set([&#39;person_reads_document&#39;, &#39;person_interacts_with_laptop&#39;, &#39;person_talks_to_person&#39;, &#39;person_purchases&#39;, &#39;person_steals_object&#39;, &#39;person_talks_on_phone&#39;, &#39;person_texts_on_phone&#39;, &#39;person_rides_bicycle&#39;, &#39;person_carries_heavy_object&#39;, &#39;person&#39;, &#39;person_walks&#39;, &#39;vehicle&#39;, &#39;car_moves&#39;])  
    merged = set([])
    mergeable_dets = [a for a in vo.activities().values() if a.id() in tofinalize and a.confidence() &gt; 0.2]  # only mergeable detections
    mergeable_dets.sort(key=lambda a: a.startframe())  # in-place
    for a in mergeable_dets:
        for o in mergeable_dets:
            if ((o._startframe &gt;= a._startframe) and (a._id != o._id) and (o._actorid == a._actorid) and (o._label == a._label) and (o._id not in merged) and (a._id not in merged) and
                ((a.temporal_distance(o) &lt;= self.temporal_support() or (a.category() in briefmerge and a.temporal_distance(o) &lt; 5*vo.framerate())))):
                a.union(o)  # in-place update
                merged.add(o.id())
    vo.activityfilter(lambda a: a.id() not in tofinalize or a.id() not in merged)

    # Group activity: Must be accompanied by a friend with the same activity detection
    categories = [&#39;person_embraces_person&#39;, &#39;hand_interacts_with_person&#39;, &#39;person_talks_to_person&#39;, &#39;person_transfers_object&#39;]           
    dstbox = {k:vo.track(a.actorid()).boundingbox(a.startframe(), a.endframe()) for (k,a) in vo.activities().items() if a.id() in tofinalize and a.category() in categories}  # precompute
    srcbox = {k:bb.clone().maxsquare().dilate(1.2) for (k,bb) in dstbox.items()}                            
    vo.activitymap(lambda a: a.confidence(0.1*a.confidence()) if (a.id() in tofinalize and
                                                                  a._label in categories and
                                                                  not any([(af._label == a._label and
                                                                            af._id != a._id and
                                                                            af._actorid != a._actorid and 
                                                                            srcbox[a._id].hasintersection(dstbox[af._id]))
                                                                           for af in vo.activities().values() if af.during_interval(a._startframe, a._endframe, inclusive=True)])) else a)
    
    # Activity group suppression:  Group activities may have at most one activity detection of this type per group in a spatial region surrounding the actor
    tosuppress = set([&#39;hand_interacts_with_person&#39;, &#39;person_embraces_person&#39;, &#39;person_transfers_object&#39;, &#39;person_steals_object&#39;, &#39;person_purchases&#39;, &#39;person_talks_to_person&#39;])
    suppressed = set([])
    activitybox = {a.id():vo.track(a.actorid()).boundingbox(a.startframe(), a.endframe()) for a in vo.activities().values() if a.id() in tofinalize and a.category() in tosuppress}
    activitybox = {k:bb.dilate(1.2).maxsquare() if bb is not None else bb for (k,bb) in activitybox.items()}
    candidates = [a for a in vo.activities().values() if a.id() in tofinalize]
    for a in sorted(candidates, key=lambda a: a.confidence(), reverse=True):  # decreasing confidence
        if a.category() in tosuppress:
            for o in candidates:  # other activities
                if (o._actorid != a._actorid and  # different tracks
                    o._label == a._label and  # same category
                    o.confidence() &lt;= a.confidence() and   # lower confidence
                    o._id not in suppressed and  # not already suppressed
                    o.during_interval(a.startframe(), a.endframe()) and # overlaps temporally by at least one frame
                    (activitybox[a._id] is not None and activitybox[o._id] is not None) and   # has valid tracks
                    activitybox[a._id].hasintersection(activitybox[o._id]) and  # has coarse overlap 
                    vo.track(a.actorid()).clone().maxsquare().dilate(1.2).segment_maxiou(vo.track(o.actorid()), a.startframe(), a.endframe()) &gt; 0):  # has fine overlap &#34;close by&#34;
                    suppressed.add(o.id())  # greedy non-maximum suppression of lower confidence activity detection
    vo.activityfilter(lambda a: a.id() not in tofinalize or a.id() not in suppressed)

    # Activity duration
    vo.activitymap(lambda a: a.padto(5) if a.id() in tofinalize and a.category() in [&#39;person_talks_to_person&#39;, &#39;person_interacts_with_laptop&#39;, &#39;person_reads_document&#39;, &#39;person_purchases&#39;] else a)   
    vo.activitymap(lambda a: a.duration(2, centered=False) if a.id() in tofinalize and a.category() in [&#39;person_opens_vehicle_door&#39;, &#39;person_closes_vehicle_door&#39;] else a)
    vo.activitymap(lambda a: a.duration(2, centered=True) if a.id() in tofinalize and a.category() in [&#39;person_enters_scene_through_structure&#39;, &#39;person_exits_scene_through_structure&#39;] else a)
    vo.activitymap(lambda a: a.startframe(0) if a.id() in tofinalize and a.startframe() &lt; 0 else a)

    # Activity confidence
    if activityconf is not None:
        vo.activityfilter(lambda a: a.id() not in tofinalize or a.confidence() &gt;= activityconf)

    return vo</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityTracker.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Overload forward for multi-gpu batch.
Don't use torch DataParallel!</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L489-L503" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def forward(self, x):
    &#34;&#34;&#34;Overload forward for multi-gpu batch.  Don&#39;t use torch DataParallel!&#34;&#34;&#34;
    if self._gpus is None:
        return super().forward(x)  # cpu
    else:
        x_forward = None            
        for b in x.pin_memory().split(self._batchsize_per_gpu*len(self._gpus)):  # pinned copy
            n_todevice = np.sum(np.array([1 if k&lt;len(b) else 0 for k in range(int(len(self._devices)*np.ceil(len(b)/len(self._devices))))]).reshape(-1, len(self._devices)), axis=0).tolist()
            todevice = [t.to(d, non_blocking=True) for (t,d) in zip(b.split(n_todevice), self._devices) if len(t)&gt;0]   # async device copy
            ondevice = [m(t) for (m,t) in zip(self._gpus, todevice)]   # async
            fromdevice = torch.cat([t.cpu() for t in ondevice], dim=0)
            x_forward = fromdevice if x_forward is None else torch.cat((x_forward, fromdevice), dim=0)
            del ondevice, todevice, fromdevice, b  # force garbage collection of GPU memory
        del x  # force garbage collection
        return x_forward</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityTracker.lrt"><code class="name flex">
<span>def <span class="ident">lrt</span></span>(<span>self, x_logits, lrt_threshold=None)</span>
</code></dt>
<dd>
<div class="desc"><p>top-k with likelihood ratio test with background null hypothesis</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L505-L515" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def lrt(self, x_logits, lrt_threshold=None):
    &#34;&#34;&#34;top-k with likelihood ratio test with background null hypothesis&#34;&#34;&#34;
    j_bg_person = self._class_to_index[&#39;person&#39;] if &#39;person&#39; in self._class_to_index else self._class_to_index[&#39;person_walks&#39;]  # FIXME
    j_bg_vehicle = self._class_to_index[&#39;vehicle&#39;] if &#39;vehicle&#39; in self._class_to_index else self._class_to_index[&#39;car_moves&#39;]  # FIXME

    yh = x_logits.detach().cpu().numpy()
    yh_softmax = F.softmax(x_logits.detach().cpu(), dim=1)
    p_null = np.maximum(yh[:, j_bg_person], yh[:, j_bg_vehicle]).reshape(yh.shape[0], 1)
    lr = yh - p_null   # ~= log likelihood ratio
    f_logistic = lambda x,b,s=1.0: float(1.0 / (1.0 + np.exp(-s*(x + b))))
    return [sorted([(self.index_to_class(j), float(s[j]), float(t[j]), f_logistic(s[j], 1.0)*f_logistic(t[j], 0.0), float(sm[j])) for j in range(len(s)) if (lrt_threshold is None or t[j] &gt;= lrt_threshold)], key=lambda x: x[3], reverse=True) for (s,t,sm) in zip(yh, lr, yh_softmax)]</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.ActivityTracker.temporal_stride"><code class="name flex">
<span>def <span class="ident">temporal_stride</span></span>(<span>self, s=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L482-L487" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def temporal_stride(self, s=None):
    if s is not None:
        self._stride = s
        return self
    else:
        return self._stride</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="heyvi.recognition.PIP_370k" href="#heyvi.recognition.PIP_370k">PIP_370k</a></b></code>:
<ul class="hlist">
<li><code><a title="heyvi.recognition.PIP_370k.configure_optimizers" href="#heyvi.recognition.PIP_250k.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="heyvi.recognition.PIP_370k.topk" href="#heyvi.recognition.PIP_370k.topk">topk</a></code></li>
<li><code><a title="heyvi.recognition.PIP_370k.totensor" href="#heyvi.recognition.PIP_250k.totensor">totensor</a></code></li>
<li><code><a title="heyvi.recognition.PIP_370k.training_step" href="#heyvi.recognition.PIP_250k.training_step">training_step</a></code></li>
<li><code><a title="heyvi.recognition.PIP_370k.validation_epoch_end" href="#heyvi.recognition.PIP_250k.validation_epoch_end">validation_epoch_end</a></code></li>
<li><code><a title="heyvi.recognition.PIP_370k.validation_step" href="#heyvi.recognition.PIP_250k.validation_step">validation_step</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="heyvi.recognition.ActivityTrackerCap"><code class="flex name class">
<span>class <span class="ident">ActivityTrackerCap</span></span>
<span>(</span><span>stride=3, activities=None, gpus=None, batchsize=None, mlbl=False, mlfl=False, modelfile=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Video Activity detection.</p>
<p>Args (<strong>call</strong>):
vi [generator of <code>vipy.video.Scene</code>]:
The input video to be updated in place with detections.
This is a generator which is output from heyvi.detection.MultiscaleVideoTracker.<strong>call</strong>
activityiou [float]: The minimum temporal iou for activity assignment
mirror [bool]:
If true, encode using the mean of a video encoding and the mirrored video encoding.
This is slower as it requires 2x GPU forward passes
minprob [float]: The minimum probability for new activity detection
trackconf [float]: The minimum object detection confidence for new tracks
maxdets [int]:
The maximum number of allowable detections per frame.
If there are more detections per frame tha maxdets, sort them by confidence and use only the top maxdets best
avgdets [int]:
The number of allowable detections per frame if throttled
buffered [bool]:
If true, then buffer streams.
This is useful for activity detection on live streams.
<br>
finalized [bool, int]:
If False then do not finalize(), If True finalize() only at the end, If int, then finalize every int frames.
This is useful for streaming activity detection on unbounded inputs. </p>
<h2 id="returns">Returns</h2>
<p>The input video is updated in place.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L702-L703" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ActivityTrackerCap(ActivityTracker, CAP):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="heyvi.recognition.ActivityTracker" href="#heyvi.recognition.ActivityTracker">ActivityTracker</a></li>
<li><a title="heyvi.recognition.CAP" href="#heyvi.recognition.CAP">CAP</a></li>
<li><a title="heyvi.recognition.PIP_370k" href="#heyvi.recognition.PIP_370k">PIP_370k</a></li>
<li><a title="heyvi.recognition.PIP_250k" href="#heyvi.recognition.PIP_250k">PIP_250k</a></li>
<li>pytorch_lightning.core.lightning.LightningModule</li>
<li>abc.ABC</li>
<li>pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.grads.GradInformation</li>
<li>pytorch_lightning.core.saving.ModelIO</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
<li><a title="heyvi.recognition.ActivityRecognition" href="#heyvi.recognition.ActivityRecognition">ActivityRecognition</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="heyvi.recognition.ActivityTrackerCap.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="heyvi.recognition.ActivityTrackerCap.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="heyvi.recognition.ActivityTracker" href="#heyvi.recognition.ActivityTracker">ActivityTracker</a></b></code>:
<ul class="hlist">
<li><code><a title="heyvi.recognition.ActivityTracker.configure_optimizers" href="#heyvi.recognition.PIP_250k.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTracker.finalize" href="#heyvi.recognition.ActivityTracker.finalize">finalize</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTracker.forward" href="#heyvi.recognition.ActivityTracker.forward">forward</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTracker.lrt" href="#heyvi.recognition.ActivityTracker.lrt">lrt</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTracker.topk" href="#heyvi.recognition.PIP_370k.topk">topk</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTracker.totensor" href="#heyvi.recognition.PIP_250k.totensor">totensor</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTracker.training_step" href="#heyvi.recognition.PIP_250k.training_step">training_step</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTracker.validation_epoch_end" href="#heyvi.recognition.PIP_250k.validation_epoch_end">validation_epoch_end</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTracker.validation_step" href="#heyvi.recognition.PIP_250k.validation_step">validation_step</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="heyvi.recognition.CAP"><code class="flex name class">
<span>class <span class="ident">CAP</span></span>
<span>(</span><span>modelfile=None, deterministic=True, pretrained=None, mlbl=None, mlfl=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Activity recognition using people in public - 250k stabilized</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L395-L443" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class CAP(PIP_370k, pl.LightningModule, ActivityRecognition):
    def __init__(self, modelfile=None, deterministic=True, pretrained=None, mlbl=None, mlfl=None):
        pl.LightningModule.__init__(self)
        ActivityRecognition.__init__(self)  

        self._input_size = 112
        self._num_frames = 16        
        self._mean = [0.485, 0.456, 0.406]
        self._std = [0.229, 0.224, 0.225]
        self._mlfl = True
        self._mlbl = False
        if deterministic:
            np.random.seed(42)
        
        # Generated using vipy.dataset.Dataset.multilabel_inverse_frequency_weight()
        # WARNING: this was truncated to one
        self._class_to_weight = {&#39;person_talks_on_phone&#39;: 0.0071500571732126764, &#39;car_moves&#39;: 0.008414980303528122, &#39;person_talks_to_person&#39;: 0.009391174775630284, &#39;person_opens_facility_door&#39;: 0.011485169340670632, &#39;person_enters_scene_through_structure&#39;: 0.011577007698961155, &#39;hand_interacts_with_person&#39;: 0.012750528623581633, &#39;person_puts_down_object&#39;: 0.012799173860425943, &#39;person_stands_up&#39;: 0.012843569626160192, &#39;person_exits_scene_through_structure&#39;: 0.014980961043628886, &#39;person_texts_on_phone&#39;: 0.01499085586892757, &#39;person_closes_facility_door&#39;: 0.016113488440289907, &#39;person_sits_down&#39;: 0.017908321006202585, &#39;person_opens_car_door&#39;: 0.01857279522480972, &#39;person_stands_up_from_floor&#39;: 0.019361727910618066, &#39;person_picks_up_object&#39;: 0.019615611319993825, &#39;car_stops&#39;: 0.02019106261740665, &#39;person_closes_car_door&#39;: 0.021202018411369025, &#39;car_starts&#39;: 0.021670313551423053, &#39;person_carries_heavy_object&#39;: 0.02274928459304244, &#39;person_unloads_car&#39;: 0.02351927526525588, &#39;car_turns_right&#39;: 0.024701873211435525, &#39;person_reads_document&#39;: 0.02499051143387395, &#39;car_reverses&#39;: 0.026255434156376472, &#39;person_transfers_object_to_person&#39;: 0.026580220081877853, &#39;person_picks_up_object_from_floor&#39;: 0.029207808500200837, &#39;person_embraces_person&#39;: 0.02945420882971027, &#39;person_interacts_with_laptop&#39;: 0.029911212318566305, &#39;person_exits_car&#39;: 0.030878325568415993, &#39;person_closes_car_trunk&#39;: 0.03114216254299982, &#39;person_loads_car&#39;: 0.03139933495976206, &#39;person_opens_car_trunk&#39;: 0.03154267942003807, &#39;person_enters_car&#39;: 0.03164745668042723, &#39;car_picks_up_person&#39;: 0.033681585653241426, &#39;car_turns_left&#39;: 0.035125658444433834, &#39;car_drops_off_person&#39;: 0.03586046019364838, &#39;person_steals_object&#39;: 0.0415729855145331, &#39;person_picks_up_object_from_table&#39;: 0.04418573296650998, &#39;person_drops_object&#39;: 0.048065094048302635, &#39;person_abandons_package&#39;: 0.04923985842254567, &#39;person_jumps&#39;: 0.05345098137582828, &#39;person_takes_phone_from_pocket&#39;: 0.05462388692326024, &#39;person_trips_on_object_on_floor&#39;: 0.05498089026389937, &#39;person_puts_phone_into_pocket&#39;: 0.056183850529544055, &#39;person_drinks_from_mug&#39;: 0.05666046130633482, &#39;person_drinks_from_bottle&#39;: 0.057329881619530565, &#39;person_sweeps_floor&#39;: 0.06141749219518637, &#39;person_uses_television_remote&#39;: 0.06148643566096271, &#39;person_wipes_mouth_with_napkin&#39;: 0.0616838064677993, &#39;person_sprays_from_bottle&#39;: 0.06264264051441124, &#39;person_salutes&#39;: 0.0628090316681978, &#39;person_puts_down_object_on_table&#39;: 0.06571369688838084, &#39;person_looks_at_wristwatch&#39;: 0.06672561082585902, &#39;person_puts_hands_in_back_pockets&#39;: 0.06787709488015589, &#39;person_punches&#39;: 0.06849983752132058, &#39;person_drinks_from_straw&#39;: 0.06898414063150268, &#39;person_strokes_chin&#39;: 0.06910772776183577, &#39;person_exercises_with_jumping_jacks&#39;: 0.06940183386381935, &#39;person_opens_laptop&#39;: 0.07058098348291678, &#39;person_grabs_person_by_bicep&#39;: 0.07060789551353987, &#39;person_grabs_person_by_forearm&#39;: 0.07071022579689283, &#39;person_strokes_hair&#39;: 0.0711640253790199, &#39;person_grabs_person_by_hair&#39;: 0.07126797516777103, &#39;person_closes_laptop&#39;: 0.07141913528726114, &#39;person_cleans_table_with_rag&#39;: 0.07177957626166052, &#39;person_searches_in_bag&#39;: 0.07240861269366403, &#39;person_exercises_with_pushup&#39;: 0.07245544215462119, &#39;person_picks_up_object_from_bed&#39;: 0.07261587289604508, &#39;person_rides_bicycle&#39;: 0.07343091926113407, &#39;person_brushes_teeth&#39;: 0.07613928807717861, &#39;person_gestures_raise_hand&#39;: 0.07666348171707751, &#39;person_sits_crisscross&#39;: 0.0777406880176164, &#39;person_gestures_be_quiet&#39;: 0.0778337212322166, &#39;person_gestures_cut&#39;: 0.07796334578685198, &#39;person_shoves_person&#39;: 0.07798921962892592, &#39;person_squats&#39;: 0.07838299609818057, &#39;person_puts_on_hat&#39;: 0.07850485876811529, &#39;person_crosses_arms&#39;: 0.07882076865889508, &#39;person_lies_down_on_bed&#39;: 0.0791809589015662, &#39;person_takes_off_hat&#39;: 0.07954505147236295, &#39;person_waves_at_person&#39;: 0.08075149917222121, &#39;person_stirs_mug&#39;: 0.08080499469999347, &#39;person_gestures_behind_me&#39;: 0.08104660431869777, &#39;person_exercises_with_situp&#39;: 0.08108438212368972, &#39;person_points_at_person&#39;: 0.0813710070044297, &#39;person_claps_hands&#39;: 0.0813710070044297, &#39;person_pats_head&#39;: 0.0825550859557632, &#39;person_gestures_listen_closely&#39;: 0.08286354585573379, &#39;person_bumps_into_wall&#39;: 0.0831459710290662, &#39;person_gestures_come_here&#39;: 0.08328790679388197, &#39;person_eats_snack_from_bag&#39;: 0.08403922538881889, &#39;person_puts_on_glasses&#39;: 0.08409178869330584, &#39;person_gestures_blow_kiss&#39;: 0.08417884023439623, &#39;person_picks_up_object_from_countertop&#39;: 0.08545575718410064, &#39;person_gestures_stop&#39;: 0.08591091430900676, &#39;person_takes_off_glasses&#39;: 0.08607984438930144, &#39;person_takes_off_facemask&#39;: 0.08657028030837606, &#39;person_gestures_lower_hand&#39;: 0.08671760324527736, &#39;person_puts_on_facemask&#39;: 0.0874573927099682, &#39;person_pours_into_bowl&#39;: 0.08757414051616512, &#39;person_gestures_watch_closely&#39;: 0.08822794900516465, &#39;person_gestures_number_five&#39;: 0.08909798356438287, &#39;person_gestures_number_three&#39;: 0.08913053671877248, &#39;person_gestures_number_one&#39;: 0.08932635627948746, &#39;person_gestures_arms_x&#39;: 0.08935907655651291, &#39;person_gestures_thumbs_up&#39;: 0.08974733507235413, &#39;person_gestures_number_four&#39;: 0.08981969035319597, &#39;person_searches_in_box&#39;: 0.09051958404425983, &#39;person_karate_kicks&#39;: 0.09061798903721408, &#39;person_gestures_heart&#39;: 0.09078908783002615, &#39;person_gestures_call_me&#39;: 0.09171063120273695, &#39;person_karate_chop&#39;: 0.09211588142846427, &#39;person_covers_face_with_hands&#39;: 0.09212623829277955, &#39;person_drinks_from_cup&#39;: 0.09237747667762447, &#39;person_bounces_ball_on_floor&#39;: 0.09314634555146249, &#39;person_takes_off_headphones&#39;: 0.09357509742972007, &#39;person_puts_on_headphones&#39;: 0.09422567748137516, &#39;person_puts_down_object_on_countertop&#39;: 0.09437718336136768, &#39;person_bows&#39;: 0.0948116125142947, &#39;person_searches_in_couch&#39;: 0.09669055846186297, &#39;person_gestures_thumbs_down&#39;: 0.09682838430256516, &#39;person_exercises_with_lunges&#39;: 0.09696408981583088, &#39;person_picks_up_object_from_couch&#39;: 0.09775818840195175, &#39;person_hugs_stuffed_animal&#39;: 0.09944976722351416, &#39;person_reads_book&#39;: 0.09957154244868582, &#39;person_sneezees into arm&#39;: 0.10051239314918431, &#39;person_takes_object_from_bag&#39;: 0.10070841311819703, &#39;person_puts_down_object_on_couch&#39;: 0.10077857312928755, &#39;person_sneezes_into_hand&#39;: 0.10219953037255143, &#39;person_lies_down_on_floor&#39;: 0.10227736307765319, &#39;person_tucks_in_shirt&#39;: 0.10246240170736202, &#39;person_puts_feet_up&#39;: 0.10246595377450825, &#39;person_yawns&#39;: 0.10284787750290855, &#39;person_crosses_legs&#39;: 0.10306306675085773, &#39;person_laughs_with_person&#39;: 0.10310178497269419, &#39;person_takes_object_from_cabinet&#39;: 0.10344711927898777, &#39;person_clips_fingernails&#39;: 0.10358822887442899, &#39;person_closes_cabinet&#39;: 0.10364868957787093, &#39;person_opens_refrigerator&#39;: 0.10400807518189813, &#39;person_puts_object_in_cabinet&#39;: 0.10437215722553372, &#39;person_applies_deodorant&#39;: 0.10438608429579814, &#39;person_reads_magazine&#39;: 0.10452025664065136, &#39;person_lies_down_on_couch&#39;: 0.10479046066920758, &#39;person_rubs_neck&#39;: 0.10515098232727597, &#39;person_dances_in_place&#39;: 0.10528096409904394, &#39;person_looks_at_hands_in_lap&#39;: 0.10560618138496981, &#39;person_shrugs&#39;: 0.10615764969507409, &#39;person_opens_curtains&#39;: 0.1064289157366757, &#39;person_carries_groceries&#39;: 0.10677998502422664, &#39;person_closes_curtains&#39;: 0.10701196252783704, &#39;person_puts_on_scarf&#39;: 0.10707652735115192, &#39;person_holds_object_above_head&#39;: 0.10746708325959484, &#39;person_puts_object_into_bag&#39;: 0.10770431743897584, &#39;person_touches_earlobe&#39;: 0.10789486023851405, &#39;person_crawls&#39;: 0.10814051028495372, &#39;person_puts_down_object_on_bed&#39;: 0.10908517308037075, &#39;person_swats_bug&#39;: 0.10929671998175639, &#39;person_rubs_foot&#39;: 0.1093457099951951, &#39;person_rubs_eyes&#39;: 0.10944382189290276, &#39;person_taps_object_with_finger&#39;: 0.10959132030515735, &#39;person_twirls&#39;: 0.10993703424933766, &#39;person_files_fingernails&#39;: 0.10993703424933766, &#39;person_gestures_swipe_up&#39;: 0.11013924537090604, &#39;person_eats_with_hands&#39;: 0.11072316058030798, &#39;person_gestures_swipe_left&#39;: 0.11113247931061442, &#39;person_closes_refrigerator&#39;: 0.1111555983642474, &#39;person_scratches_face&#39;: 0.11180122777235575, &#39;person_gestures_peace&#39;: 0.11185248922479608, &#39;person_searches_under_couch&#39;: 0.11190379770609186, &#39;person_gestures_swipe_down&#39;: 0.11198350367806151, &#39;person_tears_paper&#39;: 0.11205800597118983, &#39;person_cracks_knuckles&#39;: 0.11216104781576104, &#39;person_folds_towel&#39;: 0.11236065501421866, &#39;person_takes_off_scarf&#39;: 0.11262669819417719, &#39;person_takes_medicine_pills&#39;: 0.11262709095072956, &#39;person_untucks_shirt&#39;: 0.11283546669716941, &#39;person_folds_pants&#39;: 0.11297390288995361, &#39;person_searches_in_kitchen_drawer&#39;: 0.11309702318000939, &#39;person_gestures_swipe_right&#39;: 0.1132382679012143, &#39;person_eats_with_utensil&#39;: 0.11360054487971481, &#39;person_waves_hand_over_object&#39;: 0.11367673765110917, &#39;person_squeezes_object&#39;: 0.11372973379919825, &#39;person_folds_shirt&#39;: 0.11375160594315399, &#39;person_licks_fingers&#39;: 0.11442320778577873, &#39;person_folds_socks&#39;: 0.11491150791364328, &#39;person_puts_down_person&#39;: 0.11519465416162661, &#39;person_grabs_person_by_shoulder&#39;: 0.11572593880421266, &#39;person_carries_furniture&#39;: 0.11583583998066489, &#39;person_carries_laundry_basket&#39;: 0.11628835960688708, &#39;person_opens_cabinet&#39;: 0.1163328202475424, &#39;person_crumples_paper&#39;: 0.11633298950847891, &#39;person_spins_person_around&#39;: 0.11644404725502637, &#39;person_picks_up_object_from_cabinet&#39;: 0.11722742863973104, &#39;person_brushes_hair&#39;: 0.11752581209517853, &#39;person_puts_fingers_in_ear&#39;: 0.11756639951772542, &#39;person_wiggles_hips&#39;: 0.11819296463143424, &#39;person_nudges_person_with_elbow&#39;: 0.11864035680381292, &#39;person_touches_face_of_person&#39;: 0.1199952183961044, &#39;person_touches_back_of_person&#39;: 0.1201040449053256, &#39;person_puts_down_object_into_cabinet&#39;: 0.12100708283694457, &#39;person_brushes_hair_of_person&#39;: 0.12197513949964013, &#39;person_searches_in_backback&#39;: 0.12213018201245313, &#39;person_blows_nose&#39;: 0.12264971292070401, &#39;person_pounds_shoulders_of_person&#39;: 0.12281303232588005, &#39;person_takes_off_shoes&#39;: 0.12350532456683978, &#39;person_jumps_on_couch&#39;: 0.12378164652675162, &#39;person_mops&#39;: 0.1252311493836141, &#39;person_snaps_fingers&#39;: 0.1256180633363956, &#39;person_puts_on_shoes&#39;: 0.12636147834728292, &#39;person_washes_hands&#39;: 0.12684469836917986, &#39;person_puts_on_sunglasses&#39;: 0.12747187073013944, &#39;person_flosses&#39;: 0.1277895646931798, &#39;person_kneels&#39;: 0.1278423621928503, &#39;person_reads_newspaper&#39;: 0.12825987329089392, &#39;person_kisses_cheek_of_person&#39;: 0.12886966666628646, &#39;person_puts_object_into_backpack&#39;: 0.12991164284856274, &#39;person_polishes_car_with_rag&#39;: 0.13003746215313447, &#39;person_unloads_clothes_from_suitcase&#39;: 0.1302110517855032, &#39;person_dries_hair_with_towel&#39;: 0.1302457442601603, &#39;person_dries_hands_with_towel&#39;: 0.1306844570815548, &#39;person_turns_off_faucet&#39;: 0.13197629371214256, &#39;person_turns_on_faucet&#39;: 0.13294031635069914, &#39;person_puts_on_jacket&#39;: 0.1332673096496579, &#39;person_searches_under_bed&#39;: 0.13337904811332982, &#39;person_dries_dish&#39;: 0.1335343175960496, &#39;person_transfers_object_to_car&#39;: 0.13530242872949544, &#39;person_drums_on_chest&#39;: 0.13582977672565716, &#39;person_stretches_back&#39;: 0.13635674842915196, &#39;person_puts_on_gloves&#39;: 0.13785597770489835, &#39;person_gestures_hang_loose&#39;: 0.1381371908263195, &#39;person_applies_lip_makeup&#39;: 0.13821545552367154, &#39;person_picks_up_object_from_shelf&#39;: 0.13901394290468203, &#39;person_takes_object_from_backpack&#39;: 0.1390887336531267, &#39;person_slaps_hands_on_thighs&#39;: 0.13932054768662494, &#39;person_takes_off_sunglasses&#39;: 0.13955965617807797, &#39;person_paints_fingernails&#39;: 0.141502482018144, &#39;person_opens_home_window&#39;: 0.14158460766063857, &#39;person_applies_eye_makeup&#39;: 0.14191406573547427, &#39;person_closes_home_window&#39;: 0.14205455016553906, &#39;person_climbs_on_chair&#39;: 0.14286416865055518, &#39;person_opens_dresser_drawer&#39;: 0.14392346843615356, &#39;person_opens_closet_door&#39;: 0.14409349025356186, &#39;person_discards_trash&#39;: 0.14619789236857958, &#39;person_puts_down_object_on_shelf&#39;: 0.14623046508025042, &#39;person_climbs_off_chair&#39;: 0.1466640130633881, &#39;person_pulls_out_chair&#39;: 0.1467564560629468, &#39;person_stretches_arms_over_head&#39;: 0.14774021316767652, &#39;person_takes_off_gloves&#39;: 0.14792482083182573, &#39;person_hikes_up_pants&#39;: 0.14811795931953872, &#39;person_closes_closet_door&#39;: 0.14838824756647218, &#39;person_puts_on_wristwatch&#39;: 0.14846758639707738, &#39;person_prays&#39;: 0.149479337622108, &#39;person_opens_suitcase&#39;: 0.14999956703824965, &#39;person_washes_dish&#39;: 0.15061335914443671, &#39;person_interacts_with_tablet&#39;: 0.15067960407614595, &#39;person_sticks_out_tongue&#39;: 0.15114639343202, &#39;person_puts_up_picture_frame&#39;: 0.15124009857363935, &#39;person_stretches_arms_to_side&#39;: 0.1517687347875614, &#39;person_ties_jacket_around_waist&#39;: 0.15205524466480033, &#39;person_closes_dresser_drawer&#39;: 0.15256427704770498, &#39;person_takes_object_from_kitchen_drawer&#39;: 0.15292174958247323, &#39;person_covers_with_blanket&#39;: 0.1532186939054849, &#39;person_applies_foundation_makeup&#39;: 0.15362108249324954, &#39;person_takes_down_picture_frame&#39;: 0.15371788216715832, &#39;person_takes_selfie&#39;: 0.1544966934764283, &#39;person_walks_around_car&#39;: 0.1545946001262866, &#39;person_jumps_on_bed&#39;: 0.15469263094437558, &#39;person_zips_up_jacket&#39;: 0.15477837356130825, &#39;person_closes_suitcase&#39;: 0.1548318044264797, &#39;person_sets_upright_glass&#39;: 0.15528343666408673, &#39;person_takes_off_jacket&#39;: 0.1557468846718362, &#39;person_puts_object_into_purse&#39;: 0.15597843925785185, &#39;person_searches_in_purse&#39;: 0.1564786908269918, &#39;person_puts_on_socks&#39;: 0.15697187998993098, &#39;person_unties_jacket_around_waist&#39;: 0.1575376799198842, &#39;person_takes_off_backpack&#39;: 0.15780303861823264, &#39;person_arranges_flowers_in_vase&#39;: 0.15789662071150826, &#39;person_opens_kitchen_drawer&#39;: 0.15796859678500738, &#39;person_unzips_jacket&#39;: 0.15821465973786641, &#39;person_puts_on_backpack&#39;: 0.1591174653417599, &#39;person_closes_kitchen_drawer&#39;: 0.16220003797595112, &#39;person_puts_on_necklace&#39;: 0.1626663379799898, &#39;person_puts_object_into_kitchen_drawer&#39;: 0.16363350184412317, &#39;person_takes_off_socks&#39;: 0.1637836332966157, &#39;person_hits_person_with_pillow&#39;: 0.16383497582221643, &#39;person_climbs_on_couch&#39;: 0.16385203176578586, &#39;person_throws_object_into_air&#39;: 0.16542943627126713, &#39;person_takes_object_from_purse&#39;: 0.16550222455853475, &#39;person_kicks_car_tires&#39;: 0.1662919420581324, &#39;person_loads_clothes_into_suitcase&#39;: 0.16665572681128546, &#39;person_takes_object_from_basket&#39;: 0.16704418245100291, &#39;person_takes_off_necklace&#39;: 0.16720375531136414, &#39;person_burns_hand&#39;: 0.16766342199263248, &#39;person_walks_tiptoe&#39;: 0.16766342199263248, &#39;person_applies_sunscreen&#39;: 0.1694099159717224, &#39;person_pushes_in_chair&#39;: 0.17093473022331301, &#39;person_nods_head&#39;: 0.1730143822689931, &#39;person_puts_on_ring&#39;: 0.17320954516305123, &#39;person_puts_object_into_basket&#39;: 0.17345866516751823, &#39;person_covers_friend_with_blanket&#39;: 0.17375375997099735, &#39;person_shakes_head&#39;: 0.1741258236968453, &#39;person_takes_off_wristwatch&#39;: 0.1751258284273369, &#39;person_lights_candle&#39;: 0.17639210339788883, &#39;person_picks_up_person&#39;: 0.1775475101887047, &#39;person_shades_eyes&#39;: 0.17898039545068253, &#39;person_uncovers_friend_with_blanket&#39;: 0.17990433554519192, &#39;person_closes_clothes_washer&#39;: 0.18070391036983724, &#39;person_blows_into_hands&#39;: 0.18097201706178062, &#39;person_opens_clothes_washer&#39;: 0.18097201706178062, &#39;person_puts_on_belt&#39;: 0.1826837651926084, &#39;person_bumps_into_table&#39;: 0.18273429138522868, &#39;person_climbs_off_couch&#39;: 0.18411341811266432, &#39;person_uncovers_with_blanket&#39;: 0.1846559266696029, &#39;person_eats_apple&#39;: 0.1855135201515439, &#39;person_opens_box&#39;: 0.18707843481539896, &#39;person_takes_off_ring&#39;: 0.18750982244372041, &#39;person_throws_object_to_ground&#39;: 0.1885241723332923, &#39;person_locks_door_with_keys&#39;: 0.1891087434102948, &#39;person_unlocks_door_with_keys&#39;: 0.18954955633199708, &#39;person_vacuums_carpet&#39;: 0.18954955633199708, &#39;person_opens_microwave&#39;: 0.18967163518640828, &#39;person_extinguishes_candle&#39;: 0.18984457509671618, &#39;person_applies_facial_moisturizer&#39;: 0.19043737626797835, &#39;person_eats_banana&#39;: 0.1929986384487977, &#39;person_stubs_toe&#39;: 0.19395487244437154, &#39;person_puts_on_shirt&#39;: 0.194933929667213, &#39;person_closes_box&#39;: 0.195472979967372, &#39;person_puts_down_object_on_floor&#39;: 0.19599558450709312, &#39;person_throws_object_on_table&#39;: 0.19898065171230037, &#39;person_opens_jar&#39;: 0.20028758538528754, &#39;person_drinks_from_beverage_can&#39;: 0.20218778029011394, &#39;person_closes_microwave&#39;: 0.20322926171591793, &#39;person_exercises_with_plank&#39;: 0.2032303156689395, &#39;person_takes_clothes_from_dresser&#39;: 0.2043134664985597, &#39;person_carries_bicycle&#39;: 0.20500023445317672, &#39;person_dusts_furniture&#39;: 0.20813094360488035, &#39;person_closes_jar&#39;: 0.20921979330984586, &#39;person_knocks_over_glass&#39;: 0.21030196465455195, &#39;person_takes_off_belt&#39;: 0.21048341587513394, &#39;person_closes_door_with_foot&#39;: 0.21066518048297084, &#39;person_puts_clothes_into_dresser&#39;: 0.21066518048297084, &#39;person_kicks_object_to_person&#39;: 0.21194637619398804, &#39;person_pours_coffee_into_mug&#39;: 0.2139914728063862, &#39;person_buttons_shirt&#39;: 0.21535941093384248, &#39;person_falls_from_chair&#39;: 0.21573745621114188, &#39;person_puts_on_hoodie&#39;: 0.21625599556698347, &#39;person_opens_beverage_can&#39;: 0.21703761476804292, &#39;person_puts_on_earrings&#39;: 0.21786207374692124, &#39;person_climbs_on_table&#39;: 0.22102665713111078, &#39;person_climbs_off_table&#39;: 0.22195290532393117, &#39;person_takes_off_earrings&#39;: 0.2233976913912823, &#39;person_waters_houseplant&#39;: 0.2236024555447115, &#39;person_opens_oven_door&#39;: 0.22407580298906604, &#39;person_wraps_box&#39;: 0.22567093339433883, &#39;person_loads_clotheswasher&#39;: 0.22927657800684234, &#39;person_closes_oven_door&#39;: 0.2313013332980745, &#39;person_closes_door_with_hip&#39;: 0.23233359904693357, &#39;person_unbuttons_shirt&#39;: 0.2332220640528492, &#39;person_pushes_wheeled_cart&#39;: 0.2375367857831356, &#39;person_pulls_wheeled_cart&#39;: 0.23963681630577627, &#39;person_dries_hair_with_hairdryer&#39;: 0.24010854232212622, &#39;person_takes_off_shirt&#39;: 0.24034510246234506, &#39;person_irons_clothes&#39;: 0.2405821291906117, &#39;person_bumps_into_person&#39;: 0.2419661565158503, &#39;person_purchases_from_machine&#39;: 0.24256228370916671, &#39;person_purchases_from_cashier&#39;: 0.2500715967813632, &#39;person_takes_off_hoodie&#39;: 0.2512361266727912, &#39;person_flips_up_car_wipers&#39;: 0.25625029306647085, &#39;person_throws_object_to_person&#39;: 0.2584219057195765, &#39;person_climbs_up_stairs&#39;: 0.25924578002048915, &#39;person_drinks_from_shotglass&#39;: 0.26007492430626894, &#39;person_puts_on_pants&#39;: 0.26180106773743944, &#39;person_flips_down_car_wipers&#39;: 0.26203037486496267, &#39;person_spreads_tablecloth&#39;: 0.26287745581818994, &#39;person_takes_clothes_from_closet&#39;: 0.26373003135057327, &#39;person_folds_tablecloth&#39;: 0.26458815509683326, &#39;person_pours_liquid_into_cup&#39;: 0.26716946140944486, &#39;person_takes_selfie_with_person&#39;: 0.26837214411361965, &#39;person_washes_window&#39;: 0.2688077926543526, &#39;person_closes_car_hood&#39;: 0.271357373747809, &#39;person_climbs_down_stairs&#39;: 0.2719624069111263, &#39;person_covers_with_bedsheets&#39;: 0.2733971404050831, &#39;person_opens_car_hood&#39;: 0.274101437077843, &#39;person_puts_on_necktie&#39;: 0.2741340156665674, &#39;person_tickles_person&#39;: 0.275961854071584, &#39;person_uncovers_with_bedsheets&#39;: 0.27733168911014544, &#39;person_catches_dropped_object&#39;: 0.2784711362375529, &#39;person_lifts_dummbells&#39;: 0.2788003188563203, &#39;person_climbs_up_ladder&#39;: 0.28008068771444344, &#39;person_sets_table&#39;: 0.2810487085245164, &#39;person_puts_object_into_microwave&#39;: 0.2823920373735829, &#39;person_folds_blanket&#39;: 0.28267703244412545, &#39;person_interacts_with_handheld_game&#39;: 0.2839933399293134, &#39;person_looks_over_shoulder&#39;: 0.28532196374184826, &#39;person_sets_upright_furniture&#39;: 0.28532196374184826, &#39;person_catches_object_from_person&#39;: 0.2870003282344473, &#39;person_knocks_over_furniture&#39;: 0.2870003282344473, &#39;person_walks&#39;: 0.2893834863573906, &#39;person_puts_hair_in_ponytail&#39;: 0.290763145410346, &#39;person_climbs_down_ladder&#39;: 0.290763145410346, &#39;person_takes_off_pants&#39;: 0.2939159987943136, &#39;person_puts_clothes_into_closet&#39;: 0.2953393208223732, &#39;person_searches_in_cabinet&#39;: 0.2956973078779155, &#39;person_throws_object_on_bed&#39;: 0.29677649513294435, &#39;person_washes_face&#39;: 0.2971379768566142, &#39;person_carries_person_over_shoulder&#39;: 0.2972615412890275, &#39;person_takes_object_from_microwave&#39;: 0.2981634008906294, &#39;person_opens_can_with_can_opener&#39;: 0.29969321744383326, &#39;person_unfolds_blanket&#39;: 0.30117318394972875, &#39;person_dries_face_with_towel&#39;: 0.3041774052360103, &#39;person_loads_groceries_into_refrigerator&#39;: 0.306085670011644, &#39;person_puts_on_apron&#39;: 0.3076296078175035, &#39;person_makes_bed&#39;: 0.3099749415492761, &#39;person_takes_off_hairtie&#39;: 0.3119568785157037, &#39;person_unloads_clotheswasher&#39;: 0.31356076992195403, &#39;person_takes_off_apron&#39;: 0.31518123901715794, &#39;person_turns_off_fan&#39;: 0.3159977707244563, &#39;person_turns_on_lamp&#39;: 0.31681854415490945, &#39;person_turns_on_fan&#39;: 0.31788266098916557, &#39;person_opens_sliding_door&#39;: 0.31805773011640187, &#39;person_closes_sliding_door&#39;: 0.32141011725860374, &#39;person_takes_off_necktie&#39;: 0.324401966754362, &#39;person_hugs_person_from_behind&#39;: 0.324401966754362, &#39;person_loads_groceries_into_cabinet&#39;: 0.3257013070751405, &#39;person_carries_person_on_back&#39;: 0.3283314656787083, &#39;person_braids_hair_of_person&#39;: 0.3323573283368941, &#39;person_turns_on_stovetop&#39;: 0.3346368710552541, &#39;person_falls_from_bed&#39;: 0.3350905862080302, &#39;person_uses_bodyweight_scale&#39;: 0.3426267963473037, &#39;person_turns_off_lamp&#39;: 0.3431086905756403, &#39;person_vapes&#39;: 0.3435919422525074, &#39;person_turns_off_stovetop&#39;: 0.3460287645379862, &#39;person_closes_gate&#39;: 0.34701319914549117, &#39;person_opens_gate&#39;: 0.3475075199419947, &#39;person_puts_object_into_refrigerator&#39;: 0.3530701457147036, &#39;person_puts_object_into_oven&#39;: 0.3572873745649558, &#39;person_unscrews_lid_from_bottle&#39;: 0.35748569200453895, &#39;person_applies_shaving_cream&#39;: 0.35927876141278386, &#39;person_takes_object_from_refrigerator&#39;: 0.3595562632398506, &#39;person_shaves_face&#39;: 0.3646491464862186, &#39;person_takes_object_from_oven&#39;: 0.36930257807780786, &#39;person_screws_lid_to_bottle&#39;: 0.3758864083193841, &#39;person_loads_dryer&#39;: 0.3947415517787706, &#39;person_falls_while_standing&#39;: 0.4050786163671186, &#39;person_inserts_trashbag_into_trashcan&#39;: 0.47599885750525034, &#39;person_removes_trashbag_from_trashcan&#39;: 0.47707269147228026, &#39;person_gestures_zoom_out&#39;: 0.5082297479151672, &#39;person_gestures_zoom_in&#39;: 0.5291763101936665, &#39;person_unloads_dryer&#39;: 0.5421117311095116, &#39;person_handstand&#39;: 0.566009928072576, &#39;person_pets_dog&#39;: 0.6402894461923366, &#39;person_plugs_into_electrical_socket&#39;: 0.648803933508724, &#39;person_somersaults&#39;: 0.648803933508724, &#39;person_unplugs_from_electrical_socket&#39;: 0.6629083668458703, &#39;person_carries_person_on_shoulders&#39;: 0.6891250819188708, &#39;person_adjusts_thermostat&#39;: 0.7370099063422364, &#39;person_hugs_dog&#39;: 0.7719945537951908, &#39;person_attaches_leash_to_dog&#39;: 0.786936383868646, &#39;person_opens_dishwasher&#39;: 0.7920463603872736, &#39;person_closes_dishwasher&#39;: 0.7946263159585677, &#39;person_pets_cat&#39;: 0.8297628537390485, &#39;person_searches_jewelry_box&#39;: 0.8354461609564392, &#39;person_hugs_cat&#39;: 0.847049579858612, &#39;person_opens_jewelry_box&#39;: 0.8559658912255448, &#39;person_unattaches_leash_from_dog&#39;: 0.8620151201387994, &#39;person_unloads_dishwasher&#39;: 0.8712509964260009, &#39;person_loads_dishwasher&#39;: 0.8743737598540511, &#39;person_closes_jewelry_box&#39;: 0.9205670905633218, &#39;person_puts_object_into_toaster&#39;: 0.9642303517758113, &#39;person_takes_object_from_toaster&#39;: 0.9797199959810451, &#39;person_puts_on_boots&#39;: 0.9957154244868583, &#39;person_takes_off_boots&#39;: 1, &#39;person_unloads_box_onto_floor&#39;: 1, &#39;person_spills_on_table&#39;: 1, &#39;person_cleans_dryer_lint_trap&#39;: 1, &#39;person_exits_pool&#39;: 1, &#39;person_unloads_box_onto_table&#39;: 1, &#39;person_spills_on_floor&#39;: 1, &#39;person_texts_on_phone_while_sitting&#39;: 1, &#39;person_closes_mailbox&#39;: 1, &#39;person_opens_mailbox&#39;: 1, &#39;person_puts_object_into_box&#39;: 1, &#39;person_takes_object_from_box&#39;: 1, &#39;person_takes_down_smoke_detector&#39;: 1, &#39;person_puts_up_smoke_detector&#39;: 1, &#39;person_trips_on_stair&#39;: 1, &#39;person_jumps_into_pool&#39;: 1, &#39;person_falls_into_pool&#39;: 1, &#39;person_pulls_wheeled_trashcan&#39;: 1, &#39;person_uncrates_dog&#39;: 1, &#39;person_crates_dog&#39;: 1, &#39;person_pushes_wheeled_trashcan&#39;: 1, &#39;person_leaves_scene_through_structure&#39;: 1, &#39;person_crawls_out_from_under_vehicle&#39;: 1, &#39;person_feeds_dog&#39;: 1, &#39;person_feeds_cat&#39;: 1, &#39;person_points_to_dog&#39;: 1, &#39;person_throws_object_to_dog&#39;: 1, &#39;person_shakes_hand&#39;: 1, &#39;person_cleans_eyeglasses&#39;: 1, &#39;person_holds_hand&#39;: 1, &#39;person_embraces_sitting_person&#39;: 1}

        
        # Generated using vipy.dataset.Dataset.class_to_index()
        self._class_to_index = {&#39;car_drops_off_person&#39;: 0, &#39;car_moves&#39;: 1, &#39;car_picks_up_person&#39;: 2, &#39;car_reverses&#39;: 3, &#39;car_starts&#39;: 4, &#39;car_stops&#39;: 5, &#39;car_turns_left&#39;: 6, &#39;car_turns_right&#39;: 7, &#39;hand_interacts_with_person&#39;: 8, &#39;person_abandons_package&#39;: 9, &#39;person_adjusts_thermostat&#39;: 10, &#39;person_applies_deodorant&#39;: 11, &#39;person_applies_eye_makeup&#39;: 12, &#39;person_applies_facial_moisturizer&#39;: 13, &#39;person_applies_foundation_makeup&#39;: 14, &#39;person_applies_lip_makeup&#39;: 15, &#39;person_applies_shaving_cream&#39;: 16, &#39;person_applies_sunscreen&#39;: 17, &#39;person_arranges_flowers_in_vase&#39;: 18, &#39;person_attaches_leash_to_dog&#39;: 19, &#39;person_blows_into_hands&#39;: 20, &#39;person_blows_nose&#39;: 21, &#39;person_bounces_ball_on_floor&#39;: 22, &#39;person_bows&#39;: 23, &#39;person_braids_hair_of_person&#39;: 24, &#39;person_brushes_hair&#39;: 25, &#39;person_brushes_hair_of_person&#39;: 26, &#39;person_brushes_teeth&#39;: 27, &#39;person_bumps_into_person&#39;: 28, &#39;person_bumps_into_table&#39;: 29, &#39;person_bumps_into_wall&#39;: 30, &#39;person_burns_hand&#39;: 31, &#39;person_buttons_shirt&#39;: 32, &#39;person_carries_bicycle&#39;: 33, &#39;person_carries_furniture&#39;: 34, &#39;person_carries_groceries&#39;: 35, &#39;person_carries_heavy_object&#39;: 36, &#39;person_carries_laundry_basket&#39;: 37, &#39;person_carries_person_on_back&#39;: 38, &#39;person_carries_person_on_shoulders&#39;: 39, &#39;person_carries_person_over_shoulder&#39;: 40, &#39;person_catches_dropped_object&#39;: 41, &#39;person_catches_object_from_person&#39;: 42, &#39;person_claps_hands&#39;: 43, &#39;person_cleans_dryer_lint_trap&#39;: 44, &#39;person_cleans_eyeglasses&#39;: 45, &#39;person_cleans_table_with_rag&#39;: 46, &#39;person_climbs_down_ladder&#39;: 47, &#39;person_climbs_down_stairs&#39;: 48, &#39;person_climbs_off_chair&#39;: 49, &#39;person_climbs_off_couch&#39;: 50, &#39;person_climbs_off_table&#39;: 51, &#39;person_climbs_on_chair&#39;: 52, &#39;person_climbs_on_couch&#39;: 53, &#39;person_climbs_on_table&#39;: 54, &#39;person_climbs_up_ladder&#39;: 55, &#39;person_climbs_up_stairs&#39;: 56, &#39;person_clips_fingernails&#39;: 57, &#39;person_closes_box&#39;: 58, &#39;person_closes_cabinet&#39;: 59, &#39;person_closes_car_door&#39;: 60, &#39;person_closes_car_hood&#39;: 61, &#39;person_closes_car_trunk&#39;: 62, &#39;person_closes_closet_door&#39;: 63, &#39;person_closes_clothes_washer&#39;: 64, &#39;person_closes_curtains&#39;: 65, &#39;person_closes_dishwasher&#39;: 66, &#39;person_closes_door_with_foot&#39;: 67, &#39;person_closes_door_with_hip&#39;: 68, &#39;person_closes_dresser_drawer&#39;: 69, &#39;person_closes_facility_door&#39;: 70, &#39;person_closes_gate&#39;: 71, &#39;person_closes_home_window&#39;: 72, &#39;person_closes_jar&#39;: 73, &#39;person_closes_jewelry_box&#39;: 74, &#39;person_closes_kitchen_drawer&#39;: 75, &#39;person_closes_laptop&#39;: 76, &#39;person_closes_mailbox&#39;: 77, &#39;person_closes_microwave&#39;: 78, &#39;person_closes_oven_door&#39;: 79, &#39;person_closes_refrigerator&#39;: 80, &#39;person_closes_sliding_door&#39;: 81, &#39;person_closes_suitcase&#39;: 82, &#39;person_covers_face_with_hands&#39;: 83, &#39;person_covers_friend_with_blanket&#39;: 84, &#39;person_covers_with_bedsheets&#39;: 85, &#39;person_covers_with_blanket&#39;: 86, &#39;person_cracks_knuckles&#39;: 87, &#39;person_crates_dog&#39;: 88, &#39;person_crawls&#39;: 89, &#39;person_crawls_out_from_under_vehicle&#39;: 90, &#39;person_crosses_arms&#39;: 91, &#39;person_crosses_legs&#39;: 92, &#39;person_crumples_paper&#39;: 93, &#39;person_dances_in_place&#39;: 94, &#39;person_discards_trash&#39;: 95, &#39;person_dries_dish&#39;: 96, &#39;person_dries_face_with_towel&#39;: 97, &#39;person_dries_hair_with_hairdryer&#39;: 98, &#39;person_dries_hair_with_towel&#39;: 99, &#39;person_dries_hands_with_towel&#39;: 100, &#39;person_drinks_from_beverage_can&#39;: 101, &#39;person_drinks_from_bottle&#39;: 102, &#39;person_drinks_from_cup&#39;: 103, &#39;person_drinks_from_mug&#39;: 104, &#39;person_drinks_from_shotglass&#39;: 105, &#39;person_drinks_from_straw&#39;: 106, &#39;person_drops_object&#39;: 107, &#39;person_drums_on_chest&#39;: 108, &#39;person_dusts_furniture&#39;: 109, &#39;person_eats_apple&#39;: 110, &#39;person_eats_banana&#39;: 111, &#39;person_eats_snack_from_bag&#39;: 112, &#39;person_eats_with_hands&#39;: 113, &#39;person_eats_with_utensil&#39;: 114, &#39;person_embraces_person&#39;: 115, &#39;person_embraces_sitting_person&#39;: 116, &#39;person_enters_car&#39;: 117, &#39;person_enters_scene_through_structure&#39;: 118, &#39;person_exercises_with_jumping_jacks&#39;: 119, &#39;person_exercises_with_lunges&#39;: 120, &#39;person_exercises_with_plank&#39;: 121, &#39;person_exercises_with_pushup&#39;: 122, &#39;person_exercises_with_situp&#39;: 123, &#39;person_exits_car&#39;: 124, &#39;person_exits_pool&#39;: 125, &#39;person_exits_scene_through_structure&#39;: 126, &#39;person_extinguishes_candle&#39;: 127, &#39;person_falls_from_bed&#39;: 128, &#39;person_falls_from_chair&#39;: 129, &#39;person_falls_into_pool&#39;: 130, &#39;person_falls_while_standing&#39;: 131, &#39;person_feeds_cat&#39;: 132, &#39;person_feeds_dog&#39;: 133, &#39;person_files_fingernails&#39;: 134, &#39;person_flips_down_car_wipers&#39;: 135, &#39;person_flips_up_car_wipers&#39;: 136, &#39;person_flosses&#39;: 137, &#39;person_folds_blanket&#39;: 138, &#39;person_folds_pants&#39;: 139, &#39;person_folds_shirt&#39;: 140, &#39;person_folds_socks&#39;: 141, &#39;person_folds_tablecloth&#39;: 142, &#39;person_folds_towel&#39;: 143, &#39;person_gestures_arms_x&#39;: 144, &#39;person_gestures_be_quiet&#39;: 145, &#39;person_gestures_behind_me&#39;: 146, &#39;person_gestures_blow_kiss&#39;: 147, &#39;person_gestures_call_me&#39;: 148, &#39;person_gestures_come_here&#39;: 149, &#39;person_gestures_cut&#39;: 150, &#39;person_gestures_hang_loose&#39;: 151, &#39;person_gestures_heart&#39;: 152, &#39;person_gestures_listen_closely&#39;: 153, &#39;person_gestures_lower_hand&#39;: 154, &#39;person_gestures_number_five&#39;: 155, &#39;person_gestures_number_four&#39;: 156, &#39;person_gestures_number_one&#39;: 157, &#39;person_gestures_number_three&#39;: 158, &#39;person_gestures_peace&#39;: 159, &#39;person_gestures_raise_hand&#39;: 160, &#39;person_gestures_stop&#39;: 161, &#39;person_gestures_swipe_down&#39;: 162, &#39;person_gestures_swipe_left&#39;: 163, &#39;person_gestures_swipe_right&#39;: 164, &#39;person_gestures_swipe_up&#39;: 165, &#39;person_gestures_thumbs_down&#39;: 166, &#39;person_gestures_thumbs_up&#39;: 167, &#39;person_gestures_watch_closely&#39;: 168, &#39;person_gestures_zoom_in&#39;: 169, &#39;person_gestures_zoom_out&#39;: 170, &#39;person_grabs_person_by_bicep&#39;: 171, &#39;person_grabs_person_by_forearm&#39;: 172, &#39;person_grabs_person_by_hair&#39;: 173, &#39;person_grabs_person_by_shoulder&#39;: 174, &#39;person_handstand&#39;: 175, &#39;person_hikes_up_pants&#39;: 176, &#39;person_hits_person_with_pillow&#39;: 177, &#39;person_holds_hand&#39;: 178, &#39;person_holds_object_above_head&#39;: 179, &#39;person_hugs_cat&#39;: 180, &#39;person_hugs_dog&#39;: 181, &#39;person_hugs_person_from_behind&#39;: 182, &#39;person_hugs_stuffed_animal&#39;: 183, &#39;person_inserts_trashbag_into_trashcan&#39;: 184, &#39;person_interacts_with_handheld_game&#39;: 185, &#39;person_interacts_with_laptop&#39;: 186, &#39;person_interacts_with_tablet&#39;: 187, &#39;person_irons_clothes&#39;: 188, &#39;person_jumps&#39;: 189, &#39;person_jumps_into_pool&#39;: 190, &#39;person_jumps_on_bed&#39;: 191, &#39;person_jumps_on_couch&#39;: 192, &#39;person_karate_chop&#39;: 193, &#39;person_karate_kicks&#39;: 194, &#39;person_kicks_car_tires&#39;: 195, &#39;person_kicks_object_to_person&#39;: 196, &#39;person_kisses_cheek_of_person&#39;: 197, &#39;person_kneels&#39;: 198, &#39;person_knocks_over_furniture&#39;: 199, &#39;person_knocks_over_glass&#39;: 200, &#39;person_laughs_with_person&#39;: 201, &#39;person_leaves_scene_through_structure&#39;: 202, &#39;person_licks_fingers&#39;: 203, &#39;person_lies_down_on_bed&#39;: 204, &#39;person_lies_down_on_couch&#39;: 205, &#39;person_lies_down_on_floor&#39;: 206, &#39;person_lifts_dummbells&#39;: 207, &#39;person_lights_candle&#39;: 208, &#39;person_loads_car&#39;: 209, &#39;person_loads_clothes_into_suitcase&#39;: 210, &#39;person_loads_clotheswasher&#39;: 211, &#39;person_loads_dishwasher&#39;: 212, &#39;person_loads_dryer&#39;: 213, &#39;person_loads_groceries_into_cabinet&#39;: 214, &#39;person_loads_groceries_into_refrigerator&#39;: 215, &#39;person_locks_door_with_keys&#39;: 216, &#39;person_looks_at_hands_in_lap&#39;: 217, &#39;person_looks_at_wristwatch&#39;: 218, &#39;person_looks_over_shoulder&#39;: 219, &#39;person_makes_bed&#39;: 220, &#39;person_mops&#39;: 221, &#39;person_nods_head&#39;: 222, &#39;person_nudges_person_with_elbow&#39;: 223, &#39;person_opens_beverage_can&#39;: 224, &#39;person_opens_box&#39;: 225, &#39;person_opens_cabinet&#39;: 226, &#39;person_opens_can_with_can_opener&#39;: 227, &#39;person_opens_car_door&#39;: 228, &#39;person_opens_car_hood&#39;: 229, &#39;person_opens_car_trunk&#39;: 230, &#39;person_opens_closet_door&#39;: 231, &#39;person_opens_clothes_washer&#39;: 232, &#39;person_opens_curtains&#39;: 233, &#39;person_opens_dishwasher&#39;: 234, &#39;person_opens_dresser_drawer&#39;: 235, &#39;person_opens_facility_door&#39;: 236, &#39;person_opens_gate&#39;: 237, &#39;person_opens_home_window&#39;: 238, &#39;person_opens_jar&#39;: 239, &#39;person_opens_jewelry_box&#39;: 240, &#39;person_opens_kitchen_drawer&#39;: 241, &#39;person_opens_laptop&#39;: 242, &#39;person_opens_mailbox&#39;: 243, &#39;person_opens_microwave&#39;: 244, &#39;person_opens_oven_door&#39;: 245, &#39;person_opens_refrigerator&#39;: 246, &#39;person_opens_sliding_door&#39;: 247, &#39;person_opens_suitcase&#39;: 248, &#39;person_paints_fingernails&#39;: 249, &#39;person_pats_head&#39;: 250, &#39;person_pets_cat&#39;: 251, &#39;person_pets_dog&#39;: 252, &#39;person_picks_up_object&#39;: 253, &#39;person_picks_up_object_from_bed&#39;: 254, &#39;person_picks_up_object_from_cabinet&#39;: 255, &#39;person_picks_up_object_from_couch&#39;: 256, &#39;person_picks_up_object_from_countertop&#39;: 257, &#39;person_picks_up_object_from_floor&#39;: 258, &#39;person_picks_up_object_from_shelf&#39;: 259, &#39;person_picks_up_object_from_table&#39;: 260, &#39;person_picks_up_person&#39;: 261, &#39;person_plugs_into_electrical_socket&#39;: 262, &#39;person_points_at_person&#39;: 263, &#39;person_points_to_dog&#39;: 264, &#39;person_polishes_car_with_rag&#39;: 265, &#39;person_pounds_shoulders_of_person&#39;: 266, &#39;person_pours_coffee_into_mug&#39;: 267, &#39;person_pours_into_bowl&#39;: 268, &#39;person_pours_liquid_into_cup&#39;: 269, &#39;person_prays&#39;: 270, &#39;person_pulls_out_chair&#39;: 271, &#39;person_pulls_wheeled_cart&#39;: 272, &#39;person_pulls_wheeled_trashcan&#39;: 273, &#39;person_punches&#39;: 274, &#39;person_purchases_from_cashier&#39;: 275, &#39;person_purchases_from_machine&#39;: 276, &#39;person_pushes_in_chair&#39;: 277, &#39;person_pushes_wheeled_cart&#39;: 278, &#39;person_pushes_wheeled_trashcan&#39;: 279, &#39;person_puts_clothes_into_closet&#39;: 280, &#39;person_puts_clothes_into_dresser&#39;: 281, &#39;person_puts_down_object&#39;: 282, &#39;person_puts_down_object_into_cabinet&#39;: 283, &#39;person_puts_down_object_on_bed&#39;: 284, &#39;person_puts_down_object_on_couch&#39;: 285, &#39;person_puts_down_object_on_countertop&#39;: 286, &#39;person_puts_down_object_on_floor&#39;: 287, &#39;person_puts_down_object_on_shelf&#39;: 288, &#39;person_puts_down_object_on_table&#39;: 289, &#39;person_puts_down_person&#39;: 290, &#39;person_puts_feet_up&#39;: 291, &#39;person_puts_fingers_in_ear&#39;: 292, &#39;person_puts_hair_in_ponytail&#39;: 293, &#39;person_puts_hands_in_back_pockets&#39;: 294, &#39;person_puts_object_in_cabinet&#39;: 295, &#39;person_puts_object_into_backpack&#39;: 296, &#39;person_puts_object_into_bag&#39;: 297, &#39;person_puts_object_into_basket&#39;: 298, &#39;person_puts_object_into_box&#39;: 299, &#39;person_puts_object_into_kitchen_drawer&#39;: 300, &#39;person_puts_object_into_microwave&#39;: 301, &#39;person_puts_object_into_oven&#39;: 302, &#39;person_puts_object_into_purse&#39;: 303, &#39;person_puts_object_into_refrigerator&#39;: 304, &#39;person_puts_object_into_toaster&#39;: 305, &#39;person_puts_on_apron&#39;: 306, &#39;person_puts_on_backpack&#39;: 307, &#39;person_puts_on_belt&#39;: 308, &#39;person_puts_on_boots&#39;: 309, &#39;person_puts_on_earrings&#39;: 310, &#39;person_puts_on_facemask&#39;: 311, &#39;person_puts_on_glasses&#39;: 312, &#39;person_puts_on_gloves&#39;: 313, &#39;person_puts_on_hat&#39;: 314, &#39;person_puts_on_headphones&#39;: 315, &#39;person_puts_on_hoodie&#39;: 316, &#39;person_puts_on_jacket&#39;: 317, &#39;person_puts_on_necklace&#39;: 318, &#39;person_puts_on_necktie&#39;: 319, &#39;person_puts_on_pants&#39;: 320, &#39;person_puts_on_ring&#39;: 321, &#39;person_puts_on_scarf&#39;: 322, &#39;person_puts_on_shirt&#39;: 323, &#39;person_puts_on_shoes&#39;: 324, &#39;person_puts_on_socks&#39;: 325, &#39;person_puts_on_sunglasses&#39;: 326, &#39;person_puts_on_wristwatch&#39;: 327, &#39;person_puts_phone_into_pocket&#39;: 328, &#39;person_puts_up_picture_frame&#39;: 329, &#39;person_puts_up_smoke_detector&#39;: 330, &#39;person_reads_book&#39;: 331, &#39;person_reads_document&#39;: 332, &#39;person_reads_magazine&#39;: 333, &#39;person_reads_newspaper&#39;: 334, &#39;person_removes_trashbag_from_trashcan&#39;: 335, &#39;person_rides_bicycle&#39;: 336, &#39;person_rubs_eyes&#39;: 337, &#39;person_rubs_foot&#39;: 338, &#39;person_rubs_neck&#39;: 339, &#39;person_salutes&#39;: 340, &#39;person_scratches_face&#39;: 341, &#39;person_screws_lid_to_bottle&#39;: 342, &#39;person_searches_in_backback&#39;: 343, &#39;person_searches_in_bag&#39;: 344, &#39;person_searches_in_box&#39;: 345, &#39;person_searches_in_cabinet&#39;: 346, &#39;person_searches_in_couch&#39;: 347, &#39;person_searches_in_kitchen_drawer&#39;: 348, &#39;person_searches_in_purse&#39;: 349, &#39;person_searches_jewelry_box&#39;: 350, &#39;person_searches_under_bed&#39;: 351, &#39;person_searches_under_couch&#39;: 352, &#39;person_sets_table&#39;: 353, &#39;person_sets_upright_furniture&#39;: 354, &#39;person_sets_upright_glass&#39;: 355, &#39;person_shades_eyes&#39;: 356, &#39;person_shakes_hand&#39;: 357, &#39;person_shakes_head&#39;: 358, &#39;person_shaves_face&#39;: 359, &#39;person_shoves_person&#39;: 360, &#39;person_shrugs&#39;: 361, &#39;person_sits_crisscross&#39;: 362, &#39;person_sits_down&#39;: 363, &#39;person_slaps_hands_on_thighs&#39;: 364, &#39;person_snaps_fingers&#39;: 365, &#39;person_sneezees into arm&#39;: 366, &#39;person_sneezes_into_hand&#39;: 367, &#39;person_somersaults&#39;: 368, &#39;person_spills_on_floor&#39;: 369, &#39;person_spills_on_table&#39;: 370, &#39;person_spins_person_around&#39;: 371, &#39;person_sprays_from_bottle&#39;: 372, &#39;person_spreads_tablecloth&#39;: 373, &#39;person_squats&#39;: 374, &#39;person_squeezes_object&#39;: 375, &#39;person_stands_up&#39;: 376, &#39;person_stands_up_from_floor&#39;: 377, &#39;person_steals_object&#39;: 378, &#39;person_sticks_out_tongue&#39;: 379, &#39;person_stirs_mug&#39;: 380, &#39;person_stretches_arms_over_head&#39;: 381, &#39;person_stretches_arms_to_side&#39;: 382, &#39;person_stretches_back&#39;: 383, &#39;person_strokes_chin&#39;: 384, &#39;person_strokes_hair&#39;: 385, &#39;person_stubs_toe&#39;: 386, &#39;person_swats_bug&#39;: 387, &#39;person_sweeps_floor&#39;: 388, &#39;person_takes_clothes_from_closet&#39;: 389, &#39;person_takes_clothes_from_dresser&#39;: 390, &#39;person_takes_down_picture_frame&#39;: 391, &#39;person_takes_down_smoke_detector&#39;: 392, &#39;person_takes_medicine_pills&#39;: 393, &#39;person_takes_object_from_backpack&#39;: 394, &#39;person_takes_object_from_bag&#39;: 395, &#39;person_takes_object_from_basket&#39;: 396, &#39;person_takes_object_from_box&#39;: 397, &#39;person_takes_object_from_cabinet&#39;: 398, &#39;person_takes_object_from_kitchen_drawer&#39;: 399, &#39;person_takes_object_from_microwave&#39;: 400, &#39;person_takes_object_from_oven&#39;: 401, &#39;person_takes_object_from_purse&#39;: 402, &#39;person_takes_object_from_refrigerator&#39;: 403, &#39;person_takes_object_from_toaster&#39;: 404, &#39;person_takes_off_apron&#39;: 405, &#39;person_takes_off_backpack&#39;: 406, &#39;person_takes_off_belt&#39;: 407, &#39;person_takes_off_boots&#39;: 408, &#39;person_takes_off_earrings&#39;: 409, &#39;person_takes_off_facemask&#39;: 410, &#39;person_takes_off_glasses&#39;: 411, &#39;person_takes_off_gloves&#39;: 412, &#39;person_takes_off_hairtie&#39;: 413, &#39;person_takes_off_hat&#39;: 414, &#39;person_takes_off_headphones&#39;: 415, &#39;person_takes_off_hoodie&#39;: 416, &#39;person_takes_off_jacket&#39;: 417, &#39;person_takes_off_necklace&#39;: 418, &#39;person_takes_off_necktie&#39;: 419, &#39;person_takes_off_pants&#39;: 420, &#39;person_takes_off_ring&#39;: 421, &#39;person_takes_off_scarf&#39;: 422, &#39;person_takes_off_shirt&#39;: 423, &#39;person_takes_off_shoes&#39;: 424, &#39;person_takes_off_socks&#39;: 425, &#39;person_takes_off_sunglasses&#39;: 426, &#39;person_takes_off_wristwatch&#39;: 427, &#39;person_takes_phone_from_pocket&#39;: 428, &#39;person_takes_selfie&#39;: 429, &#39;person_takes_selfie_with_person&#39;: 430, &#39;person_talks_on_phone&#39;: 431, &#39;person_talks_to_person&#39;: 432, &#39;person_taps_object_with_finger&#39;: 433, &#39;person_tears_paper&#39;: 434, &#39;person_texts_on_phone&#39;: 435, &#39;person_texts_on_phone_while_sitting&#39;: 436, &#39;person_throws_object_into_air&#39;: 437, &#39;person_throws_object_on_bed&#39;: 438, &#39;person_throws_object_on_table&#39;: 439, &#39;person_throws_object_to_dog&#39;: 440, &#39;person_throws_object_to_ground&#39;: 441, &#39;person_throws_object_to_person&#39;: 442, &#39;person_tickles_person&#39;: 443, &#39;person_ties_jacket_around_waist&#39;: 444, &#39;person_touches_back_of_person&#39;: 445, &#39;person_touches_earlobe&#39;: 446, &#39;person_touches_face_of_person&#39;: 447, &#39;person_transfers_object_to_car&#39;: 448, &#39;person_transfers_object_to_person&#39;: 449, &#39;person_trips_on_object_on_floor&#39;: 450, &#39;person_trips_on_stair&#39;: 451, &#39;person_tucks_in_shirt&#39;: 452, &#39;person_turns_off_fan&#39;: 453, &#39;person_turns_off_faucet&#39;: 454, &#39;person_turns_off_lamp&#39;: 455, &#39;person_turns_off_stovetop&#39;: 456, &#39;person_turns_on_fan&#39;: 457, &#39;person_turns_on_faucet&#39;: 458, &#39;person_turns_on_lamp&#39;: 459, &#39;person_turns_on_stovetop&#39;: 460, &#39;person_twirls&#39;: 461, &#39;person_unattaches_leash_from_dog&#39;: 462, &#39;person_unbuttons_shirt&#39;: 463, &#39;person_uncovers_friend_with_blanket&#39;: 464, &#39;person_uncovers_with_bedsheets&#39;: 465, &#39;person_uncovers_with_blanket&#39;: 466, &#39;person_uncrates_dog&#39;: 467, &#39;person_unfolds_blanket&#39;: 468, &#39;person_unloads_box_onto_floor&#39;: 469, &#39;person_unloads_box_onto_table&#39;: 470, &#39;person_unloads_car&#39;: 471, &#39;person_unloads_clothes_from_suitcase&#39;: 472, &#39;person_unloads_clotheswasher&#39;: 473, &#39;person_unloads_dishwasher&#39;: 474, &#39;person_unloads_dryer&#39;: 475, &#39;person_unlocks_door_with_keys&#39;: 476, &#39;person_unplugs_from_electrical_socket&#39;: 477, &#39;person_unscrews_lid_from_bottle&#39;: 478, &#39;person_unties_jacket_around_waist&#39;: 479, &#39;person_untucks_shirt&#39;: 480, &#39;person_unzips_jacket&#39;: 481, &#39;person_uses_bodyweight_scale&#39;: 482, &#39;person_uses_television_remote&#39;: 483, &#39;person_vacuums_carpet&#39;: 484, &#39;person_vapes&#39;: 485, &#39;person_walks&#39;: 486, &#39;person_walks_around_car&#39;: 487, &#39;person_walks_tiptoe&#39;: 488, &#39;person_washes_dish&#39;: 489, &#39;person_washes_face&#39;: 490, &#39;person_washes_hands&#39;: 491, &#39;person_washes_window&#39;: 492, &#39;person_waters_houseplant&#39;: 493, &#39;person_waves_at_person&#39;: 494, &#39;person_waves_hand_over_object&#39;: 495, &#39;person_wiggles_hips&#39;: 496, &#39;person_wipes_mouth_with_napkin&#39;: 497, &#39;person_wraps_box&#39;: 498, &#39;person_yawns&#39;: 499, &#39;person_zips_up_jacket&#39;: 500}

        
        self._verb_to_noun = {k:set([&#39;car&#39;,&#39;vehicle&#39;,&#39;motorcycle&#39;,&#39;bus&#39;,&#39;truck&#39;]) if (k.startswith(&#39;car&#39;) or k.startswith(&#39;motorcycle&#39;) or k.startswith(&#39;vehicle&#39;)) else set([&#39;person&#39;]) for k in self.classlist()}

        # Generated using vipy.dataset.Dataset.class_to_shortlabel()        
        self._class_to_shortlabel =  {&#39;person_exits_scene_through_structure&#39;: &#39;Leaves scene&#39;, &#39;person_stands_up&#39;: &#39;Stands&#39;, &#39;person_talks_on_phone&#39;: &#39;Talks on phone&#39;, &#39;person_talks_to_person&#39;: &#39;Talks to person&#39;, &#39;person_enters_scene_through_structure&#39;: &#39;Comes into scene&#39;, &#39;person_closes_car_door&#39;: &#39;Unloads&#39;, &#39;person_picks_up_object&#39;: &#39;Picks up&#39;, &#39;person_enters_car&#39;: &#39;Opens&#39;, &#39;person_opens_facility_door&#39;: &#39;Opens door&#39;, &#39;person_sits_down&#39;: &#39;Sits down&#39;, &#39;person_opens_car_trunk&#39;: &#39;Closing&#39;, &#39;person_exits_car&#39;: &#39;Opens&#39;, &#39;person_texts_on_phone&#39;: &#39;Texts on phone&#39;, &#39;person_closes_facility_door&#39;: &#39;Closes door&#39;, &#39;person_opens_car_door&#39;: &#39;Opens&#39;, &#39;person_puts_down_object&#39;: &#39;Puts down&#39;, &#39;hand_interacts_with_person&#39;: &#39;Hold hands&#39;, &#39;person_closes_car_trunk&#39;: &#39;Unloading&#39;, &#39;person_purchases_from_cashier&#39;: &#39;Purchases (cashier)&#39;, &#39;person_carries_heavy_object&#39;: &#39;Carries heavy object&#39;, &#39;person_abandons_package&#39;: &#39;Abandons&#39;, &#39;person_reads_document&#39;: &#39;Reads&#39;, &#39;person_rides_bicycle&#39;: &#39;Rides&#39;, &#39;person_transfers_object_to_person&#39;: &#39;Transfers&#39;, &#39;person_embraces_person&#39;: &#39;Embraces person&#39;, &#39;person_unloads_car&#39;: &#39;Unloads&#39;, &#39;person_loads_car&#39;: &#39;Loads&#39;, &#39;person_interacts_with_laptop&#39;: &#39;Interacts with laptop&#39;, &#39;car_turns_right&#39;: &#39;Turns right&#39;, &#39;car_turns_left&#39;: &#39;Turns left&#39;, &#39;car_stops&#39;: &#39;Stops&#39;, &#39;car_reverses&#39;: &#39;Reverses&#39;, &#39;car_starts&#39;: &#39;Starts&#39;, &#39;car_drops_off_person&#39;: &#39;Stops&#39;, &#39;car_picks_up_person&#39;: &#39;Stops&#39;, &#39;person_steals_object&#39;: &#39;Steals&#39;, &#39;person_walks&#39;: &#39;Walks&#39;, &#39;person_purchases_from_machine&#39;: &#39;Purchases (machine)&#39;, &#39;person_transfers_object_to_car&#39;: &#39;Transferring&#39;, &#39;car_moves&#39;: &#39;moves&#39;, &#39;person_scratches_face&#39;: &#39;Scratches&#39;, &#39;person_drops_object&#39;: &#39;Drops object&#39;, &#39;person_picks_up_object_from_floor&#39;: &#39;Picks up object from floor&#39;, &#39;person_interacts_with_handheld_game&#39;: &#39;Uses&#39;, &#39;person_picks_up_object_from_countertop&#39;: &#39;Picks up object from countertop&#39;, &#39;person_puts_down_object_on_countertop&#39;: &#39;Puts down object on countertop&#39;, &#39;person_gestures_raise_hand&#39;: &#39;Gestures raise hand&#39;, &#39;person_gestures_lower_hand&#39;: &#39;Gestures lower hand&#39;, &#39;person_unlocks_door_with_keys&#39;: &#39;Unlocks door&#39;, &#39;person_locks_door_with_keys&#39;: &#39;Locks door&#39;, &#39;person_gestures_come_here&#39;: &#39;Come here sign&#39;, &#39;person_gestures_peace&#39;: &#39;Peace sign&#39;, &#39;person_nudges_person_with_elbow&#39;: &#39;Nudges with elbow&#39;, &#39;person_touches_back_of_person&#39;: &#39;Touches back of person&#39;, &#39;person_touches_face_of_person&#39;: &#39;Touches face of person&#39;, &#39;person_picks_up_object_from_bed&#39;: &#39;Picks up object from bed&#39;, &#39;person_puts_down_object_on_bed&#39;: &#39;Puts down object on bed&#39;, &#39;person_gestures_thumbs_down&#39;: &#39;Gestures thumbs down&#39;, &#39;person_waves_hand_over_object&#39;: &#39;Waves over&#39;, &#39;person_opens_laptop&#39;: &#39;Opens laptop&#39;, &#39;person_closes_laptop&#39;: &#39;Closes laptop&#39;, &#39;person_bumps_into_wall&#39;: &#39;Bumps into&#39;, &#39;person_waves_at_person&#39;: &#39;Waves at person&#39;, &#39;person_opens_car_hood&#39;: &#39;Opens&#39;, &#39;person_closes_car_hood&#39;: &#39;Closes&#39;, &#39;person_exercises_with_lunges&#39;: &#39;Exercises with lunges&#39;, &#39;person_reads_newspaper&#39;: &#39;Reads&#39;, &#39;person_points_at_person&#39;: &#39;Points to&#39;, &#39;person_gestures_thumbs_up&#39;: &#39;Gestures thumbs up&#39;, &#39;person_squeezes_object&#39;: &#39;Squeezes&#39;, &#39;person_cracks_knuckles&#39;: &#39;Cracks knuckles&#39;, &#39;person_closes_door_with_foot&#39;: &#39;Closes&#39;, &#39;person_gestures_swipe_left&#39;: &#39;Gestures swipe left&#39;, &#39;person_gestures_swipe_right&#39;: &#39;Gestures swipe right&#39;, &#39;person_opens_gate&#39;: &#39;Opens&#39;, &#39;person_closes_gate&#39;: &#39;Closes&#39;, &#39;person_trips_on_object_on_floor&#39;: &#39;Trips&#39;, &#39;person_stands_up_from_floor&#39;: &#39;Stands up from floor&#39;, &#39;person_carries_laundry_basket&#39;: &#39;Carries laundry basket&#39;, &#39;person_squats&#39;: &#39;Squats&#39;, &#39;person_picks_up_object_from_couch&#39;: &#39;Picks up object from couch&#39;, &#39;person_puts_down_object_on_couch&#39;: &#39;Puts down object on couch&#39;, &#39;person_picks_up_object_from_cabinet&#39;: &#39;Picks up&#39;, &#39;person_puts_down_object_into_cabinet&#39;: &#39;Puts down&#39;, &#39;person_exercises_with_jumping_jacks&#39;: &#39;Exercises with jumping jacks&#39;, &#39;person_reads_magazine&#39;: &#39;Reads&#39;, &#39;person_closes_door_with_hip&#39;: &#39;Closes&#39;, &#39;person_carries_groceries&#39;: &#39;Carries groceries&#39;, &#39;person_looks_at_hands_in_lap&#39;: &#39;Looks at&#39;, &#39;person_interacts_with_tablet&#39;: &#39;Uses&#39;, &#39;person_polishes_car_with_rag&#39;: &#39;Polishes&#39;, &#39;person_kisses_cheek_of_person&#39;: &#39;Kisses cheek of person&#39;, &#39;person_exercises_with_situp&#39;: &#39;Exercises with situp&#39;, &#39;person_carries_furniture&#39;: &#39;Carries&#39;, &#39;person_carries_bicycle&#39;: &#39;Carries&#39;, &#39;person_exercises_with_pushup&#39;: &#39;Exercises with pushup&#39;, &#39;person_bows&#39;: &#39;Bows&#39;, &#39;person_sits_crisscross&#39;: &#39;Sits&#39;, &#39;person_reads_book&#39;: &#39;Reads book&#39;, &#39;person_gestures_swipe_up&#39;: &#39;Gestures swipe up&#39;, &#39;person_gestures_swipe_down&#39;: &#39;Gestures swipe down&#39;, &#39;person_looks_at_wristwatch&#39;: &#39;Looks at wristwatch&#39;, &#39;person_kicks_car_tires&#39;: &#39;Kicks&#39;, &#39;person_taps_object_with_finger&#39;: &#39;Taps&#39;, &#39;person_pounds_shoulders_of_person&#39;: &#39;Pounds shoulders of person&#39;, &#39;person_pushes_wheeled_cart&#39;: &#39;Pushes&#39;, &#39;person_pulls_wheeled_cart&#39;: &#39;Pulls&#39;, &#39;person_flips_up_car_wipers&#39;: &#39;Flips up&#39;, &#39;person_flips_down_car_wipers&#39;: &#39;Flips down&#39;, &#39;person_walks_around_car&#39;: &#39;Walks around&#39;, &#39;person_opens_sliding_door&#39;: &#39;Opens&#39;, &#39;person_closes_sliding_door&#39;: &#39;Closes&#39;, &#39;person_drinks_from_bottle&#39;: &#39;Drinks from bottle&#39;, &#39;person_shoves_person&#39;: &#39;Shoves&#39;, &#39;person_jumps&#39;: &#39;Jumps&#39;, &#39;person_eats_snack_from_bag&#39;: &#39;Eats snack from bag&#39;, &#39;person_takes_off_glasses&#39;: &#39;Takes off glasses&#39;, &#39;person_puts_on_glasses&#39;: &#39;Puts on glasses&#39;, &#39;person_lies_down_on_couch&#39;: &#39;Lies down&#39;, &#39;person_lies_down_on_bed&#39;: &#39;Lies down on bed&#39;, &#39;person_puts_on_hat&#39;: &#39;Puts on hat&#39;, &#39;person_takes_off_hat&#39;: &#39;Takes off hat&#39;, &#39;person_puts_on_shoes&#39;: &#39;Puts on shoes&#39;, &#39;person_takes_off_shoes&#39;: &#39;Takes off shoes&#39;, &#39;person_laughs_with_person&#39;: &#39;Laughs&#39;, &#39;person_lies_down_on_floor&#39;: &#39;Lies down&#39;, &#39;person_puts_on_scarf&#39;: &#39;Puts on scarf&#39;, &#39;person_takes_off_scarf&#39;: &#39;Takes off scarf&#39;, &#39;person_takes_off_headphones&#39;: &#39;Takes off headphones&#39;, &#39;person_puts_on_headphones&#39;: &#39;Puts on headphones&#39;, &#39;person_takes_phone_from_pocket&#39;: &#39;Takes phone from pocket&#39;, &#39;person_puts_phone_into_pocket&#39;: &#39;Puts phone in&#39;, &#39;person_puts_on_gloves&#39;: &#39;Puts on gloves&#39;, &#39;person_takes_off_gloves&#39;: &#39;Takes off gloves&#39;, &#39;person_wipes_mouth_with_napkin&#39;: &#39;Wipes mouth with napkin&#39;, &#39;person_drinks_from_straw&#39;: &#39;Drinks&#39;, &#39;person_grabs_person_by_forearm&#39;: &#39;Grabs&#39;, &#39;person_puts_on_jacket&#39;: &#39;Puts on jacket&#39;, &#39;person_zips_up_jacket&#39;: &#39;Zips up jacket&#39;, &#39;person_unzips_jacket&#39;: &#39;Unzips jacket&#39;, &#39;person_takes_off_jacket&#39;: &#39;Takes off jacket&#39;, &#39;person_grabs_person_by_bicep&#39;: &#39;Grabs&#39;, &#39;person_grabs_person_by_shoulder&#39;: &#39;Grabs&#39;, &#39;person_spins_person_around&#39;: &#39;Spins around&#39;, &#39;person_grabs_person_by_hair&#39;: &#39;Grabs&#39;, &#39;person_carries_person_over_shoulder&#39;: &#39;Carries&#39;, &#39;person_puts_down_person&#39;: &#39;Puts down&#39;, &#39;person_picks_up_person&#39;: &#39;Picks up&#39;, &#39;person_searches_in_backback&#39;: &#39;Searches in backback&#39;, &#39;person_takes_object_from_backpack&#39;: &#39;Takes object from backpack&#39;, &#39;person_puts_object_into_backpack&#39;: &#39;Puts object into backpack&#39;, &#39;person_searches_in_purse&#39;: &#39;Searches&#39;, &#39;person_puts_object_into_purse&#39;: &#39;Puts object into purse&#39;, &#39;person_takes_object_from_purse&#39;: &#39;Takes from&#39;, &#39;person_takes_off_facemask&#39;: &#39;Takes off facemask&#39;, &#39;person_puts_on_facemask&#39;: &#39;Puts on facemask&#39;, &#39;person_sweeps_floor&#39;: &#39;Sweeps&#39;, &#39;person_vacuums_carpet&#39;: &#39;Vacuums carpet&#39;, &#39;person_tucks_in_shirt&#39;: &#39;Tucks in shirt&#39;, &#39;person_untucks_shirt&#39;: &#39;Untucks&#39;, &#39;person_applies_deodorant&#39;: &#39;Uses deodorant&#39;, &#39;person_claps_hands&#39;: &#39;Claps&#39;, &#39;person_puts_feet_up&#39;: &#39;Puts feet up&#39;, &#39;person_climbs_up_stairs&#39;: &#39;Climbs up stairs&#39;, &#39;person_climbs_down_stairs&#39;: &#39;Climbs down stairs&#39;, &#39;person_kneels&#39;: &#39;Kneels&#39;, &#39;person_jumps_into_pool&#39;: &#39;Jumps into pool&#39;, &#39;person_exits_pool&#39;: &#39;Exits pool&#39;, &#39;person_puts_on_belt&#39;: &#39;Puts on belt&#39;, &#39;person_takes_off_belt&#39;: &#39;Takes off&#39;, &#39;person_picks_up_object_from_table&#39;: &#39;Picks up object from table&#39;, &#39;person_drinks_from_shotglass&#39;: &#39;Drinks from shotglass&#39;, &#39;person_puts_down_object_on_table&#39;: &#39;Puts down object on table&#39;, &#39;person_karate_kicks&#39;: &#39;Karate kicks&#39;, &#39;person_karate_chop&#39;: &#39;Karate chop&#39;, &#39;person_hikes_up_pants&#39;: &#39;Adjusts pants&#39;, &#39;person_opens_curtains&#39;: &#39;Opens curtains&#39;, &#39;person_closes_curtains&#39;: &#39;Closes curtains&#39;, &#39;person_puts_on_necklace&#39;: &#39;Puts on necklace&#39;, &#39;person_takes_off_necklace&#39;: &#39;Takes off&#39;, &#39;person_slaps_hands_on_thighs&#39;: &#39;Slaps&#39;, &#39;person_flosses&#39;: &#39;Flosses&#39;, &#39;person_plugs_into_electrical_socket&#39;: &#39;Plugs&#39;, &#39;person_unplugs_from_electrical_socket&#39;: &#39;Unplugs&#39;, &#39;person_brushes_teeth&#39;: &#39;Brushes&#39;, &#39;person_puts_on_socks&#39;: &#39;Puts on socks&#39;, &#39;person_takes_off_socks&#39;: &#39;Takes off socks&#39;, &#39;person_puts_on_sunglasses&#39;: &#39;Puts on sunglasses&#39;, &#39;person_takes_off_sunglasses&#39;: &#39;Takes off&#39;, &#39;person_punches&#39;: &#39;Punches&#39;, &#39;person_brushes_hair&#39;: &#39;Brushes hair&#39;, &#39;person_crawls&#39;: &#39;Crawls&#39;, &#39;person_applies_sunscreen&#39;: &#39;Uses sunscreen&#39;, &#39;person_drinks_from_mug&#39;: &#39;Drinks from mug&#39;, &#39;person_stretches_back&#39;: &#39;Stretches back&#39;, &#39;person_stretches_arms_to_side&#39;: &#39;Stretches arms to side&#39;, &#39;person_stretches_arms_over_head&#39;: &#39;Stretches arms over head&#39;, &#39;person_unscrews_lid_from_bottle&#39;: &#39;Unscrews lid from bottle&#39;, &#39;person_pours_liquid_into_cup&#39;: &#39;Pours liquid into cup&#39;, &#39;person_screws_lid_to_bottle&#39;: &#39;Screws lid to bottle&#39;, &#39;person_ties_jacket_around_waist&#39;: &#39;Ties&#39;, &#39;person_unties_jacket_around_waist&#39;: &#39;Unties&#39;, &#39;person_crosses_arms&#39;: &#39;Crosses arms&#39;, &#39;person_opens_home_window&#39;: &#39;Opens home window&#39;, &#39;person_closes_home_window&#39;: &#39;Closes home window&#39;, &#39;person_puts_on_wristwatch&#39;: &#39;Puts on wristwatch&#39;, &#39;person_takes_off_wristwatch&#39;: &#39;Takes off&#39;, &#39;person_turns_off_fan&#39;: &#39;Turns off fan&#39;, &#39;person_turns_on_fan&#39;: &#39;Turns on fan&#39;, &#39;person_puts_on_ring&#39;: &#39;Puts on ring&#39;, &#39;person_takes_off_ring&#39;: &#39;Takes off&#39;, &#39;person_opens_closet_door&#39;: &#39;Opens&#39;, &#39;person_closes_closet_door&#39;: &#39;Closes&#39;, &#39;person_pulls_wheeled_trashcan&#39;: &#39;Pulls trashcan&#39;, &#39;person_pushes_wheeled_trashcan&#39;: &#39;Pushes trashcan&#39;, &#39;person_drums_on_chest&#39;: &#39;Drums&#39;, &#39;person_eats_with_hands&#39;: &#39;Eats with hands&#39;, &#39;person_snaps_fingers&#39;: &#39;Snaps&#39;, &#39;person_folds_socks&#39;: &#39;Folds socks&#39;, &#39;person_dries_dish&#39;: &#39;Dries dish&#39;, &#39;person_puts_on_earrings&#39;: &#39;Puts on earrings&#39;, &#39;person_takes_off_earrings&#39;: &#39;Takes off&#39;, &#39;person_folds_pants&#39;: &#39;Folds pants&#39;, &#39;person_sprays_from_bottle&#39;: &#39;Sprays from bottle&#39;, &#39;person_opens_microwave&#39;: &#39;Opens microwave&#39;, &#39;person_puts_object_into_microwave&#39;: &#39;Puts object into microwave&#39;, &#39;person_closes_microwave&#39;: &#39;Closes microwave&#39;, &#39;person_takes_object_from_microwave&#39;: &#39;Takes object from microwave&#39;, &#39;person_unloads_dishwasher&#39;: &#39;Unloads dishwasher&#39;, &#39;person_washes_hands&#39;: &#39;Washes hands&#39;, &#39;person_dries_hands_with_towel&#39;: &#39;Dries hands with towel&#39;, &#39;person_washes_dish&#39;: &#39;Washes dish&#39;, &#39;person_applies_lip_makeup&#39;: &#39;Applies&#39;, &#39;person_opens_refrigerator&#39;: &#39;Opens refrigerator&#39;, &#39;person_takes_object_from_refrigerator&#39;: &#39;Takes object from refrigerator&#39;, &#39;person_closes_refrigerator&#39;: &#39;Closes refrigerator&#39;, &#39;person_puts_object_into_refrigerator&#39;: &#39;Puts object into refrigerator&#39;, &#39;person_eats_with_utensil&#39;: &#39;Eats with utensil&#39;, &#39;person_opens_cabinet&#39;: &#39;Opens cabinet&#39;, &#39;person_closes_cabinet&#39;: &#39;Closes cabinet&#39;, &#39;person_licks_fingers&#39;: &#39;Licks&#39;, &#39;person_applies_facial_moisturizer&#39;: &#39;Uses moisturizer&#39;, &#39;person_unloads_clotheswasher&#39;: &#39;Unloads&#39;, &#39;person_loads_dryer&#39;: &#39;Loads&#39;, &#39;person_takes_clothes_from_dresser&#39;: &#39;Takes from&#39;, &#39;person_puts_clothes_into_dresser&#39;: &#39;Puts into&#39;, &#39;person_removes_trashbag_from_trashcan&#39;: &#39;Removes trashbag from trashcan&#39;, &#39;person_inserts_trashbag_into_trashcan&#39;: &#39;Inserts trashbag into trashcan&#39;, &#39;person_turns_on_lamp&#39;: &#39;Turns on&#39;, &#39;person_turns_off_lamp&#39;: &#39;Turns off&#39;, &#39;person_throws_object_to_person&#39;: &#39;Throws object to person&#39;, &#39;person_catches_object_from_person&#39;: &#39;Catches&#39;, &#39;person_waters_houseplant&#39;: &#39;Waters&#39;, &#39;person_applies_foundation_makeup&#39;: &#39;Applies&#39;, &#39;person_folds_shirt&#39;: &#39;Folds shirt&#39;, &#39;person_takes_medicine_pills&#39;: &#39;Takes pills&#39;, &#39;person_drinks_from_cup&#39;: &#39;Drinks from cup&#39;, &#39;person_opens_mailbox&#39;: &#39;Opens&#39;, &#39;person_closes_mailbox&#39;: &#39;Closes mailbox&#39;, &#39;person_blows_into_hands&#39;: &#39;Blows&#39;, &#39;person_puts_fingers_in_ear&#39;: &#39;Put into ears&#39;, &#39;person_cleans_table_with_rag&#39;: &#39;Cleans table with rag&#39;, &#39;person_loads_dishwasher&#39;: &#39;Loads&#39;, &#39;person_discards_trash&#39;: &#39;Discards trash&#39;, &#39;person_lights_candle&#39;: &#39;Lights candle&#39;, &#39;person_extinguishes_candle&#39;: &#39;Extinguishes candle&#39;, &#39;person_falls_into_pool&#39;: &#39;Falls into pool&#39;, &#39;person_yawns&#39;: &#39;Yawns&#39;, &#39;person_opens_dresser_drawer&#39;: &#39;Opens&#39;, &#39;person_closes_dresser_drawer&#39;: &#39;Closes&#39;, &#39;person_pulls_out_chair&#39;: &#39;Pulls out chair&#39;, &#39;person_pushes_in_chair&#39;: &#39;Pushes in chair&#39;, &#39;person_kicks_object_to_person&#39;: &#39;Kicks object&#39;, &#39;person_puts_clothes_into_closet&#39;: &#39;Puts into&#39;, &#39;person_takes_clothes_from_closet&#39;: &#39;Takes clothes from closet&#39;, &#39;person_looks_over_shoulder&#39;: &#39;Looks over&#39;, &#39;person_pets_dog&#39;: &#39;Pets dog&#39;, &#39;person_nods_head&#39;: &#39;Nods head&#39;, &#39;person_shakes_head&#39;: &#39;Shakes head&#39;, &#39;person_applies_eye_makeup&#39;: &#39;Applies&#39;, &#39;person_loads_clotheswasher&#39;: &#39;Loads&#39;, &#39;person_unbuttons_shirt&#39;: &#39;Unbuttons&#39;, &#39;person_buttons_shirt&#39;: &#39;Buttons shirt&#39;, &#39;person_opens_jar&#39;: &#39;Opens&#39;, &#39;person_closes_jar&#39;: &#39;Closes&#39;, &#39;person_falls_from_bed&#39;: &#39;Falls from bed&#39;, &#39;person_falls_from_chair&#39;: &#39;Falls from chair&#39;, &#39;person_hugs_cat&#39;: &#39;Hugs cat&#39;, &#39;person_prays&#39;: &#39;Prays&#39;, &#39;person_takes_selfie&#39;: &#39;Takes selfie&#39;, &#39;person_puts_hair_in_ponytail&#39;: &#39;Puts hair in ponytail&#39;, &#39;person_takes_off_hairtie&#39;: &#39;Removes hairtie&#39;, &#39;person_hugs_dog&#39;: &#39;Hugs dog&#39;, &#39;person_pets_cat&#39;: &#39;Pets cat&#39;, &#39;person_picks_up_object_from_shelf&#39;: &#39;Picks up object from shelf&#39;, &#39;person_puts_down_object_on_shelf&#39;: &#39;Puts down object on shelf&#39;, &#39;person_shrugs&#39;: &#39;Shrugs&#39;, &#39;person_opens_can_with_can_opener&#39;: &#39;Opens&#39;, &#39;person_rubs_neck&#39;: &#39;Rubs&#39;, &#39;person_takes_selfie_with_person&#39;: &#39;Takes selfie&#39;, &#39;person_wiggles_hips&#39;: &#39;Wiggles&#39;, &#39;person_dances_in_place&#39;: &#39;Dances in place&#39;, &#39;person_trips_on_stair&#39;: &#39;Trips on stair&#39;, &#39;person_sneezees into arm&#39;: &#39;Sneezees into arm&#39;, &#39;person_throws_object_on_table&#39;: &#39;Throws&#39;, &#39;person_touches_earlobe&#39;: &#39;Touches ear&#39;, &#39;person_twirls&#39;: &#39;Twirls&#39;, &#39;person_unloads_dryer&#39;: &#39;Unloads&#39;, &#39;person_shades_eyes&#39;: &#39;Shades eyes&#39;, &#39;person_washes_face&#39;: &#39;Washes&#39;, &#39;person_dries_face_with_towel&#39;: &#39;Dries&#39;, &#39;person_throws_object_to_ground&#39;: &#39;Throws&#39;, &#39;person_tickles_person&#39;: &#39;Tickles&#39;, &#39;person_bumps_into_person&#39;: &#39;Bumps into person&#39;, &#39;person_throws_object_on_bed&#39;: &#39;Throws&#39;, &#39;person_rubs_eyes&#39;: &#39;Rubs&#39;, &#39;person_stubs_toe&#39;: &#39;Stubs toe&#39;, &#39;person_sneezes_into_hand&#39;: &#39;Sneezes into hand&#39;, &#39;person_vapes&#39;: &#39;Vapes&#39;, &#39;person_tears_paper&#39;: &#39;Tears paper&#39;, &#39;person_spills_on_table&#39;: &#39;Spills on table&#39;, &#39;person_sets_upright_glass&#39;: &#39;Sets upright glass&#39;, &#39;person_covers_friend_with_blanket&#39;: &#39;Covers friend with blanket&#39;, &#39;person_uncovers_friend_with_blanket&#39;: &#39;Uncovers friend with blanket&#39;, &#39;person_takes_off_boots&#39;: &#39;Takes off boots&#39;, &#39;person_puts_on_boots&#39;: &#39;Puts on boots&#39;, &#39;person_uses_television_remote&#39;: &#39;Uses television remote&#39;, &#39;person_sets_table&#39;: &#39;Sets table&#39;, &#39;person_puts_on_apron&#39;: &#39;Puts on apron&#39;, &#39;person_takes_off_apron&#39;: &#39;Takes off apron&#39;, &#39;person_turns_on_stovetop&#39;: &#39;Turns on stovetop&#39;, &#39;person_turns_off_stovetop&#39;: &#39;Turns off stovetop&#39;, &#39;person_spreads_tablecloth&#39;: &#39;Spreads tablecloth&#39;, &#39;person_folds_tablecloth&#39;: &#39;Folds tablecloth&#39;, &#39;person_folds_blanket&#39;: &#39;Folds blanket&#39;, &#39;person_unfolds_blanket&#39;: &#39;Unfolds blanket&#39;, &#39;person_covers_with_blanket&#39;: &#39;Covers with blanket&#39;, &#39;person_uncovers_with_blanket&#39;: &#39;Uncovers with blanket&#39;, &#39;person_crumples_paper&#39;: &#39;Crumples paper&#39;, &#39;person_sets_upright_furniture&#39;: &#39;Sets upright furniture&#39;, &#39;person_knocks_over_furniture&#39;: &#39;Knock over furniture&#39;, &#39;person_knocks_over_glass&#39;: &#39;Knocks over glass&#39;, &#39;person_loads_groceries_into_refrigerator&#39;: &#39;Loads groceries (refrigerator)&#39;, &#39;person_uncovers_with_bedsheets&#39;: &#39;Uncovers with bedsheets&#39;, &#39;person_covers_with_bedsheets&#39;: &#39;Covers with bedsheets&#39;, &#39;person_clips_fingernails&#39;: &#39;Clips fingernails&#39;, &#39;person_loads_groceries_into_cabinet&#39;: &#39;Loads groceries (cabinet)&#39;, &#39;person_files_fingernails&#39;: &#39;Files fingernails&#39;, &#39;person_loads_clothes_into_suitcase&#39;: &#39;Loads clothes into suitcase&#39;, &#39;person_mops&#39;: &#39;Mops&#39;, &#39;person_spills_on_floor&#39;: &#39;Spills on floor&#39;, &#39;person_washes_window&#39;: &#39;Washes window&#39;, &#39;person_unloads_clothes_from_suitcase&#39;: &#39;Unloads clothes from suitcase&#39;, &#39;person_puts_on_backpack&#39;: &#39;Puts on backpack&#39;, &#39;person_takes_off_backpack&#39;: &#39;Takes off backpack&#39;, &#39;person_opens_jewelry_box&#39;: &#39;Opens jewelry box&#39;, &#39;person_searches_jewelry_box&#39;: &#39;Searches jewelry box&#39;, &#39;person_closes_jewelry_box&#39;: &#39;Closes jewelry box&#39;, &#39;person_lifts_dummbells&#39;: &#39;Lifts dumbbells&#39;, &#39;person_swats_bug&#39;: &#39;Swats bug&#39;, &#39;person_irons_clothes&#39;: &#39;Irons clothes&#39;, &#39;person_uses_bodyweight_scale&#39;: &#39;Uses scale&#39;, &#39;person_makes_bed&#39;: &#39;Makes bed&#39;, &#39;person_unloads_box_onto_floor&#39;: &#39;Unloads box (floor)&#39;, &#39;person_unloads_box_onto_table&#39;: &#39;Unloads box (table)&#39;, &#39;person_searches_in_cabinet&#39;: &#39;Searches (cabinet)&#39;, &#39;person_dusts_furniture&#39;: &#39;Dusts furniture&#39;, &#39;person_handstand&#39;: &#39;Handstands&#39;, &#39;person_opens_beverage_can&#39;: &#39;Opens beverage can&#39;, &#39;person_drinks_from_beverage_can&#39;: &#39;Drinks from beverage can&#39;, &#39;person_somersaults&#39;: &#39;Somersaults&#39;, &#39;person_jumps_on_couch&#39;: &#39;Jumps on couch&#39;, &#39;person_climbs_up_ladder&#39;: &#39;Climbs up ladder&#39;, &#39;person_climbs_down_ladder&#39;: &#39;Climbs down ladder&#39;, &#39;person_covers_face_with_hands&#39;: &#39;Covers face&#39;, &#39;person_climbs_on_table&#39;: &#39;Climbs on table&#39;, &#39;person_climbs_off_table&#39;: &#39;Climbs off table&#39;, &#39;person_arranges_flowers_in_vase&#39;: &#39;Arranges flowers&#39;, &#39;person_crosses_legs&#39;: &#39;Crosses legs&#39;, &#39;person_climbs_on_chair&#39;: &#39;Climbs on chair&#39;, &#39;person_climbs_off_chair&#39;: &#39;Climbs off chair&#39;, &#39;person_bumps_into_table&#39;: &#39;Bumps into&#39;, &#39;person_climbs_on_couch&#39;: &#39;Climbs on couch&#39;, &#39;person_climbs_off_couch&#39;: &#39;Climbs off couch&#39;, &#39;person_opens_box&#39;: &#39;Opens box&#39;, &#39;person_closes_box&#39;: &#39;Closes box&#39;, &#39;person_cleans_dryer_lint_trap&#39;: &#39;Cleans&#39;, &#39;person_strokes_hair&#39;: &#39;Strokes hair&#39;, &#39;person_puts_on_shirt&#39;: &#39;Puts on shirt&#39;, &#39;person_takes_off_shirt&#39;: &#39;Takes off&#39;, &#39;person_applies_shaving_cream&#39;: &#39;Applies&#39;, &#39;person_shaves_face&#39;: &#39;Shaves&#39;, &#39;person_jumps_on_bed&#39;: &#39;Jumps on bed&#39;, &#39;person_puts_hands_in_back_pockets&#39;: &#39;Puts hands in&#39;, &#39;person_searches_in_bag&#39;: &#39;Searches in bag&#39;, &#39;person_folds_towel&#39;: &#39;Folds towel&#39;, &#39;person_puts_on_pants&#39;: &#39;Puts on pants&#39;, &#39;person_takes_object_from_toaster&#39;: &#39;Takes from&#39;, &#39;person_puts_object_into_toaster&#39;: &#39;Puts into&#39;, &#39;person_searches_under_couch&#39;: &#39;Searches&#39;, &#39;person_puts_up_picture_frame&#39;: &#39;Puts up picture frame&#39;, &#39;person_takes_down_picture_frame&#39;: &#39;Takes down picture frame&#39;, &#39;person_strokes_chin&#39;: &#39;Strokes chin&#39;, &#39;person_hugs_stuffed_animal&#39;: &#39;Hugs stuffy&#39;, &#39;person_crawls_out_from_under_vehicle&#39;: &#39;Crawls&#39;, &#39;person_puts_on_hoodie&#39;: &#39;Puts on hoodie&#39;, &#39;person_takes_off_hoodie&#39;: &#39;Takes off hoodie&#39;, &#39;person_searches_in_box&#39;: &#39;Searches in box&#39;, &#39;person_adjusts_thermostat&#39;: &#39;Adjusts&#39;, &#39;person_stirs_mug&#39;: &#39;Stirs&#39;, &#39;person_opens_oven_door&#39;: &#39;Opens oven door&#39;, &#39;person_closes_oven_door&#39;: &#39;Closes oven door&#39;, &#39;person_burns_hand&#39;: &#39;Burns hand&#39;, &#39;person_searches_in_couch&#39;: &#39;Searches couch&#39;, &#39;person_salutes&#39;: &#39;Salutes&#39;, &#39;person_crates_dog&#39;: &#39;Crates dog&#39;, &#39;person_uncrates_dog&#39;: &#39;Uncrates dog&#39;, &#39;person_bounces_ball_on_floor&#39;: &#39;Bounces ball on floor&#39;, &#39;person_takes_off_pants&#39;: &#39;Takes off pants&#39;, &#39;person_puts_object_into_oven&#39;: &#39;Puts object into oven&#39;, &#39;person_takes_object_from_oven&#39;: &#39;Takes object from oven&#39;, &#39;person_searches_under_bed&#39;: &#39;Searches&#39;, &#39;person_attaches_leash_to_dog&#39;: &#39;Attaches leash to dog&#39;, &#39;person_unattaches_leash_from_dog&#39;: &#39;Unattaches leash&#39;, &#39;person_takes_down_smoke_detector&#39;: &#39;Takes down&#39;, &#39;person_puts_up_smoke_detector&#39;: &#39;Puts up&#39;, &#39;person_holds_object_above_head&#39;: &#39;Holds object up&#39;, &#39;person_rubs_foot&#39;: &#39;Rubs foot&#39;, &#39;person_gestures_listen_closely&#39;: &#39;Gestures listen closely&#39;, &#39;person_gestures_blow_kiss&#39;: &#39;Gestures blow kiss&#39;, &#39;person_dries_hair_with_hairdryer&#39;: &#39;Blow dries hair&#39;, &#39;person_gestures_cut&#39;: &#39;Gestures cut&#39;, &#39;person_gestures_watch_closely&#39;: &#39;Gestures watch closely&#39;, &#39;person_gestures_be_quiet&#39;: &#39;Gestures be quiet&#39;, &#39;person_blows_nose&#39;: &#39;Blows nose&#39;, &#39;person_sticks_out_tongue&#39;: &#39;Sticks out tongue&#39;, &#39;person_dries_hair_with_towel&#39;: &#39;Towel dries hair&#39;, &#39;person_gestures_arms_x&#39;: &#39;Gestures arms X&#39;, &#39;person_gestures_behind_me&#39;: &#39;Gestures behind me&#39;, &#39;person_pats_head&#39;: &#39;Pats head&#39;, &#39;person_puts_on_necktie&#39;: &#39;Puts on necktie&#39;, &#39;person_takes_off_necktie&#39;: &#39;Takes off necktie&#39;, &#39;person_paints_fingernails&#39;: &#39;Paints fingernails&#39;, &#39;person_wraps_box&#39;: &#39;Wraps box&#39;, &#39;person_carries_person_on_shoulders&#39;: &#39;Carries person on shoulders&#39;, &#39;person_braids_hair_of_person&#39;: &#39;Braids hair&#39;, &#39;person_carries_person_on_back&#39;: &#39;Carries person on back&#39;, &#39;person_gestures_hang_loose&#39;: &#39;Gestures hang loose&#39;, &#39;person_throws_object_into_air&#39;: &#39;Throws object into air&#39;, &#39;person_gestures_zoom_in&#39;: &#39;Gestures zoom in&#39;, &#39;person_gestures_zoom_out&#39;: &#39;Gestures zoom out&#39;, &#39;person_gestures_number_four&#39;: &#39;Gestures number four&#39;, &#39;person_brushes_hair_of_person&#39;: &#39;Brushes hair of person&#39;, &#39;person_gestures_heart&#39;: &#39;Gestures heart&#39;, &#39;person_gestures_number_one&#39;: &#39;Gestures number one&#39;, &#39;person_gestures_number_five&#39;: &#39;Gestures number five&#39;, &#39;person_gestures_number_three&#39;: &#39;Gestures number three&#39;, &#39;person_gestures_call_me&#39;: &#39;Gestures call me&#39;, &#39;person_gestures_stop&#39;: &#39;Gestures stop&#39;, &#39;person_walks_tiptoe&#39;: &#39;Walks tiptoe&#39;, &#39;person_pours_coffee_into_mug&#39;: &#39;Pours coffee&#39;, &#39;person_hugs_person_from_behind&#39;: &#39;Hugs from behind&#39;, &#39;person_opens_clothes_washer&#39;: &#39;Opens clothes washer&#39;, &#39;person_closes_clothes_washer&#39;: &#39;Closes clothes washer&#39;, &#39;person_pours_into_bowl&#39;: &#39;Pours into bowl&#39;, &#39;person_searches_in_kitchen_drawer&#39;: &#39;Searches in kitchen drawer&#39;, &#39;person_puts_down_object_on_floor&#39;: &#39;Puts down object on floor&#39;, &#39;person_falls_while_standing&#39;: &#39;Falls while standing&#39;, &#39;person_opens_suitcase&#39;: &#39;Opens suitcase&#39;, &#39;person_closes_suitcase&#39;: &#39;Closes suitcase&#39;, &#39;person_opens_kitchen_drawer&#39;: &#39;Opens kitchen drawer&#39;, &#39;person_closes_kitchen_drawer&#39;: &#39;Closes kitchen drawer&#39;, &#39;person_turns_off_faucet&#39;: &#39;Turns off faucet&#39;, &#39;person_takes_object_from_cabinet&#39;: &#39;Takes object from cabinet&#39;, &#39;person_takes_object_from_kitchen_drawer&#39;: &#39;Takes object from kitchen drawer&#39;, &#39;person_puts_object_into_kitchen_drawer&#39;: &#39;Puts object into kitchen drawer&#39;, &#39;person_turns_on_faucet&#39;: &#39;Turns on faucet&#39;, &#39;person_texts_on_phone_while_sitting&#39;: &#39;Texts on phone while sitting&#39;, &#39;person_exercises_with_plank&#39;: &#39;Exercises with plank&#39;, &#39;person_takes_object_from_bag&#39;: &#39;Takes object from bag&#39;, &#39;person_puts_object_in_cabinet&#39;: &#39;Puts object in cabinet&#39;, &#39;person_hits_person_with_pillow&#39;: &#39;Hits with pillow&#39;, &#39;person_takes_object_from_basket&#39;: &#39;Takes object from basket&#39;, &#39;person_puts_object_into_basket&#39;: &#39;Puts object into basket&#39;, &#39;person_catches_dropped_object&#39;: &#39;Catches dropped object&#39;, &#39;person_eats_apple&#39;: &#39;Eats apple&#39;, &#39;person_eats_banana&#39;: &#39;Eats banana&#39;, &#39;person_puts_object_into_bag&#39;: &#39;Puts object into bag&#39;, &#39;person_opens_dishwasher&#39;: &#39;Opens dishwasher&#39;, &#39;person_closes_dishwasher&#39;: &#39;Closes dishwasher&#39;, &#39;person_leaves_scene_through_structure&#39;: &#39;Leaves scene&#39;, &#39;person_feeds_cat&#39;: &#39;Feeds cat&#39;, &#39;person_points_to_dog&#39;: &#39;Points to dog&#39;, &#39;person_throws_object_to_dog&#39;: &#39;Throws object to dog&#39;, &#39;person_feeds_dog&#39;: &#39;Feeds dog&#39;, &#39;person_embraces_sitting_person&#39;: &#39;Embraces sitting person&#39;, &#39;person_holds_hand&#39;: &#39;Holds hand&#39;, &#39;person_shakes_hand&#39;: &#39;Shakes hand&#39;, &#39;person_takes_object_from_box&#39;: &#39;Takes object from box&#39;, &#39;person_puts_object_into_box&#39;: &#39;Puts object into box&#39;, &#39;person_cleans_eyeglasses&#39;: &#39;Cleans eyeglasses&#39;}

        if modelfile is not None:
            self._load_trained(modelfile)
        else:
            self._load_pretrained()
            self.net.fc = nn.Linear(self.net.fc.in_features, self.num_classes())
        

    #---- &lt;LIGHTNING&gt;
    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x[&#39;val_loss&#39;] for x in outputs]).mean()
        self.log(&#39;val_loss&#39;, avg_loss, on_epoch=True, prog_bar=False, logger=True)
        self.log(&#39;avg_val_loss&#39;, avg_loss, on_epoch=True, prog_bar=True, logger=True)                
        #return {&#39;val_loss&#39;: avg_loss, &#39;avg_val_loss&#39;: avg_loss}   # as of 9.1, this does not return anything
    #---- &lt;/LIGHTNING&gt;
        
    def totensor(self, v=None, training=False, validation=False, show=False, doflip=False, asjson=False):
        &#34;&#34;&#34;Return captured lambda function if v=None, else return tensor&#34;&#34;&#34;    
        assert v is None or isinstance(v, vipy.video.Scene), &#34;Invalid input&#34;
        f = (lambda v, num_frames=self._num_frames, input_size=self._input_size, mean=self._mean, std=self._std, training=training, validation=validation, show=show, classname=self.__class__.__name__:
             PIP_370k._totensor(v, training, validation, input_size, num_frames, mean, std, noflip=[&#39;car_turns_left&#39;, &#39;car_turns_right&#39;, &#39;vehicle_turns_left&#39;, &#39;vehicle_turns_right&#39;, &#39;motorcycle_turns_left&#39;, &#39;motorcycle_turns_right&#39;], show=show, doflip=doflip, asjson=asjson, classname=classname))
        return f(v) if v is not None else f</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="heyvi.recognition.PIP_370k" href="#heyvi.recognition.PIP_370k">PIP_370k</a></li>
<li><a title="heyvi.recognition.PIP_250k" href="#heyvi.recognition.PIP_250k">PIP_250k</a></li>
<li>pytorch_lightning.core.lightning.LightningModule</li>
<li>abc.ABC</li>
<li>pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.grads.GradInformation</li>
<li>pytorch_lightning.core.saving.ModelIO</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
<li><a title="heyvi.recognition.ActivityRecognition" href="#heyvi.recognition.ActivityRecognition">ActivityRecognition</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="heyvi.recognition.ActivityTrackerCap" href="#heyvi.recognition.ActivityTrackerCap">ActivityTrackerCap</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="heyvi.recognition.CAP.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="heyvi.recognition.CAP.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="heyvi.recognition.PIP_370k" href="#heyvi.recognition.PIP_370k">PIP_370k</a></b></code>:
<ul class="hlist">
<li><code><a title="heyvi.recognition.PIP_370k.configure_optimizers" href="#heyvi.recognition.PIP_250k.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="heyvi.recognition.PIP_370k.forward" href="#heyvi.recognition.PIP_250k.forward">forward</a></code></li>
<li><code><a title="heyvi.recognition.PIP_370k.topk" href="#heyvi.recognition.PIP_370k.topk">topk</a></code></li>
<li><code><a title="heyvi.recognition.PIP_370k.totensor" href="#heyvi.recognition.PIP_250k.totensor">totensor</a></code></li>
<li><code><a title="heyvi.recognition.PIP_370k.training_step" href="#heyvi.recognition.PIP_250k.training_step">training_step</a></code></li>
<li><code><a title="heyvi.recognition.PIP_370k.validation_epoch_end" href="#heyvi.recognition.PIP_250k.validation_epoch_end">validation_epoch_end</a></code></li>
<li><code><a title="heyvi.recognition.PIP_370k.validation_step" href="#heyvi.recognition.PIP_250k.validation_step">validation_step</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="heyvi.recognition.PIP_250k"><code class="flex name class">
<span>class <span class="ident">PIP_250k</span></span>
<span>(</span><span>pretrained=True, deterministic=False, modelfile=None, mlbl=False, mlfl=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Activity recognition using people in public - 250k stabilized</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L88-L282" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class PIP_250k(pl.LightningModule, ActivityRecognition):
    &#34;&#34;&#34;Activity recognition using people in public - 250k stabilized&#34;&#34;&#34;
    
    def __init__(self, pretrained=True, deterministic=False, modelfile=None, mlbl=False, mlfl=False):

        # FIXME: remove dependencies here
        from heyvi.model.pyvideoresearch.bases.resnet50_3d import ResNet503D, ResNet3D, Bottleneck3D
        import heyvi.model.ResNets_3D_PyTorch.resnet

        
        super().__init__()
        self._input_size = 112
        self._num_frames = 16        
        self._mean = [0.485, 0.456, 0.406]
        self._std = [0.229, 0.224, 0.225]
        self._mlfl = mlfl
        self._mlbl = mlbl

        if deterministic:
            np.random.seed(42)

        self._class_to_weight = {&#39;car_drops_off_person&#39;: 1.4162811344926518, &#39;car_picks_up_person&#39;: 1.4103618337303332, &#39;car_reverses&#39;: 1.0847976470131024, &#39;car_starts&#39;: 1.0145749063037774, &#39;car_stops&#39;: 0.6659236295324015, &#39;car_turns_left&#39;: 2.942269221156227, &#39;car_turns_right&#39;: 1.1077783089040996, &#39;hand_interacts_with_person_highfive&#39;: 2.793646013249904, &#39;person&#39;: 0.4492053391155403, &#39;person_abandons_object&#39;: 1.0944029463871692, &#39;person_carries_heavy_object&#39;: 0.5848339202761978, &#39;person_closes_car_door&#39;: 0.8616907697519004, &#39;person_closes_car_trunk&#39;: 1.468393359799126, &#39;person_closes_facility_door&#39;: 0.8927495923340439, &#39;person_embraces_person&#39;: 0.6072654081071569, &#39;person_enters_car&#39;: 1.3259274145537951, &#39;person_enters_scene_through_structure&#39;: 0.6928103470838287, &#39;person_exits_car&#39;: 1.6366577285051707, &#39;person_exits_scene_through_structure&#39;: 0.8368692178634396, &#39;person_holds_hand&#39;: 1.2378881634203558, &#39;person_interacts_with_laptop&#39;: 1.6276031281396193, &#39;person_loads_car&#39;: 2.170167410167583, &#39;person_opens_car_door&#39;: 0.7601817241565009, &#39;person_opens_car_trunk&#39;: 1.7255285914206204, &#39;person_opens_facility_door&#39;: 0.9167411017455822, &#39;person_picks_up_object_from_floor&#39;: 1.123251610875369, &#39;person_picks_up_object_from_table&#39;: 3.5979689180114205, &#39;person_purchases_from_cashier&#39;: 7.144918373837205, &#39;person_purchases_from_machine&#39;: 5.920886403645001, &#39;person_puts_down_object_on_floor&#39;: 0.7295795950752353, &#39;person_puts_down_object_on_shelf&#39;: 9.247614426653692, &#39;person_puts_down_object_on_table&#39;: 1.9884672074906158, &#39;person_reads_document&#39;: 0.7940480628992879, &#39;person_rides_bicycle&#39;: 2.662661823600623, &#39;person_shakes_hand&#39;: 0.7819547332927879, &#39;person_sits_down&#39;: 0.8375202893491961, &#39;person_stands_up&#39;: 1.0285510019795079, &#39;person_steals_object_from_person&#39;: 1.0673909796893626, &#39;person_talks_on_phone&#39;: 0.3031855242664589, &#39;person_talks_to_person&#39;: 0.334895684562076, &#39;person_texts_on_phone&#39;: 0.713951043919232, &#39;person_transfers_object_to_car&#39;: 3.2832615561297605, &#39;person_transfers_object_to_person&#39;: 0.9633429807282274, &#39;person_unloads_car&#39;: 1.1051597100801462, &#39;vehicle&#39;: 1.1953172363332243}
        self._class_to_weight[&#39;person_puts_down_object_on_shelf&#39;] = 1.0   # run 5

        self._class_to_index = {&#39;car_drops_off_person&#39;: 0, &#39;car_picks_up_person&#39;: 1, &#39;car_reverses&#39;: 2, &#39;car_starts&#39;: 3, &#39;car_stops&#39;: 4, &#39;car_turns_left&#39;: 5, &#39;car_turns_right&#39;: 6, &#39;hand_interacts_with_person_highfive&#39;: 7, &#39;person&#39;: 8, &#39;person_abandons_object&#39;: 9, &#39;person_carries_heavy_object&#39;: 10, &#39;person_closes_car_door&#39;: 11, &#39;person_closes_car_trunk&#39;: 12, &#39;person_closes_facility_door&#39;: 13, &#39;person_embraces_person&#39;: 14, &#39;person_enters_car&#39;: 15, &#39;person_enters_scene_through_structure&#39;: 16, &#39;person_exits_car&#39;: 17, &#39;person_exits_scene_through_structure&#39;: 18, &#39;person_holds_hand&#39;: 19, &#39;person_interacts_with_laptop&#39;: 20, &#39;person_loads_car&#39;: 21, &#39;person_opens_car_door&#39;: 22, &#39;person_opens_car_trunk&#39;: 23, &#39;person_opens_facility_door&#39;: 24, &#39;person_picks_up_object_from_floor&#39;: 25, &#39;person_picks_up_object_from_table&#39;: 26, &#39;person_purchases_from_cashier&#39;: 27, &#39;person_purchases_from_machine&#39;: 28, &#39;person_puts_down_object_on_floor&#39;: 29, &#39;person_puts_down_object_on_shelf&#39;: 30, &#39;person_puts_down_object_on_table&#39;: 31, &#39;person_reads_document&#39;: 32, &#39;person_rides_bicycle&#39;: 33, &#39;person_shakes_hand&#39;: 34, &#39;person_sits_down&#39;: 35, &#39;person_stands_up&#39;: 36, &#39;person_steals_object_from_person&#39;: 37, &#39;person_talks_on_phone&#39;: 38, &#39;person_talks_to_person&#39;: 39, &#39;person_texts_on_phone&#39;: 40, &#39;person_transfers_object_to_car&#39;: 41, &#39;person_transfers_object_to_person&#39;: 42, &#39;person_unloads_car&#39;: 43, &#39;vehicle&#39;: 44}

        self._verb_to_noun = {k:set([&#39;car&#39;,&#39;vehicle&#39;,&#39;motorcycle&#39;,&#39;bus&#39;,&#39;truck&#39;]) if (k.startswith(&#39;car&#39;) or k.startswith(&#39;motorcycle&#39;) or k.startswith(&#39;vehicle&#39;)) else set([&#39;person&#39;]) for k in self.classlist()}        
        self._class_to_shortlabel = pycollector.label.pip_to_shortlabel  # FIXME: remove dependency here

        if pretrained:
            self._load_pretrained()
            self.net.fc = nn.Linear(self.net.fc.in_features, self.num_classes())
        elif modelfile is not None:
            self._load_trained(modelfile)
        
    def category(self, x):
        yh = self.forward(x if x.ndim == 5 else torch.unsqueeze(x, 0))
        return [self.index_to_class(int(k)) for (c,k) in zip(*torch.max(yh, dim=1))]

    def category_confidence(self, x):
        yh = self.forward(x if x.ndim == 5 else torch.unsqueeze(x, 0))
        return [(self.index_to_class(int(k)), float(c)) for (c,k) in zip(*torch.max(yh, dim=1))]

    def topk(self, x_logits, k):
        yh = x_logits.detach().cpu().numpy()
        topk = [[(self.index_to_class(j), s[j]) for j in i[-k:][::-1]] for (s,i) in zip(yh, np.argsort(yh, axis=1))]
        return topk

    def topk_probability(self, x_logits, k):
        yh = x_logits.detach().cpu().numpy()
        yh_prob = F.softmax(x_logits, dim=1).detach().cpu().numpy()
        topk = [[(self.index_to_class(j), c[j], p[j]) for j in i[-k:][::-1]] for (c,p,i) in zip(yh, yh_prob, np.argsort(yh, axis=1))]
        return topk
        
    # ---- &lt;LIGHTNING&gt;
    def forward(self, x):
        return self.net(x)  # lighting handles device

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)
        return optimizer

    def training_step(self, batch, batch_nb, logging=True, valstep=False):
        (x,Y) = batch  
        y_hat = self.forward(x)
        y_hat_softmax = F.softmax(y_hat, dim=1)

        (loss, n_valid) = (0, 0)
        C = torch.tensor([self._class_to_weight[k] for (k,v) in sorted(self._class_to_index.items(), key=lambda x: x[1])], device=y_hat.device)  # inverse class frequency        
        for (yh, yhs, labelstr) in zip(y_hat, y_hat_softmax, Y):
            labels = json.loads(labelstr)
            if labels is None:
                continue  # skip me
            lbllist = [l for lbl in labels for l in lbl]  # list of multi-labels within clip (unpack from JSON to use default collate_fn)
            lbl_frequency = vipy.util.countby(lbllist, lambda x: x)  # frequency within clip
            lbl_weight = {k:v/float(len(lbllist)) for (k,v) in lbl_frequency.items()}  # multi-label likelihood within clip, sums to one
            for (y,w) in lbl_weight.items():
                if valstep:
                    # Pick all labels normalized (https://papers.nips.cc/paper/2019/file/da647c549dde572c2c5edc4f5bef039c-Paper.pdf
                    loss += float(w)*F.cross_entropy(torch.unsqueeze(yh, dim=0), torch.tensor([self._class_to_index[y]], device=y_hat.device), weight=C)
                elif self._mlfl:
                    # Pick all labels normalized, with multi-label focal loss
                    loss += torch.min(torch.tensor(1.0, device=y_hat.device), ((w-yhs[self._class_to_index[y]])/w)**2)*float(w)*F.cross_entropy(torch.unsqueeze(yh, dim=0), torch.tensor([self._class_to_index[y]], device=y_hat.device), weight=C)  
                elif self._mlbl:
                    # Pick all labels normalized with multi-label background loss
                    j_bg_person = self._class_to_index[&#39;person&#39;] if &#39;person&#39; in self._class_to_index else self._class_to_index[&#39;person_walks&#39;]  # FIXME: does not generalize
                    j_bg_vehicle = self._class_to_index[&#39;vehicle&#39;] if &#39;vehicle&#39; in self._class_to_index else self._class_to_index[&#39;car_moves&#39;]  # FIXME: does not generalize
                    j = j_bg_person if (y.startswith(&#39;person&#39;) or y.startswith(&#39;hand&#39;)) else j_bg_vehicle
                    loss += ((1-torch.sqrt(yhs[j]*yhs[self._class_to_index[y]]))**2)*float(w)*F.cross_entropy(torch.unsqueeze(yh, dim=0), torch.tensor([self._class_to_index[y]], device=y_hat.device), weight=C)  
                else:
                    # Pick all labels normalized (https://papers.nips.cc/paper/2019/file/da647c549dde572c2c5edc4f5bef039c-Paper.pdf
                    loss += float(w)*F.cross_entropy(torch.unsqueeze(yh, dim=0), torch.tensor([self._class_to_index[y]], device=y_hat.device), weight=C)

            n_valid += 1
        loss = loss / float(max(1, n_valid))  # batch reduction: mean

        if logging:
            self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return {&#39;loss&#39;: loss}

    def validation_step(self, batch, batch_nb):
        loss = self.training_step(batch, batch_nb, logging=False, valstep=True)[&#39;loss&#39;]
        self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=False, logger=True)
        return {&#39;val_loss&#39;: loss}

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x[&#39;val_loss&#39;] for x in outputs]).mean()
        self.log(&#39;val_loss&#39;, avg_loss, on_epoch=True, prog_bar=False, logger=True)
        self.log(&#39;avg_val_loss&#39;, avg_loss, on_epoch=True, prog_bar=True, logger=True)                
        return {&#39;val_loss&#39;: avg_loss, &#39;avg_val_loss&#39;: avg_loss}                         
    #---- &lt;/LIGHTNING&gt;
    
    @classmethod
    def from_checkpoint(cls, checkpointpath):
        return cls().load_from_checkpoint(checkpointpath)  # lightning
            
    def _load_trained(self, ckptfile):
        self.net = heyvi.model.ResNets_3D_PyTorch.resnet.generate_model(50, n_classes=self.num_classes())
        t = torch.split(self.net.conv1.weight.data, dim=1, split_size_or_sections=1)
        self.net.conv1.weight.data = torch.cat( (*t, t[-1]), dim=1).contiguous()
        self.net.conv1.in_channels = 4  # inflate RGB -&gt; RGBA
        self.load_state_dict(torch.load(ckptfile)[&#39;state_dict&#39;])  # FIXME
        self.eval()
        return self
        
    def _load_pretrained(self):

        pthfile = vipy.downloader.downloadif(&#39;https://dl.dropboxusercontent.com/s/t3xge6lrfqpklr0/r3d50_kms_200ep.pth&#39;,
                                                vipy.util.tocache(&#39;r3d50_KMS_200ep.pth&#39;),  # set VIPY_CACHE env 
                                                sha1=&#39;39ea626355308d8f75307cab047a8d75862c3261&#39;)
        
        net = heyvi.model.ResNets_3D_PyTorch.resnet.generate_model(50, n_classes=1139)
        pretrain = torch.load(pthfile, map_location=&#39;cpu&#39;)
        net.load_state_dict(pretrain[&#39;state_dict&#39;])

        # Inflate RGB -&gt; RGBA         
        t = torch.split(net.conv1.weight.data, dim=1, split_size_or_sections=1)
        net.conv1.weight.data = torch.cat( (*t, t[-1]), dim=1).contiguous()
        net.conv1.in_channels = 4

        self.net = net

        return self

    @staticmethod
    def _totensor(v, training, validation, input_size, num_frames, mean, std, noflip=None, show=False, doflip=False):
        assert isinstance(v, vipy.video.Scene), &#34;Invalid input&#34;
        
        try:
            v = v.download() if (not v.hasfilename() and v.hasurl()) else v  # fetch it if necessary, but do not do this during training!        
            if training or validation:
                random.seed()  # force randomness after fork() 
                (ai,aj) = (v.primary_activity().startframe(), v.primary_activity().endframe())  # activity (start,end)
                (ti,tj) = (v.actor().startframe(), v.actor().endframe())  # track (start,end) 
                startframe = random.randint(max(0, ti-(num_frames//2)), max(1, tj-(num_frames//2)))  # random startframe that contains track
                endframe = min((startframe+num_frames), aj)  # endframe truncated to be end of activity
                (startframe, endframe) = (startframe, endframe) if (startframe &lt; endframe) else (max(0, aj-num_frames), aj)  # fallback
                assert endframe - startframe &lt;= num_frames
                vc = v.clone().clip(startframe, endframe)    # may fail for some short clips
                vc = vc.trackcrop(dilate=1.2, maxsquare=True)  # may be None if clip contains no track
                vc = vc.resize(input_size, input_size)   
                vc = vc.fliplr() if (doflip or (random.random() &gt; 0.5)) and (noflip is None or vc.category() not in noflip) else vc
            else:
                vc = v.trackcrop(dilate=1.2, maxsquare=True)  # may be None if clip contains no track
                vc = vc.resize(input_size, input_size)  # TESTING: this may introduce a preview()
                vc = vc.fliplr() if doflip and (noflip is None or vc.category() not in noflip) else vc
                
            if show:
                vc.clone().resize(512,512).show(timestamp=True)
                vc.clone().binarymask().frame(0).rgb().show(figure=&#39;binary mask: frame 0&#39;)
                
            vc = vc.load(shape=(input_size, input_size, 3)).normalize(mean=mean, std=std, scale=1.0/255.0)  # [0,255] -&gt; [0,1], triggers load() with known shape
            (t,lbl) = vc.torch(startframe=0, length=num_frames, boundary=&#39;cyclic&#39;, order=&#39;cdhw&#39;, withlabel=training or validation, nonelabel=True)  # (c=3)x(d=num_frames)x(H=input_size)x(W=input_size), reuses vc._array
            t = torch.cat((t, vc.asfloatmask(fg=0.5, bg=-0.5).torch(startframe=0, length=num_frames, boundary=&#39;cyclic&#39;, order=&#39;cdhw&#39;)), dim=0)  # (c=4) x (d=num_frames) x (H=input_size) x (W=input_size), copy
            
        except Exception as e:
            if training or validation:
                #print(&#39;ERROR: %s&#39; % (str(v)))
                t = torch.zeros(4, num_frames, input_size, input_size)  # skip me
                lbl = None
            else:
                print(&#39;WARNING: discarding tensor for video &#34;%s&#34; with exception &#34;%s&#34;&#39; % (str(v), str(e)))
                t = torch.zeros(4, num_frames, input_size, input_size)  # skip me (should never get here)
            
        if training or validation:
            return (t, json.dumps(lbl))  # json to use default collate_fn
        else:
            return t

    def totensor(self, v=None, training=False, validation=False, show=False, doflip=False):
        &#34;&#34;&#34;Return captured lambda function if v=None, else return tensor&#34;&#34;&#34;    
        assert v is None or isinstance(v, vipy.video.Scene), &#34;Invalid input&#34;
        f = (lambda v, num_frames=self._num_frames, input_size=self._input_size, mean=self._mean, std=self._std, training=training, validation=validation, show=show:
             PIP_250k._totensor(v, training, validation, input_size, num_frames, mean, std, noflip=[&#39;car_turns_left&#39;, &#39;car_turns_right&#39;], show=show, doflip=doflip))
        return f(v) if v is not None else f</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.lightning.LightningModule</li>
<li>abc.ABC</li>
<li>pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.grads.GradInformation</li>
<li>pytorch_lightning.core.saving.ModelIO</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
<li><a title="heyvi.recognition.ActivityRecognition" href="#heyvi.recognition.ActivityRecognition">ActivityRecognition</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="heyvi.recognition.PIP_370k" href="#heyvi.recognition.PIP_370k">PIP_370k</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="heyvi.recognition.PIP_250k.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="heyvi.recognition.PIP_250k.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="heyvi.recognition.PIP_250k.from_checkpoint"><code class="name flex">
<span>def <span class="ident">from_checkpoint</span></span>(<span>checkpointpath)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L200-L202" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@classmethod
def from_checkpoint(cls, checkpointpath):
    return cls().load_from_checkpoint(checkpointpath)  # lightning</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="heyvi.recognition.PIP_250k.category"><code class="name flex">
<span>def <span class="ident">category</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L123-L125" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def category(self, x):
    yh = self.forward(x if x.ndim == 5 else torch.unsqueeze(x, 0))
    return [self.index_to_class(int(k)) for (c,k) in zip(*torch.max(yh, dim=1))]</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.PIP_250k.category_confidence"><code class="name flex">
<span>def <span class="ident">category_confidence</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L127-L129" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def category_confidence(self, x):
    yh = self.forward(x if x.ndim == 5 else torch.unsqueeze(x, 0))
    return [(self.index_to_class(int(k)), float(c)) for (c,k) in zip(*torch.max(yh, dim=1))]</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.PIP_250k.configure_optimizers"><code class="name flex">
<span>def <span class="ident">configure_optimizers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you'd need one. But in the case of GANs or similar you might have multiple.</p>
<h2 id="return">Return</h2>
<p>Any of these 6 options.</p>
<ul>
<li>Single optimizer.</li>
<li>List or Tuple - List of optimizers.</li>
<li>Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).</li>
<li>Dictionary, with an 'optimizer' key, and (optionally) a 'lr_scheduler'
key whose value is a single LR scheduler or lr_dict.</li>
<li>Tuple of dictionaries as described, with an optional 'frequency' key.</li>
<li>None - Fit will run without any optimizer.</li>
</ul>
<h2 id="note">Note</h2>
<p>The 'frequency' value is an int corresponding to the number of sequential batches
optimized with the specific optimizer. It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.</p>
<p>The lr_dict is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<p>.. code-block:: python</p>
<pre><code>{
    'scheduler': lr_scheduler, # The LR scheduler instance (required)
    'interval': 'epoch', # The unit of the scheduler's step size
    'frequency': 1, # The frequency of the scheduler
    'reduce_on_plateau': False, # For ReduceLROnPlateau scheduler
    'monitor': 'val_loss', # Metric for ReduceLROnPlateau to monitor
    'strict': True, # Whether to crash the training if &lt;code&gt;monitor&lt;/code&gt; is not found
    'name': None, # Custom name for LearningRateMonitor to use
}
</code></pre>
<p>Only the <code>scheduler</code> key is required, the rest will be set to the defaults above.</p>
<h2 id="examples">Examples</h2>
<p>.. code-block:: python</p>
<pre><code># most cases
def configure_optimizers(self):
    opt = Adam(self.parameters(), lr=1e-3)
    return opt

# multiple optimizer case (e.g.: GAN)
def configure_optimizers(self):
    generator_opt = Adam(self.model_gen.parameters(), lr=0.01)
    disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02)
    return generator_opt, disriminator_opt

# example with learning rate schedulers
def configure_optimizers(self):
    generator_opt = Adam(self.model_gen.parameters(), lr=0.01)
    disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02)
    discriminator_sched = CosineAnnealing(discriminator_opt, T_max=10)
    return [generator_opt, disriminator_opt], [discriminator_sched]

# example with step-based learning rate schedulers
def configure_optimizers(self):
    gen_opt = Adam(self.model_gen.parameters(), lr=0.01)
    dis_opt = Adam(self.model_disc.parameters(), lr=0.02)
    gen_sched = {'scheduler': ExponentialLR(gen_opt, 0.99),
                 'interval': 'step'}  # called after each training step
    dis_sched = CosineAnnealing(discriminator_opt, T_max=10) # called every epoch
    return [gen_opt, dis_opt], [gen_sched, dis_sched]

# example with optimizer frequencies
# see training procedure in &lt;code&gt;Improved Training of Wasserstein GANs&lt;/code&gt;, Algorithm 1
# &lt;https://arxiv.org/abs/1704.00028&gt;
def configure_optimizers(self):
    gen_opt = Adam(self.model_gen.parameters(), lr=0.01)
    dis_opt = Adam(self.model_disc.parameters(), lr=0.02)
    n_critic = 5
    return (
        {'optimizer': dis_opt, 'frequency': n_critic},
        {'optimizer': gen_opt, 'frequency': 1}
    )
</code></pre>
<h2 id="note_1">Note</h2>
<p>Some things to know:</p>
<ul>
<li>
<p>Lightning calls <code>.backward()</code> and <code>.step()</code> on each optimizer
and learning rate scheduler as needed.</p>
</li>
<li>
<p>If you use 16-bit precision (<code>precision=16</code>), Lightning will automatically
handle the optimizers for you.</p>
</li>
<li>
<p>If you use multiple optimizers, :meth:<code>training_step</code> will have an additional
<code>optimizer_idx</code> parameter.</p>
</li>
<li>
<p>If you use LBFGS Lightning handles the closure function automatically for you.</p>
</li>
<li>
<p>If you use multiple optimizers, gradients will be calculated only
for the parameters of current optimizer at each training step.</p>
</li>
<li>
<p>If you need to control how often those optimizers step or override the
default <code>.step()</code> schedule, override the :meth:<code>optimizer_step</code> hook.</p>
</li>
<li>
<p>If you only want to call a learning rate scheduler every <code>x</code> step or epoch,
or want to monitor a custom metric, you can specify these in a lr_dict:</p>
</li>
</ul>
<p>.. code-block:: python</p>
<pre><code>  {
      'scheduler': lr_scheduler,
      'interval': 'step',  # or 'epoch'
      'monitor': 'val_f1',
      'frequency': x,
  }
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L146-L148" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def configure_optimizers(self):
    optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)
    return optimizer</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.PIP_250k.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Same as :meth:<code>torch.nn.Module.forward()</code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you'd call <code>self()</code> from your :meth:<code>training_step</code> method.
This makes it easy to write a complex system for training with the outputs
you'd want in a prediction setting.</p>
<p>You may also find the :func:<code>~pytorch_lightning.core.decorators.auto_move_data</code> decorator useful
when using the module outside Lightning in a production setting.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>Whatever you decide to pass into the forward method.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments are also possible.</dd>
</dl>
<h2 id="return">Return</h2>
<p>Predicted output</p>
<h2 id="examples">Examples</h2>
<p>.. code-block:: python</p>
<pre><code># example if we were using this model as a feature extractor
def forward(self, x):
    feature_maps = self.convnet(x)
    return feature_maps

def training_step(self, batch, batch_idx):
    x, y = batch
    feature_maps = self(x)
    logits = self.classifier(feature_maps)

    # ...
    return loss

# splitting it this way allows model to be used a feature extractor
model = MyModelAbove()

inputs = server.get_request()
results = model(inputs)
server.write_results(results)

# -------------
# This is in stark contrast to torch.nn.Module where normally you would have this:
def forward(self, batch):
    x, y = batch
    feature_maps = self.convnet(x)
    logits = self.classifier(feature_maps)
    return logits
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L143-L144" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def forward(self, x):
    return self.net(x)  # lighting handles device</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.PIP_250k.topk"><code class="name flex">
<span>def <span class="ident">topk</span></span>(<span>self, x_logits, k)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L131-L134" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def topk(self, x_logits, k):
    yh = x_logits.detach().cpu().numpy()
    topk = [[(self.index_to_class(j), s[j]) for j in i[-k:][::-1]] for (s,i) in zip(yh, np.argsort(yh, axis=1))]
    return topk</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.PIP_250k.topk_probability"><code class="name flex">
<span>def <span class="ident">topk_probability</span></span>(<span>self, x_logits, k)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L136-L140" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def topk_probability(self, x_logits, k):
    yh = x_logits.detach().cpu().numpy()
    yh_prob = F.softmax(x_logits, dim=1).detach().cpu().numpy()
    topk = [[(self.index_to_class(j), c[j], p[j]) for j in i[-k:][::-1]] for (c,p,i) in zip(yh, yh_prob, np.argsort(yh, axis=1))]
    return topk</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.PIP_250k.totensor"><code class="name flex">
<span>def <span class="ident">totensor</span></span>(<span>self, v=None, training=False, validation=False, show=False, doflip=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Return captured lambda function if v=None, else return tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L277-L282" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def totensor(self, v=None, training=False, validation=False, show=False, doflip=False):
    &#34;&#34;&#34;Return captured lambda function if v=None, else return tensor&#34;&#34;&#34;    
    assert v is None or isinstance(v, vipy.video.Scene), &#34;Invalid input&#34;
    f = (lambda v, num_frames=self._num_frames, input_size=self._input_size, mean=self._mean, std=self._std, training=training, validation=validation, show=show:
         PIP_250k._totensor(v, training, validation, input_size, num_frames, mean, std, noflip=[&#39;car_turns_left&#39;, &#39;car_turns_right&#39;], show=show, doflip=doflip))
    return f(v) if v is not None else f</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.PIP_250k.training_step"><code class="name flex">
<span>def <span class="ident">training_step</span></span>(<span>self, batch, batch_nb, logging=True, valstep=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>
<h2 id="args">Args</h2>
<dl>
<dt>batch (:class:<code>~torch.Tensor</code> | (:class:<code>~torch.Tensor</code>, &hellip;) | [:class:<code>~torch.Tensor</code>, &hellip;]):</dt>
<dt>The output of your :class:<code>~torch.utils.data.DataLoader</code>. A tensor, tuple or list.</dt>
<dt><strong><code>batch_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>Integer displaying index of this batch</dd>
<dt><strong><code>optimizer_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>When using multiple optimizers, this argument will also be present.</dd>
</dl>
<p>hiddens(:class:<code>~torch.Tensor</code>): Passed in if
:paramref:<code>~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps</code> &gt; 0.</p>
<h2 id="return">Return</h2>
<p>Any of.</p>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key 'loss'</li>
<li><code>None</code> - Training will skip to the next batch</li>
</ul>
<p>In this step you'd normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example::</p>
<pre><code>def training_step(self, batch, batch_idx):
    x, y, z = batch
    out = self.encoder(x)
    loss = self.loss(out, x)
    return loss
</code></pre>
<p>If you define multiple optimizers, this step will be called with an additional
<code>optimizer_idx</code> parameter.</p>
<p>.. code-block:: python</p>
<pre><code># Multiple optimizers (e.g.: GANs)
def training_step(self, batch, batch_idx, optimizer_idx):
    if optimizer_idx == 0:
        # do training_step with encoder
    if optimizer_idx == 1:
        # do training_step with decoder
</code></pre>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<p>.. code-block:: python</p>
<pre><code># Truncated back-propagation through time
def training_step(self, batch, batch_idx, hiddens):
    # hiddens are the hidden states from the previous truncated backprop step
    ...
    out, hiddens = self.lstm(data, hiddens)
    ...
    return {'loss': loss, 'hiddens': hiddens}
</code></pre>
<h2 id="note">Note</h2>
<p>The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L150-L186" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def training_step(self, batch, batch_nb, logging=True, valstep=False):
    (x,Y) = batch  
    y_hat = self.forward(x)
    y_hat_softmax = F.softmax(y_hat, dim=1)

    (loss, n_valid) = (0, 0)
    C = torch.tensor([self._class_to_weight[k] for (k,v) in sorted(self._class_to_index.items(), key=lambda x: x[1])], device=y_hat.device)  # inverse class frequency        
    for (yh, yhs, labelstr) in zip(y_hat, y_hat_softmax, Y):
        labels = json.loads(labelstr)
        if labels is None:
            continue  # skip me
        lbllist = [l for lbl in labels for l in lbl]  # list of multi-labels within clip (unpack from JSON to use default collate_fn)
        lbl_frequency = vipy.util.countby(lbllist, lambda x: x)  # frequency within clip
        lbl_weight = {k:v/float(len(lbllist)) for (k,v) in lbl_frequency.items()}  # multi-label likelihood within clip, sums to one
        for (y,w) in lbl_weight.items():
            if valstep:
                # Pick all labels normalized (https://papers.nips.cc/paper/2019/file/da647c549dde572c2c5edc4f5bef039c-Paper.pdf
                loss += float(w)*F.cross_entropy(torch.unsqueeze(yh, dim=0), torch.tensor([self._class_to_index[y]], device=y_hat.device), weight=C)
            elif self._mlfl:
                # Pick all labels normalized, with multi-label focal loss
                loss += torch.min(torch.tensor(1.0, device=y_hat.device), ((w-yhs[self._class_to_index[y]])/w)**2)*float(w)*F.cross_entropy(torch.unsqueeze(yh, dim=0), torch.tensor([self._class_to_index[y]], device=y_hat.device), weight=C)  
            elif self._mlbl:
                # Pick all labels normalized with multi-label background loss
                j_bg_person = self._class_to_index[&#39;person&#39;] if &#39;person&#39; in self._class_to_index else self._class_to_index[&#39;person_walks&#39;]  # FIXME: does not generalize
                j_bg_vehicle = self._class_to_index[&#39;vehicle&#39;] if &#39;vehicle&#39; in self._class_to_index else self._class_to_index[&#39;car_moves&#39;]  # FIXME: does not generalize
                j = j_bg_person if (y.startswith(&#39;person&#39;) or y.startswith(&#39;hand&#39;)) else j_bg_vehicle
                loss += ((1-torch.sqrt(yhs[j]*yhs[self._class_to_index[y]]))**2)*float(w)*F.cross_entropy(torch.unsqueeze(yh, dim=0), torch.tensor([self._class_to_index[y]], device=y_hat.device), weight=C)  
            else:
                # Pick all labels normalized (https://papers.nips.cc/paper/2019/file/da647c549dde572c2c5edc4f5bef039c-Paper.pdf
                loss += float(w)*F.cross_entropy(torch.unsqueeze(yh, dim=0), torch.tensor([self._class_to_index[y]], device=y_hat.device), weight=C)

        n_valid += 1
    loss = loss / float(max(1, n_valid))  # batch reduction: mean

    if logging:
        self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
    return {&#39;loss&#39;: loss}</code></pre>
</details>
</dd>
<dt id="heyvi.recognition.PIP_250k.validation_epoch_end"><code class="name flex">
<span>def <span class="ident">validation_epoch_end</span></span>(<span>self, outputs)</span>
</code></dt>
<dd>
<div class="desc"><p>Called at the end of the validation epoch with the outputs of all validation steps.</p>
<p>.. code-block:: python</p>
<pre><code># the pseudocode for these calls
val_outs = []
for val_batch in val_data:
    out = validation_step(val_batch)
    val_outs.append(out)
validation_epoch_end(val_outs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>outputs</code></strong></dt>
<dd>List of outputs you defined in :meth:<code>validation_step</code>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader.</dd>
</dl>
<h2 id="return">Return</h2>
<p>None</p>
<h2 id="note">Note</h2>
<p>If you didn't define a :meth:<code>validation_step</code>, this won't be called.</p>
<h2 id="examples">Examples</h2>
<p>With a single dataloader:</p>
<p>.. code-block:: python</p>
<pre><code>def validation_epoch_end(self, val_step_outputs):
    for out in val_step_outputs:
        # do something
</code></pre>
<p>With multiple dataloaders, <code>outputs</code> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each validation step for that dataloader.</p>
<p>.. code-block:: python</p>
<pre><code>def validation_epoch_end(self, outputs):
    for dataloader_output_result in outputs:
        dataloader_outs = dataloader_output_result.dataloader_i_outputs

    self.log('final_metric', final_value)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L193-L197" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def validation_epoch_end(self, outputs):
    avg_loss = torch.stack([x[&#39;val_loss&#39;] for x in outputs]).mean()
    self.log(&#39;val_loss&#39;, avg_loss, on_epoch=True, prog_bar=False, logger=True)
    self.log(&#39;avg_val_loss&#39;, avg_loss, on_epoch=True, prog_bar=True, logger=True)                
    return {&#39;val_loss&#39;: avg_loss, &#39;avg_val_loss&#39;: avg_loss}                         </code></pre>
</details>
</dd>
<dt id="heyvi.recognition.PIP_250k.validation_step"><code class="name flex">
<span>def <span class="ident">validation_step</span></span>(<span>self, batch, batch_nb)</span>
</code></dt>
<dd>
<div class="desc"><p>Operates on a single batch of data from the validation set.
In this step you'd might generate examples or calculate anything of interest like accuracy.</p>
<p>.. code-block:: python</p>
<pre><code># the pseudocode for these calls
val_outs = []
for val_batch in val_data:
    out = validation_step(val_batch)
    val_outs.append(out)
    validation_epoch_end(val_outs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt>batch (:class:<code>~torch.Tensor</code> | (:class:<code>~torch.Tensor</code>, &hellip;) | [:class:<code>~torch.Tensor</code>, &hellip;]):</dt>
<dt>The output of your :class:<code>~torch.utils.data.DataLoader</code>. A tensor, tuple or list.</dt>
<dt><strong><code>batch_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of this batch</dd>
<dt><strong><code>dataloader_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of the dataloader that produced this batch
(only if multiple val datasets used)</dd>
</dl>
<h2 id="return">Return</h2>
<p>Any of.</p>
<ul>
<li>Any object or value</li>
<li><code>None</code> - Validation will skip to the next batch</li>
</ul>
<p>.. code-block:: python</p>
<pre><code># pseudocode of order
out = validation_step()
if defined('validation_step_end'):
    out = validation_step_end(out)
out = validation_epoch_end(out)
</code></pre>
<p>.. code-block:: python</p>
<pre><code># if you have one val dataloader:
def validation_step(self, batch, batch_idx)

# if you have multiple val dataloaders:
def validation_step(self, batch, batch_idx, dataloader_idx)
</code></pre>
<h2 id="examples">Examples</h2>
<p>.. code-block:: python</p>
<pre><code># CASE 1: A single validation dataset
def validation_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({'val_loss': loss, 'val_acc': val_acc})
</code></pre>
<p>If you pass in multiple val datasets, validation_step will have an additional argument.</p>
<p>.. code-block:: python</p>
<pre><code># CASE 2: multiple validation datasets
def validation_step(self, batch, batch_idx, dataloader_idx):
    # dataloader_idx tells you which dataset this is.
</code></pre>
<h2 id="note">Note</h2>
<p>If you don't need to validate you don't need to implement this method.</p>
<h2 id="note_1">Note</h2>
<p>When the :meth:<code>validation_step</code> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L188-L191" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def validation_step(self, batch, batch_nb):
    loss = self.training_step(batch, batch_nb, logging=False, valstep=True)[&#39;loss&#39;]
    self.log(&#39;val_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=False, logger=True)
    return {&#39;val_loss&#39;: loss}</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="heyvi.recognition.PIP_370k"><code class="flex name class">
<span>class <span class="ident">PIP_370k</span></span>
<span>(</span><span>pretrained=True, deterministic=False, modelfile=None, mlbl=False, mlfl=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Activity recognition using people in public - 250k stabilized</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L286-L392" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class PIP_370k(PIP_250k, pl.LightningModule, ActivityRecognition):

    def __init__(self, pretrained=True, deterministic=False, modelfile=None, mlbl=False, mlfl=False):
        pl.LightningModule.__init__(self)
        ActivityRecognition.__init__(self)  

        self._input_size = 112
        self._num_frames = 16        
        self._mean = [0.485, 0.456, 0.406]
        self._std = [0.229, 0.224, 0.225]
        self._mlfl = mlfl
        self._mlbl = mlbl
        if deterministic:
            np.random.seed(42)
        
        # Generated using vipy.dataset.Dataset(...).multilabel_inverse_frequency_weight()
        self._class_to_weight = {&#39;car_drops_off_person&#39;: 0.7858124882763793, &#39;car_moves&#39;: 0.18439798528529147, &#39;car_picks_up_person&#39;: 0.7380666753394193, &#39;car_reverses&#39;: 0.5753369570213479, &#39;car_starts&#39;: 0.47486292483745757, &#39;car_stops&#39;: 0.44244800737774037, &#39;car_turns_left&#39;: 0.7697107319736983, &#39;car_turns_right&#39;: 0.5412936796835607, &#39;hand_interacts_with_person&#39;: 0.2794031245117859, &#39;person_abandons_package&#39;: 1.0789960714517162, &#39;person_carries_heavy_object&#39;: 0.5032333530901552, &#39;person_closes_car_door&#39;: 0.46460114438995603, &#39;person_closes_car_trunk&#39;: 0.6824201392305784, &#39;person_closes_facility_door&#39;: 0.38990434394080076, &#39;person_embraces_person&#39;: 0.6457437695527715, &#39;person_enters_car&#39;: 0.6934926810021877, &#39;person_enters_scene_through_structure&#39;: 0.2586965095740063, &#39;person_exits_car&#39;: 0.6766386632434479, &#39;person_exits_scene_through_structure&#39;: 0.33054895987676847, &#39;person_interacts_with_laptop&#39;: 0.6720176496986436, &#39;person_loads_car&#39;: 0.6880555743488312, &#39;person_opens_car_door&#39;: 0.4069868136393968, &#39;person_opens_car_trunk&#39;: 0.6911966903970317, &#39;person_opens_facility_door&#39;: 0.3018924474724252, &#39;person_picks_up_object&#39;: 0.4298381074082487, &#39;person_purchases_from_cashier&#39;: 5.479834409621331, &#39;person_purchases_from_machine&#39;: 5.31528236654537, &#39;person_puts_down_object&#39;: 0.2804690906037155, &#39;person_reads_document&#39;: 0.5476186269530937, &#39;person_rides_bicycle&#39;: 1.6090962879286763, &#39;person_sits_down&#39;: 0.4750148103149501, &#39;person_stands_up&#39;: 0.5022364750834624, &#39;person_steals_object&#39;: 0.910991409921711, &#39;person_talks_on_phone&#39;: 0.15771902851484076, &#39;person_talks_to_person&#39;: 0.21362675034201736, &#39;person_texts_on_phone&#39;: 0.3328378404741194, &#39;person_transfers_object_to_car&#39;: 2.964890512157848, &#39;person_transfers_object_to_person&#39;: 0.6481292773603928, &#39;person_unloads_car&#39;: 0.515379337544623, &#39;person_walks&#39;: 6.341278284010202}
        
        # Generated using vipy.dataset.Dataset(...).class_to_index()
        self._class_to_index = {&#39;car_drops_off_person&#39;: 0, &#39;car_moves&#39;: 1, &#39;car_picks_up_person&#39;: 2, &#39;car_reverses&#39;: 3, &#39;car_starts&#39;: 4, &#39;car_stops&#39;: 5, &#39;car_turns_left&#39;: 6, &#39;car_turns_right&#39;: 7, &#39;hand_interacts_with_person&#39;: 8, &#39;person_abandons_package&#39;: 9, &#39;person_carries_heavy_object&#39;: 10, &#39;person_closes_car_door&#39;: 11, &#39;person_closes_car_trunk&#39;: 12, &#39;person_closes_facility_door&#39;: 13, &#39;person_embraces_person&#39;: 14, &#39;person_enters_car&#39;: 15, &#39;person_enters_scene_through_structure&#39;: 16, &#39;person_exits_car&#39;: 17, &#39;person_exits_scene_through_structure&#39;: 18, &#39;person_interacts_with_laptop&#39;: 19, &#39;person_loads_car&#39;: 20, &#39;person_opens_car_door&#39;: 21, &#39;person_opens_car_trunk&#39;: 22, &#39;person_opens_facility_door&#39;: 23, &#39;person_picks_up_object&#39;: 24, &#39;person_purchases_from_cashier&#39;: 25, &#39;person_purchases_from_machine&#39;: 26, &#39;person_puts_down_object&#39;: 27, &#39;person_reads_document&#39;: 28, &#39;person_rides_bicycle&#39;: 29, &#39;person_sits_down&#39;: 30, &#39;person_stands_up&#39;: 31, &#39;person_steals_object&#39;: 32, &#39;person_talks_on_phone&#39;: 33, &#39;person_talks_to_person&#39;: 34, &#39;person_texts_on_phone&#39;: 35, &#39;person_transfers_object_to_car&#39;: 36, &#39;person_transfers_object_to_person&#39;: 37, &#39;person_unloads_car&#39;: 38, &#39;person_walks&#39;: 39}
        
        self._verb_to_noun = {k:set([&#39;car&#39;,&#39;vehicle&#39;,&#39;motorcycle&#39;,&#39;bus&#39;,&#39;truck&#39;]) if (k.startswith(&#39;car&#39;) or k.startswith(&#39;motorcycle&#39;) or k.startswith(&#39;vehicle&#39;)) else set([&#39;person&#39;]) for k in self.classlist()}        
        self._class_to_shortlabel = heyvi.label.pip_to_shortlabel
        self._class_to_shortlabel.update( vipy.data.meva.d_category_to_shortlabel )

        if pretrained:
            self._load_pretrained()
            self.net.fc = nn.Linear(self.net.fc.in_features, self.num_classes())
        elif modelfile is not None:
            self._load_trained(modelfile)

    def topk(self, x, k=None):
        &#34;&#34;&#34;Return the top-k classes for a 3 second activity proposal along with framewise ground truth&#34;&#34;&#34;        
        yh = self.forward(x if x.ndim == 5 else x.unsqueeze(0)).detach().cpu().numpy()
        k = k if k is not None else self.num_classes()
        return [ [self.index_to_class(int(j)) for j in i[-k:][::-1]] for (s,i) in zip(yh, np.argsort(yh, axis=1))]
            
    @staticmethod
    def _totensor(v, training, validation, input_size, num_frames, mean, std, noflip=None, show=False, doflip=False, stride_jitter=3, asjson=False, classname=&#39;heyvi.recognition.PIP_370k&#39;):
        assert isinstance(v, vipy.video.Scene), &#34;Invalid input&#34;
        
        try:
            v = v.download() if (v.hasurl() and not v.hasfilename()) else v  # fetch it if necessary, but do not do this during training!        
            vc = v.clone()
            if training or validation:
                random.seed()  # force randomness after fork() 
                (clipstart, clipend) = vc.cliprange()  # clip (start, end) relative to video 
                (clipstart, clipend) = (clipstart if clipstart is not None else 0,   
                                        clipend if clipend is not None else int(np.floor(v.duration_in_frames_of_videofile() * (vc.framerate() / v.framerate_of_videofile()))))  # (yuck)
                # WARNINGS: 
                # - There exist videos with tracks outside the image rectangle due to the padding in stabilization.  
                # - There exist MEVA videos that have no tracks at the beginning and end of the padded clip since the annotations only exist for the activity
                # - There exist MEVA videos with activities that are longer than the tracks, if so, keep the interval of the activity that contains the track
                # - There exist MEVA videos with multiple objects, need to include only primary actor
                
                # - turning activities may be outside the frame (filter these)
                # - turning activities may turn into the stabilized black area.  Is this avoidaable?
                # - all of the training activities should be centered on the activity.  See if not.
                # - 
                
                if (clipend - clipstart) &gt; (num_frames + stride_jitter):
                    a = vc.primary_activity().clone().padto(num_frames/float(vc.framerate()))  # for context only, may be past end of clip now!
                    (ai, aj) = (a.startframe(), a.endframe())  # activity (start,end) relative to (clipstart, clipend)
                    (ai, aj) = (max(ai, vc.actor().startframe()), min(aj, vc.actor().endframe()))  # clip activity to when actor is present
                    startframe = random.randint(ai, aj-num_frames-1) if aj-num_frames-1 &gt; ai else ai
                    startframe = max(0, startframe + random.randint(-stride_jitter, stride_jitter))   # +/- 3 frames jitter for activity stride
                    endframe = min(clipend-clipstart-1, startframe + num_frames)  # new end cannot be past duration of clip
                    if (endframe &gt; startframe) and ((endframe - startframe) &lt; (clipend - clipstart)):
                        vc = vc.clip(startframe, endframe)
                    else: 
                        raise ValueError(&#39;invalid clip for &#34;%s&#34;&#39; % str(v))
                vc = vc.trackcrop(dilate=1.2, maxsquare=True)  # may be None if clip contains no track
                vc = vc.resize(input_size, input_size)   
                vc = vc.fliplr() if (doflip or (random.random() &gt; 0.5)) and (noflip is None or vc.category() not in noflip) else vc
            else:
                vc = vc.trackcrop(dilate=1.2, maxsquare=True)  # may be None if clip contains no track
                vc = vc.resize(input_size, input_size)  # This may introduce a preview()
                vc = vc.fliplr() if doflip and (noflip is None or vc.category() not in noflip) else vc
                
            if show:
                vc.clone().resize(512,512).show(timestamp=True)
                vc.clone().binarymask().frame(0).gain(255).rgb().show(figure=&#39;binary mask: frame 0&#39;)
                
            vc = vc.load(shape=(input_size, input_size, 3)).normalize(mean=mean, std=std, scale=1.0/255.0)  # [0,255] -&gt; [0,1], triggers load() with known shape
            (t,lbl) = vc.torch(startframe=0, length=num_frames, boundary=&#39;repeat&#39;, order=&#39;cdhw&#39;, withlabel=training or validation, nonelabel=True)  # (c=3)x(d=num_frames)x(H=input_size)x(W=input_size), reuses vc._array
            t = torch.cat((t, vc.asfloatmask(fg=0.5, bg=-0.5).torch(startframe=0, length=num_frames, boundary=&#39;repeat&#39;, order=&#39;cdhw&#39;)), dim=0)  # (c=4) x (d=num_frames) x (H=input_size) x (W=input_size), copy

        except Exception as e:
            if training or validation:
                print(&#39;[heyvi.recognition.%s._totensor][SKIPPING]: video=&#34;%s&#34;, exception=&#34;%s&#34;&#39; % (classname, str(vc), str(e)))
                (t, lbl) = (torch.zeros(4, num_frames, input_size, input_size), None)  # must always return conformal tensor (label=None means it will be ignored)
            else:
                print(&#39;[heyvi.recognition.%s._totensor][ERROR]: discarding tensor for video &#34;%s&#34; with exception &#34;%s&#34;&#39; % (classname, str(vc), str(e)))
                #t = torch.zeros(4, num_frames, input_size, input_size)  # skip me (should never get here)
                raise

        if training or validation:
            return (t, json.dumps(lbl) if not asjson else lbl)  # json to use default torch collate_fn
        else:
            return t

    def totensor(self, v=None, training=False, validation=False, show=False, doflip=False, asjson=False):
        &#34;&#34;&#34;Return captured lambda function if v=None, else return tensor&#34;&#34;&#34;    
        assert v is None or isinstance(v, vipy.video.Scene), &#34;Invalid input&#34;
        f = (lambda v, num_frames=self._num_frames, input_size=self._input_size, mean=self._mean, std=self._std, training=training, validation=validation, show=show, classname=self.__class__.__name__:
             PIP_370k._totensor(v, training, validation, input_size, num_frames, mean, std, noflip=[&#39;car_turns_left&#39;, &#39;car_turns_right&#39;, &#39;vehicle_turns_left&#39;, &#39;vehicle_turns_right&#39;, &#39;motorcycle_turns_left&#39;, &#39;motorcycle_turns_right&#39;], show=show, doflip=doflip, asjson=asjson, classname=classname))
        return f(v) if v is not None else f</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="heyvi.recognition.PIP_250k" href="#heyvi.recognition.PIP_250k">PIP_250k</a></li>
<li>pytorch_lightning.core.lightning.LightningModule</li>
<li>abc.ABC</li>
<li>pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.grads.GradInformation</li>
<li>pytorch_lightning.core.saving.ModelIO</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
<li><a title="heyvi.recognition.ActivityRecognition" href="#heyvi.recognition.ActivityRecognition">ActivityRecognition</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="heyvi.recognition.ActivityTracker" href="#heyvi.recognition.ActivityTracker">ActivityTracker</a></li>
<li><a title="heyvi.recognition.CAP" href="#heyvi.recognition.CAP">CAP</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="heyvi.recognition.PIP_370k.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="heyvi.recognition.PIP_370k.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="heyvi.recognition.PIP_370k.topk"><code class="name flex">
<span>def <span class="ident">topk</span></span>(<span>self, x, k=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the top-k classes for a 3 second activity proposal along with framewise ground truth</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/heyvi/blob/d783c7f1f50d81996eb7930afb750ebbb0145993/heyvi/recognition.py#L317-L321" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def topk(self, x, k=None):
    &#34;&#34;&#34;Return the top-k classes for a 3 second activity proposal along with framewise ground truth&#34;&#34;&#34;        
    yh = self.forward(x if x.ndim == 5 else x.unsqueeze(0)).detach().cpu().numpy()
    k = k if k is not None else self.num_classes()
    return [ [self.index_to_class(int(j)) for j in i[-k:][::-1]] for (s,i) in zip(yh, np.argsort(yh, axis=1))]</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="heyvi.recognition.PIP_250k" href="#heyvi.recognition.PIP_250k">PIP_250k</a></b></code>:
<ul class="hlist">
<li><code><a title="heyvi.recognition.PIP_250k.configure_optimizers" href="#heyvi.recognition.PIP_250k.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.forward" href="#heyvi.recognition.PIP_250k.forward">forward</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.totensor" href="#heyvi.recognition.PIP_250k.totensor">totensor</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.training_step" href="#heyvi.recognition.PIP_250k.training_step">training_step</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.validation_epoch_end" href="#heyvi.recognition.PIP_250k.validation_epoch_end">validation_epoch_end</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.validation_step" href="#heyvi.recognition.PIP_250k.validation_step">validation_step</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="Hey Vi!" href="https://github.com/visym/heyvi/">
<img src="https://www.visym.com/labs/images/visym_logo_black_notext.png" alt="" width="150"> <p> </p>
</a>
</header>
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="heyvi" href="index.html">heyvi</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="heyvi.recognition.ActivityRecognition" href="#heyvi.recognition.ActivityRecognition">ActivityRecognition</a></code></h4>
<ul class="two-column">
<li><code><a title="heyvi.recognition.ActivityRecognition.activity" href="#heyvi.recognition.ActivityRecognition.activity">activity</a></code></li>
<li><code><a title="heyvi.recognition.ActivityRecognition.binary_vector" href="#heyvi.recognition.ActivityRecognition.binary_vector">binary_vector</a></code></li>
<li><code><a title="heyvi.recognition.ActivityRecognition.class_to_index" href="#heyvi.recognition.ActivityRecognition.class_to_index">class_to_index</a></code></li>
<li><code><a title="heyvi.recognition.ActivityRecognition.classlist" href="#heyvi.recognition.ActivityRecognition.classlist">classlist</a></code></li>
<li><code><a title="heyvi.recognition.ActivityRecognition.fromindex" href="#heyvi.recognition.ActivityRecognition.fromindex">fromindex</a></code></li>
<li><code><a title="heyvi.recognition.ActivityRecognition.index_to_class" href="#heyvi.recognition.ActivityRecognition.index_to_class">index_to_class</a></code></li>
<li><code><a title="heyvi.recognition.ActivityRecognition.label_confidence" href="#heyvi.recognition.ActivityRecognition.label_confidence">label_confidence</a></code></li>
<li><code><a title="heyvi.recognition.ActivityRecognition.num_classes" href="#heyvi.recognition.ActivityRecognition.num_classes">num_classes</a></code></li>
<li><code><a title="heyvi.recognition.ActivityRecognition.temporal_support" href="#heyvi.recognition.ActivityRecognition.temporal_support">temporal_support</a></code></li>
<li><code><a title="heyvi.recognition.ActivityRecognition.top1" href="#heyvi.recognition.ActivityRecognition.top1">top1</a></code></li>
<li><code><a title="heyvi.recognition.ActivityRecognition.topk" href="#heyvi.recognition.ActivityRecognition.topk">topk</a></code></li>
<li><code><a title="heyvi.recognition.ActivityRecognition.totensor" href="#heyvi.recognition.ActivityRecognition.totensor">totensor</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="heyvi.recognition.ActivityTracker" href="#heyvi.recognition.ActivityTracker">ActivityTracker</a></code></h4>
<ul class="two-column">
<li><code><a title="heyvi.recognition.ActivityTracker.dump_patches" href="#heyvi.recognition.ActivityTracker.dump_patches">dump_patches</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTracker.finalize" href="#heyvi.recognition.ActivityTracker.finalize">finalize</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTracker.forward" href="#heyvi.recognition.ActivityTracker.forward">forward</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTracker.lrt" href="#heyvi.recognition.ActivityTracker.lrt">lrt</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTracker.temporal_stride" href="#heyvi.recognition.ActivityTracker.temporal_stride">temporal_stride</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTracker.training" href="#heyvi.recognition.ActivityTracker.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="heyvi.recognition.ActivityTrackerCap" href="#heyvi.recognition.ActivityTrackerCap">ActivityTrackerCap</a></code></h4>
<ul class="">
<li><code><a title="heyvi.recognition.ActivityTrackerCap.dump_patches" href="#heyvi.recognition.ActivityTrackerCap.dump_patches">dump_patches</a></code></li>
<li><code><a title="heyvi.recognition.ActivityTrackerCap.training" href="#heyvi.recognition.ActivityTrackerCap.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="heyvi.recognition.CAP" href="#heyvi.recognition.CAP">CAP</a></code></h4>
<ul class="">
<li><code><a title="heyvi.recognition.CAP.dump_patches" href="#heyvi.recognition.CAP.dump_patches">dump_patches</a></code></li>
<li><code><a title="heyvi.recognition.CAP.training" href="#heyvi.recognition.CAP.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="heyvi.recognition.PIP_250k" href="#heyvi.recognition.PIP_250k">PIP_250k</a></code></h4>
<ul class="">
<li><code><a title="heyvi.recognition.PIP_250k.category" href="#heyvi.recognition.PIP_250k.category">category</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.category_confidence" href="#heyvi.recognition.PIP_250k.category_confidence">category_confidence</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.configure_optimizers" href="#heyvi.recognition.PIP_250k.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.dump_patches" href="#heyvi.recognition.PIP_250k.dump_patches">dump_patches</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.forward" href="#heyvi.recognition.PIP_250k.forward">forward</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.from_checkpoint" href="#heyvi.recognition.PIP_250k.from_checkpoint">from_checkpoint</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.topk" href="#heyvi.recognition.PIP_250k.topk">topk</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.topk_probability" href="#heyvi.recognition.PIP_250k.topk_probability">topk_probability</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.totensor" href="#heyvi.recognition.PIP_250k.totensor">totensor</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.training" href="#heyvi.recognition.PIP_250k.training">training</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.training_step" href="#heyvi.recognition.PIP_250k.training_step">training_step</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.validation_epoch_end" href="#heyvi.recognition.PIP_250k.validation_epoch_end">validation_epoch_end</a></code></li>
<li><code><a title="heyvi.recognition.PIP_250k.validation_step" href="#heyvi.recognition.PIP_250k.validation_step">validation_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="heyvi.recognition.PIP_370k" href="#heyvi.recognition.PIP_370k">PIP_370k</a></code></h4>
<ul class="">
<li><code><a title="heyvi.recognition.PIP_370k.dump_patches" href="#heyvi.recognition.PIP_370k.dump_patches">dump_patches</a></code></li>
<li><code><a title="heyvi.recognition.PIP_370k.topk" href="#heyvi.recognition.PIP_370k.topk">topk</a></code></li>
<li><code><a title="heyvi.recognition.PIP_370k.training" href="#heyvi.recognition.PIP_370k.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>